
@article{baoMixedModelsShrinkage2024,
	title = {Mixed models and shrinkage estimation for balanced and unbalanced designs},
	volume = {53},
	issn = {0361-0918, 1532-4141},
	url = {https://www.tandfonline.com/doi/full/10.1080/03610918.2021.2022697},
	doi = {10.1080/03610918.2021.2022697},
	pages = {398--408},
	number = {1},
	journaltitle = {Communications in Statistics - Simulation and Computation},
	shortjournal = {Communications in Statistics - Simulation and Computation},
	author = {Bao, Yihan and Booth, James G.},
	urldate = {2024-07-24},
	date = {2024-01-02},
	langid = {english},
	keywords = {balanced\_design\_psychology},
	file = {Versione inviata:C\:\\Users\\fgfra\\Zotero\\storage\\ZVN6BBD9\\Bao e Booth - 2024 - Mixed models and shrinkage estimation for balanced.pdf:application/pdf},
}

@article{bartosRobustBayesianMeta2023,
	title = {Robust Bayesian meta‐analysis: Model‐averaging across complementary publication bias adjustment methods},
	volume = {14},
	issn = {1759-2879},
	doi = {10.1002/jrsm.1594},
	shorttitle = {Robust Bayesian meta‐analysis},
	abstract = {Publication bias is a ubiquitous threat to the validity of meta‐analysis and the accumulation of scientific evidence. In order to estimate and counteract the impact of publication bias, multiple methods have been developed; however, recent simulation studies have shown the methods' performance to depend on the true data generating process, and no method consistently outperforms the others across a wide range of conditions. Unfortunately, when different methods lead to contradicting conclusions, researchers can choose those methods that lead to a desired outcome. To avoid the condition‐dependent, all‐or‐none choice between competing methods and conflicting results, we extend robust Bayesian meta‐analysis and model‐average across two prominent approaches of adjusting for publication bias: (1) selection models of p‐values and (2) models adjusting for small‐study effects. The resulting model ensemble weights the estimates and the evidence for the absence/presence of the effect from the competing approaches with the support they receive from the data. Applications, simulations, and comparisons to preregistered, multi‐lab replications demonstrate the benefits of Bayesian model‐averaging of complementary publication bias adjustment methods. ({PsycInfo} Database Record (c) 2023 {APA}, all rights reserved)},
	pages = {99--116},
	number = {1},
	journaltitle = {Research Synthesis Methods},
	shortjournal = {Research Synthesis Methods},
	author = {Bartoš, František and Maier, Maximilian and Wagenmakers, Eric‐Jan and Doucouliagos, Hristos and Stanley, T. D.},
	date = {2023-01},
	note = {Publisher: John Wiley \& Sons},
	keywords = {Bayesian model‐averaging, Meta Analysis, meta‐analysis, Models, {PET}‐{PEESE}, publication bias, Scientific Communication, selection models, Simulation, Standard Deviation, Statistical Probability, bayesian model averaging, balanced\_design\_psychology},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\HU7NZV7U\\Bartoš et al. - 2023 - Robust Bayesian meta‐analysis Model‐averaging acr.pdf:application/pdf},
}

@article{vanginkelMultipleImputationBalance2021,
	title = {Multiple imputation to balance unbalanced designs for two-way analysis of variance},
	volume = {17},
	issn = {1614-1881},
	doi = {10.5964/meth.6085},
	abstract = {A balanced {ANOVA} design provides an unambiguous interpretation of the F-tests, and has more power than an unbalanced design. In earlier literature, multiple imputation was proposed to create balance in unbalanced designs, as an alternative to Type-{III} sum of squares. In the current simulation study we studied four pooled statistics for multiple imputation, namely D₀, D₁, D₂, and D₃ in unbalanced data, and compared them with Type-{III} sum of squares. Statistics D₁ and D₂ generally performed best regarding Type-I error rates, and had power rates closest to that of Type-{III} sum of squares. Additionally, for the interaction, D₁ produced power rates higher than Type-{III} sum of squares. For multiply imputed datasets D₁ and D₂ may be the best methods for pooling the results in multiply imputed datasets, and for unbalanced data, D₁ might be a good alternative to Type-{III} sum of squares regarding the interaction. ({PsycInfo} Database Record (c) 2021 {APA}, all rights reserved)},
	pages = {39--57},
	number = {1},
	journaltitle = {Methodology: European Journal of Research Methods for the Behavioral and Social Sciences},
	shortjournal = {Methodology: European Journal of Research Methods for the Behavioral and Social Sciences},
	author = {van Ginkel, Joost R. and Kroonenberg, Pieter M.},
	date = {2021},
	note = {Publisher: Leibniz Institute for Psychology Information},
	keywords = {Analysis of Variance, missing data, multiple imputation, Statistical Data, two-way analysis of variance, Type I Errors, type-{III} sum of squares, unbalanced designs, balanced\_design\_psychology},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\CNQ6NVXP\\van Ginkel e Kroonenberg - 2021 - Multiple imputation to balance unbalanced designs .pdf:application/pdf},
}

@article{fEvaluatingFitIndices2024,
	title = {Evaluating fit indices in a multilevel latent growth model with unbalanced design: a Monte Carlo study},
	volume = {15},
	issn = {1664-1078},
	url = {https://pubmed.ncbi.nlm.nih.gov/38765833/},
	doi = {10.3389/fpsyg.2024.1366850},
	shorttitle = {Evaluating fit indices in a multilevel latent growth model with unbalanced design},
	abstract = {This study informed researchers about the performance of different level-specific and target-specific model fit indices in the Multilevel Latent Growth Model ({MLGM}) with unbalanced design. As the use of {MLGMs} is relatively new in applied research domain, this study helped researchers using specific …},
	journaltitle = {Frontiers in psychology},
	author = {F, Pan and Q, Liu},
	urldate = {2024-07-25},
	date = {2024-05-03},
	langid = {english},
	pmid = {38765833},
	note = {Publisher: Front Psychol},
	keywords = {balanced\_design\_psychology},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\IN4CZ9HY\\F e Q - 2024 - Evaluating fit indices in a multilevel latent grow.pdf:application/pdf;Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\2YC5LRZJ\\38765833.html:text/html},
}

@article{shuBalancedPartiallyBalanced2010,
	title = {Balanced and partially balanced incomplete block designs with autocorrelation errors},
	volume = {140},
	issn = {0378-3758},
	doi = {10.1016/j.jspi.2010.04.021},
	abstract = {The paper aims to find variance balanced and variance partially balanced incomplete block designs when observations within blocks are autocorrelated and we call them {BIBAC} and {PBIBAC} designs. Orthogonal arrays of type I and type {II} when used as {BIBAC} designs have smaller average variance of elementary contrasts of treatment effects compared to the corresponding Balanced Incomplete Block ({BIB}) designs with homoscedastic, uncorrelated errors. The relative efficiency of {BIB} designs compared to {BIBAC} designs depends on the block size
k and the autocorrelation
ρ and is independent of the number of treatments. Further this relative efficiency increases with increasing
k. Partially balanced incomplete block designs with autocorrelated errors are introduced using partially balanced incomplete block designs and orthogonal arrays of type I and type {II}.},
	pages = {3230--3235},
	number = {11},
	journaltitle = {Journal of statistical planning and inference},
	author = {Shu, Xiaohua and Raghavarao, Damaraju},
	date = {2010},
	note = {Place: Kidlington
Publisher: Elsevier B.V},
	keywords = {Statistics, Experimental design, Mathematics, Multivariate analysis},
}

@article{shuBalancedPartiallyBalanced2010a,
	title = {Balanced and partially balanced incomplete block designs with autocorrelation errors},
	volume = {140},
	issn = {0378-3758},
	url = {https://www.sciencedirect.com/science/article/pii/S0378375810002016},
	doi = {10.1016/j.jspi.2010.04.021},
	abstract = {The paper aims to find variance balanced and variance partially balanced incomplete block designs when observations within blocks are autocorrelated and we call them {BIBAC} and {PBIBAC} designs. Orthogonal arrays of type I and type {II} when used as {BIBAC} designs have smaller average variance of elementary contrasts of treatment effects compared to the corresponding Balanced Incomplete Block ({BIB}) designs with homoscedastic, uncorrelated errors. The relative efficiency of {BIB} designs compared to {BIBAC} designs depends on the block size k and the autocorrelation ρ and is independent of the number of treatments. Further this relative efficiency increases with increasing k. Partially balanced incomplete block designs with autocorrelated errors are introduced using partially balanced incomplete block designs and orthogonal arrays of type I and type {II}.},
	pages = {3230--3235},
	number = {11},
	journaltitle = {Journal of Statistical Planning and Inference},
	shortjournal = {Journal of Statistical Planning and Inference},
	author = {Shu, Xiaohua and Raghavarao, Damaraju},
	urldate = {2024-07-25},
	date = {2010-11-01},
	keywords = {Autocorrelation, Average variance, {BIB} designs, Orthogonal arrays of type I and type {II}, {PBIB} designs, Relative efficiency},
	file = {ScienceDirect Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\725EU4L7\\S0378375810002016.html:text/html},
}

@article{abeynayakeNeighborBalancedBipartiteBlock2011,
	title = {Neighbor-Balanced Bipartite Block Designs},
	volume = {40},
	issn = {0361-0926},
	url = {https://doi.org/10.1080/03610926.2010.505692},
	doi = {10.1080/03610926.2010.505692},
	abstract = {This article deals with the neighbor-balanced block design setting when there are two disjoint sets of treatments, one set consisting of test treatments and the other of control treatments. The interest here is to estimate the contrasts pertaining to test treatments vs. control treatments (with respect to direct and neighbors) with as high precision as possible. Some series of neighbor-balanced block designs for comparing a set of test treatments to a set of control treatments have been developed. The designs obtained are totally balanced in the sense that all the contrasts among test treatments for direct and neighbor effects are estimated with same variance and all the contrasts pertaining to test vs. control for direct and neighbor effects are estimated with the same variance.},
	pages = {4041--4052},
	number = {22},
	journaltitle = {Communications in Statistics - Theory and Methods},
	author = {Abeynayake, N. R. and Jaggi, Seema and Varghese, Cini},
	urldate = {2024-07-25},
	date = {2011-11-15},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/03610926.2010.505692},
	keywords = {62K10, Circular design, Control treatments, Direct effects, Neighbor effects, Test treatments, Totally balanced design},
}

@article{shuBalancedPartiallyBalanced2010b,
	title = {Balanced and partially balanced incomplete block designs with autocorrelation errors},
	volume = {140},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {03783758},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378375810002016},
	doi = {10.1016/j.jspi.2010.04.021},
	abstract = {The paper aims to ﬁnd variance balanced and variance partially balanced incomplete block designs when observations within blocks are autocorrelated and we call them {BIBAC} and {PBIBAC} designs. Orthogonal arrays of type I and type {II} when used as {BIBAC} designs have smaller average variance of elementary contrasts of treatment effects compared to the corresponding Balanced Incomplete Block ({BIB}) designs with homoscedastic, uncorrelated errors. The relative efﬁciency of {BIB} designs compared to {BIBAC} designs depends on the block size k and the autocorrelation r and is independent of the number of treatments. Further this relative efﬁciency increases with increasing k. Partially balanced incomplete block designs with autocorrelated errors are introduced using partially balanced incomplete block designs and orthogonal arrays of type I and type {II}.},
	pages = {3230--3235},
	number = {11},
	journaltitle = {Journal of Statistical Planning and Inference},
	shortjournal = {Journal of Statistical Planning and Inference},
	author = {Shu, Xiaohua and Raghavarao, Damaraju},
	urldate = {2024-07-25},
	date = {2010-11},
	langid = {english},
	keywords = {balanced\_design\_psychology},
	file = {Shu e Raghavarao - 2010 - Balanced and partially balanced incomplete block d.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\CFFJJ4W6\\Shu e Raghavarao - 2010 - Balanced and partially balanced incomplete block d.pdf:application/pdf},
}

@article{filipiakOptimalityNeighborBalanced2003,
	title = {Optimality of neighbor balanced designs under mixed effects model},
	volume = {61},
	issn = {0167-7152},
	url = {https://www.sciencedirect.com/science/article/pii/S0167715202003395},
	doi = {10.1016/S0167-7152(02)00339-5},
	abstract = {This paper generalizes Druilhet's results on optimality of circular neighbor balanced block designs under the model with fixed neighbor effects. It is shown that some of these designs are also optimal under the model with random neighbor effects.},
	pages = {225--234},
	number = {3},
	journaltitle = {Statistics \& Probability Letters},
	shortjournal = {Statistics \& Probability Letters},
	author = {Filipiak, K. and Markiewicz, A.},
	urldate = {2024-07-25},
	date = {2003-02-01},
	keywords = {Circular neighbor balanced design, Mixed effects model, Optimal experimental designs},
	file = {ScienceDirect Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\4AHHERZR\\S0167715202003395.html:text/html},
}

@article{laurencelleUnweightedHarmonicMean2017,
	title = {The unweighted ``harmonic mean'' solution for unbalanced anova designs : A detailed argument},
	volume = {13},
	issn = {2292-1354},
	url = {http://www.tqmp.org/RegularArticles/vol13-1/p095},
	doi = {10.20982/tqmp.13.1.p095},
	shorttitle = {The unweighted ``harmonic mean'' solution for unbalanced anova designs},
	abstract = {The treatment of unbalanced designs in analysis of variance (anova) has a long and still controversial history, an issue being the choice between the so-called “harmonic mean” or unweighted solution and the classical weighted solution. We here argue in favour of the unweighted, i.e. equally weighted solution, based on the following reasons. The classical solution gives more weight to the means obtained from a more numerous group of data, thus inducing a positive bias in the computation of the between-group mean square, irrespective of the groups’ effect sizes. Indeed, this differential weighing is at variance with the determination and handling of effect sizes, whose values are kept free of the various group sizes implied, so that the ﬁnal ‘weighted’ F statistic cannot stand for a truthful reﬂection of those. Besides, the oft-quoted argument around the demographic representativeness of the various groups compared is specious in the context of most anova applications, the purpose of anova being to compare groups/conditions one to the other, whatever their sample sizes. Finally, in the cases of two- or multi-way designs, the weighted solution precludes the calculation of truly orthogonal and additive variance components, the ‘linear regression’ alternatives for this problem being complex and essentially arbitrary. The “harmonic mean” solution preserves orthogonality and additivity in the variance decomposition for multi-dimensional designs, is congruent with effect sizes and entails no differential bias in the calculation of the F test whatever the sample sizes. On the other hand, it suffers from a positive bias in the F ’s signiﬁcance, a bias negligible for mildly unbalanced group sizes and aptly corrected by {RANKIN}’s (1974) modiﬁed degrees of freedom.},
	pages = {95--104},
	number = {1},
	journaltitle = {The Quantitative Methods for Psychology},
	shortjournal = {{TQMP}},
	author = {Laurencelle, Louis},
	urldate = {2024-07-25},
	date = {2017-01-01},
	langid = {french},
	file = {Laurencelle - 2017 - The unweighted ``harmonic mean'' solution for unba.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\MXN2VR8R\\Laurencelle - 2017 - The unweighted ``harmonic mean'' solution for unba.pdf:application/pdf},
}

@article{laurencelleUnweightedHarmonicMean2017a,
	title = {The unweighted ``harmonic mean'' solution for unbalanced anova designs : A detailed argument},
	volume = {13},
	doi = {10.20982/tqmp.13.1.p095},
	shorttitle = {The unweighted ``harmonic mean'' solution for unbalanced anova designs},
	abstract = {The treatment of unbalanced designs in analysis of variance (anova) has a long and still controversial history, an issue being the choice between the so-called harmonic mean or unweighted solution and the classical weighted solution. We here argue in favour of the unweighted, i.e. equally weighted solution, based on the following reasons. The classical solution gives more weight to the means obtained from a more numerous group of data, thus inducing a positive bias in the computation of the between-group mean square, irrespective of the groups' effect sizes. Indeed, this differential weighing is at variance with the determination and handling of effect sizes, whose values are kept free of the various group sizes implied, so that the final weighted' \$F\$ statistic cannot stand for a truthful reflection of those. Besides, the oft-quoted argument around the demographic representativeness of the various groups compared is specious in the context of most anova applications, the purpose of anova being to compare groups/conditions one to the other, whatever their sample sizes. Finally, in the cases of two- or multi-way designs, the weighted solution precludes the calculation of truly orthogonal and additive variance components, the linear regression' alternatives for this problem being complex and essentially arbitrary. The harmonic mean solution preserves orthogonality and additivity in the variance decomposition for multi-dimensional designs, is congruent with effect sizes and entails no differential bias in the calculation of the \$F\$ test whatever the sample sizes. On the other hand, it suffers from a positive bias in the \$F\$'s significance, a bias negligible for mildly unbalanced group sizes and aptly corrected by Rankin (1974) modified degrees of freedom.},
	pages = {95--104},
	number = {1},
	journaltitle = {Tutorials in Quantitative Methods for Psychology},
	shortjournal = {Tutorials in Quantitative Methods for Psychology},
	author = {Laurencelle, Louis},
	date = {2017-01-01},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\D4FUB37Q\\Laurencelle - 2017 - The unweighted ``harmonic mean'' solution for unba.pdf:application/pdf},
}

@article{ortmannProperExperimentalDesign2004,
	title = {Proper experimental design and implementation are necessary conditions for a balanced social psychology},
	volume = {27},
	doi = {10.1017/S0140525X04470083},
	abstract = {We applaud the authors' basic message. We note that the negative research emphasis is not special solely to social psychology and judgment and decision-making. We argue that the proposed integration of null hypothesis significance testing ({NHST}) and Bayesian analysis is promising but will ultimately succeed only if more attention is paid to proper experimental design and implementation.},
	pages = {352--353},
	number = {3},
	journaltitle = {Behavioral and Brain Sciences},
	author = {Ortmann, A. and Ostatnicky, M.},
	date = {2004},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\7EKVPD64\\display.html:text/html},
}

@article{kruegerBalancedSocialPsychology2004,
	title = {Towards a balanced social psychology: Causes, consequences, and cures for the problem-seeking approach to social behavior and cognition},
	volume = {27},
	rights = {https://www.cambridge.org/core/terms},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X04000081/type/journal_article},
	doi = {10.1017/S0140525X04000081},
	shorttitle = {Towards a balanced social psychology},
	abstract = {We applaud the authors’ basic message. We note that the negative research emphasis is not special solely to social psychology and judgment and decision-making. We argue that the proposed integration of null hypothesis significance testing ({NHST}) and Bayesian analysis is promising but will ultimately succeed only if more attention is paid to proper experimental design and implementation.},
	pages = {313--327},
	number = {3},
	journaltitle = {Behavioral and Brain Sciences},
	shortjournal = {Behav Brain Sci},
	author = {Krueger, Joachim I. and Funder, David C.},
	urldate = {2024-07-25},
	date = {2004-06},
	langid = {english},
	keywords = {balanced\_design\_psychology},
	file = {Krueger e Funder - 2004 - Towards a balanced social psychology Causes, cons.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\5ZKJ52P2\\Krueger e Funder - 2004 - Towards a balanced social psychology Causes, cons.pdf:application/pdf},
}

@article{kruegerBalancedSocialPsychology2004a,
	title = {Towards a balanced social psychology: Causes, consequences, and cures for the problem-seeking approach to social behavior and cognition},
	volume = {27},
	rights = {https://www.cambridge.org/core/terms},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X04000081/type/journal_article},
	doi = {10.1017/S0140525X04000081},
	shorttitle = {Towards a balanced social psychology},
	abstract = {We applaud the authors’ basic message. We note that the negative research emphasis is not special solely to social psychology and judgment and decision-making. We argue that the proposed integration of null hypothesis significance testing ({NHST}) and Bayesian analysis is promising but will ultimately succeed only if more attention is paid to proper experimental design and implementation.},
	pages = {313--327},
	number = {3},
	journaltitle = {Behavioral and Brain Sciences},
	shortjournal = {Behav Brain Sci},
	author = {Krueger, Joachim I. and Funder, David C.},
	urldate = {2024-07-25},
	date = {2004-06},
	langid = {english},
	file = {Krueger e Funder - 2004 - Towards a balanced social psychology Causes, cons.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\U4C6DZI5\\Krueger e Funder - 2004 - Towards a balanced social psychology Causes, cons.pdf:application/pdf},
}

@article{keselmanAnalysingUnbalancedRepeated1990,
	title = {Analysing unbalanced repeated measures designs},
	volume = {43},
	issn = {2044-8317},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1990.tb00940.x},
	doi = {10.1111/j.2044-8317.1990.tb00940.x},
	abstract = {For repeated measures designs containing at least one between-subjects factor (split-plot designs), exact univariate F tests of within-subjects effects depend on the assumption of multisample sphericity. Monte Carlo methods were used to examine univariate and multivariate data-analytic strategies for unbalanced split-plot repeated measures designs in which the data do not conform to the condition of multisample sphericity. For varying degrees of assumption violation, empirical Type I error and power rates were determined for varying numbers of levels of the within-subjects factor, total sample size, and degree of sample size inequality. Only the approximate univariate procedures provided robust tests of within-subjects main effects in unbalanced designs. Robust univariate and multivariate tests of within-subjects interaction effects were only achieved when the design was balanced and the number of repeated measurements was moderate.},
	pages = {265--282},
	number = {2},
	journaltitle = {British Journal of Mathematical and Statistical Psychology},
	author = {Keselman, Joanne C. and Keselman, H. J.},
	urldate = {2024-07-25},
	date = {1990},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1990.tb00940.x},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\PFVBQLG3\\j.2044-8317.1990.tb00940.html:text/html},
}

@article{keselmanAnalysingUnbalancedRepeated1990a,
	title = {Analysing unbalanced repeated measures designs},
	volume = {43},
	issn = {2044-8317},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1990.tb00940.x},
	doi = {10.1111/j.2044-8317.1990.tb00940.x},
	abstract = {For repeated measures designs containing at least one between-subjects factor (split-plot designs), exact univariate F tests of within-subjects effects depend on the assumption of multisample sphericity. Monte Carlo methods were used to examine univariate and multivariate data-analytic strategies for unbalanced split-plot repeated measures designs in which the data do not conform to the condition of multisample sphericity. For varying degrees of assumption violation, empirical Type I error and power rates were determined for varying numbers of levels of the within-subjects factor, total sample size, and degree of sample size inequality. Only the approximate univariate procedures provided robust tests of within-subjects main effects in unbalanced designs. Robust univariate and multivariate tests of within-subjects interaction effects were only achieved when the design was balanced and the number of repeated measurements was moderate.},
	pages = {265--282},
	number = {2},
	journaltitle = {British Journal of Mathematical and Statistical Psychology},
	author = {Keselman, Joanne C. and Keselman, H. J.},
	urldate = {2024-07-25},
	date = {1990},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1990.tb00940.x},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\MCCLIRJR\\j.2044-8317.1990.tb00940.html:text/html},
}

@article{guoNewHeterogeneousTest2011,
	title = {New heterogeneous test statistics for the unbalanced fixed-effect nested design},
	volume = {64},
	issn = {0007-1102},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/10.1348/000711010X512688},
	doi = {10.1348/000711010X512688},
	abstract = {When the underlying variances are unknown or/and unequal, using the conventional F test is problematic in the two-factor hierarchical data structure. Prompted by the approximate test statistics (Welch and Alexander-Govern methods), the authors develop four new heterogeneous test statistics to test factor A and factor B nested within A for the unbalanced fixed-effect two-stage nested design under variance heterogeneity. The actual significance levels and statistical power of the test statistics were compared in a simulation study. The results show that the proposed procedures maintain better Type I error rate control and have greater statistical power than those obtained by the conventional F test in various conditions. Therefore, the proposed test statistics are recommended in terms of robustness and easy implementation.},
	pages = {259--276},
	number = {2},
	journaltitle = {{BRITISH} {JOURNAL} {OF} {MATHEMATICAL} \& {STATISTICAL} {PSYCHOLOGY}},
	shortjournal = {Br. J. Math. Stat. Psychol.},
	author = {Guo, Jiin-Huarng and Billard, L. and Luh, Wei-Ming},
	urldate = {2024-07-25},
	date = {2011-05},
	note = {Num Pages: 18
Place: Malden
Publisher: Wiley-Blackwell
Web of Science {ID}: {WOS}:000289631700005},
	keywords = {{ANOVA}, {APPROXIMATION}, {HETEROSCEDASTICITY}, {NONNORMALITY}, {VARIANCE} {HETEROGENEITY}},
}

@article{guoNewHeterogeneousTest2011a,
	title = {New heterogeneous test statistics for the unbalanced fixed-effect nested design},
	volume = {64},
	issn = {2044-8317},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1348/000711010X512688},
	doi = {10.1348/000711010X512688},
	abstract = {When the underlying variances are unknown or/and unequal, using the conventional F test is problematic in the two-factor hierarchical data structure. Prompted by the approximate test statistics (Welch and Alexander–Govern methods), the authors develop four new heterogeneous test statistics to test factor A and factor B nested within A for the unbalanced fixed-effect two-stage nested design under variance heterogeneity. The actual significance levels and statistical power of the test statistics were compared in a simulation study. The results show that the proposed procedures maintain better Type I error rate control and have greater statistical power than those obtained by the conventional F test in various conditions. Therefore, the proposed test statistics are recommended in terms of robustness and easy implementation.},
	pages = {259--276},
	number = {2},
	journaltitle = {British Journal of Mathematical and Statistical Psychology},
	author = {Guo, Jiin-Huarng and Billard, L. and Luh, Wei-Ming},
	urldate = {2024-07-25},
	date = {2011},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1348/000711010X512688},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\8EBKAM5V\\000711010X512688.html:text/html},
}

@article{kherad-pajouhGeneralPermutationApproach2015,
	title = {A general permutation approach for analyzing repeated measures {ANOVA} and mixed-model designs},
	volume = {56},
	issn = {1613-9798},
	url = {https://doi.org/10.1007/s00362-014-0617-3},
	doi = {10.1007/s00362-014-0617-3},
	abstract = {Repeated measures {ANOVA} and mixed-model designs are the main classes of experimental designs used in psychology. The usual analysis relies on some parametric assumptions (typically Gaussianity). In this article, we propose methods to analyze the data when the parametric conditions do not hold. The permutation test, which is a non-parametric test, is suitable for hypothesis testing and can be applied to experimental designs. The application of permutation tests in simpler experimental designs such as factorial {ANOVA} or {ANOVA} with only between-subject factors has already been considered. The main purpose of this paper is to focus on more complex designs that include only within-subject factors (repeated measures) or designs that include both within-subject and between-subject factors (mixed-model designs). First, a general approximate permutation test (permutation of the residuals under the reduced model or reduced residuals) is proposed for any repeated measures and mixed-model designs, for any number of repetitions per cell, any number of subjects and factors and for both balanced and unbalanced designs (all-cell-filled). Next, a permutation test that uses residuals that are exchangeable up to the second moment is introduced for balanced cases in the same class of experimental designs. This permutation test is therefore exact for spherical data. Finally, we provide simulations results for the comparison of the level and the power of the proposed methods.},
	pages = {947--967},
	number = {4},
	journaltitle = {Statistical Papers},
	shortjournal = {Stat Papers},
	author = {Kherad-Pajouh, Sara and Renaud, Olivier},
	urldate = {2024-07-25},
	date = {2015-11-01},
	langid = {english},
	keywords = {Balance Design, Covariance Matrix, Design Matrice, Permutation Test, Repeat Measure Design, balanced\_design\_psychology},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\8ZHVXT26\\Kherad-Pajouh e Renaud - 2015 - A general permutation approach for analyzing repea.pdf:application/pdf},
}

@article{guoNewHeterogeneousTest2011b,
	title = {New heterogeneous test statistics for the unbalanced fixed-effect nested design},
	volume = {64},
	issn = {2044-8317},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1348/000711010X512688},
	doi = {10.1348/000711010X512688},
	abstract = {When the underlying variances are unknown or/and unequal, using the conventional F test is problematic in the two-factor hierarchical data structure. Prompted by the approximate test statistics (Welch and Alexander–Govern methods), the authors develop four new heterogeneous test statistics to test factor A and factor B nested within A for the unbalanced fixed-effect two-stage nested design under variance heterogeneity. The actual significance levels and statistical power of the test statistics were compared in a simulation study. The results show that the proposed procedures maintain better Type I error rate control and have greater statistical power than those obtained by the conventional F test in various conditions. Therefore, the proposed test statistics are recommended in terms of robustness and easy implementation.},
	pages = {259--276},
	number = {2},
	journaltitle = {British Journal of Mathematical and Statistical Psychology},
	author = {Guo, Jiin-Huarng and Billard, L. and Luh, Wei-Ming},
	urldate = {2024-07-25},
	date = {2011},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1348/000711010X512688},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\W4RF3BGC\\000711010X512688.html:text/html},
}

@article{guoNewHeterogeneousTest2011c,
	title = {New heterogeneous test statistics for the unbalanced fixed-effect nested design},
	volume = {64},
	rights = {©2010 The British Psychological Society},
	issn = {2044-8317},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1348/000711010X512688},
	doi = {10.1348/000711010X512688},
	abstract = {When the underlying variances are unknown or/and unequal, using the conventional F test is problematic in the two-factor hierarchical data structure. Prompted by the approximate test statistics (Welch and Alexander–Govern methods), the authors develop four new heterogeneous test statistics to test factor A and factor B nested within A for the unbalanced fixed-effect two-stage nested design under variance heterogeneity. The actual significance levels and statistical power of the test statistics were compared in a simulation study. The results show that the proposed procedures maintain better Type I error rate control and have greater statistical power than those obtained by the conventional F test in various conditions. Therefore, the proposed test statistics are recommended in terms of robustness and easy implementation.},
	pages = {259--276},
	number = {2},
	journaltitle = {British Journal of Mathematical and Statistical Psychology},
	author = {Guo, Jiin-Huarng and Billard, L. and Luh, Wei-Ming},
	urldate = {2024-07-25},
	date = {2011},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1348/000711010X512688},
	keywords = {balanced\_design\_psychology},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\CS5SD23K\\Guo et al. - 2011 - New heterogeneous test statistics for the unbalanc.pdf:application/pdf;Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\P98R448S\\000711010X512688.html:text/html},
}

@online{zotero-52,
	url = {file:///C:/Users/fgfra/Desktop/tesi/tesi/tesi_articoli/},
	urldate = {2024-07-25},
	file = {:C\:\\Users\\fgfra\\Zotero\\storage\\G3JQH7BS\\tesi_articoli.xml:text/xml},
}

@article{valizadehAbstractScreeningUsing2022,
	title = {Abstract screening using the automated tool Rayyan: results of effectiveness in three diagnostic test accuracy systematic reviews},
	volume = {22},
	issn = {1471-2288},
	url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-022-01631-8},
	doi = {10.1186/s12874-022-01631-8},
	shorttitle = {Abstract screening using the automated tool Rayyan},
	abstract = {Abstract
            
              Objective
              To evaluate the performance of the automated abstract screening tool Rayyan.
            
            
              Methods
              The records obtained from the search for three systematic reviews were manually screened in four stages. At the end of each stage, Rayyan was used to predict the eligibility score for the remaining records. At two different thresholds (≤2.5 and {\textless} 2.5 for exclusion of a record) Rayyan-generated ratings were compared with the decisions made by human reviewers in the manual screening process and the tool’s accuracy metrics were calculated.
            
            
              Results
              Two thousand fifty-four records were screened manually, of which 379 were judged to be eligible for full-text assessment, and 112 were eventually included in the final review. For finding records eligible for full-text assessment, at the threshold of {\textless} 2.5 for exclusion, Rayyan managed to achieve sensitivity values of 97-99\% with specificity values of 19-58\%, while at the threshold of ≤2.5 for exclusion it had a specificity of 100\% with sensitivity values of 1-29\%. For the task of finding eligible reports for inclusion in the final review, almost similar results were obtained.
            
            
              Discussion
              At the threshold of {\textless} 2.5 for exclusion, Rayyan managed to be a reliable tool for excluding ineligible records, but it was not much reliable for finding eligible records. We emphasize that this study was conducted on diagnostic test accuracy reviews, which are more difficult to screen due to inconsistent terminology.},
	pages = {160},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	shortjournal = {{BMC} Med Res Methodol},
	author = {Valizadeh, Amir and Moassefi, Mana and Nakhostin-Ansari, Amin and Hosseini Asl, Seyed Hossein and Saghab Torbati, Mehrnush and Aghajani, Reyhaneh and Maleki Ghorbani, Zahra and Faghani, Shahriar},
	urldate = {2024-07-25},
	date = {2022-12},
	langid = {english},
	keywords = {automatic\_abstract\_screening},
	file = {Valizadeh et al. - 2022 - Abstract screening using the automated tool Rayyan.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\W2DXJA5K\\Valizadeh et al. - 2022 - Abstract screening using the automated tool Rayyan.pdf:application/pdf},
}

@article{ngSemiautomatingAbstractScreening2023,
	title = {Semi-automating abstract screening with a natural language model pretrained on biomedical literature},
	volume = {12},
	issn = {2046-4053},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-023-02353-8},
	doi = {10.1186/s13643-023-02353-8},
	abstract = {We demonstrate the performance and workload impact of incorporating a natural language model, pretrained on citations of biomedical literature, on a workflow of abstract screening for studies on prognostic factors in endstage lung disease. The model was optimized on one-third of the abstracts, and model performance on the remain‑ing abstracts was reported. Performance of the model, in terms of sensitivity, precision, F1 and inter-rater agree‑ment, was moderate in comparison with other published models. However, incorporating it into the screening workflow, with the second reviewer screening only abstracts with conflicting decisions, translated into a 65\% reduction in the number of abstracts screened by the second reviewer. Subsequent work will look at incorporating the pre-trained {BERT} model into screening workflows for other studies prospectively, as well as improving model performance.},
	pages = {172},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Syst Rev},
	author = {Ng, Sheryl Hui-Xian and Teow, Kiok Liang and Ang, Gary Yee and Tan, Woan Shin and Hum, Allyn},
	urldate = {2024-07-25},
	date = {2023-09-23},
	langid = {english},
	keywords = {automatic\_abstract\_screening},
	file = {Ng et al. - 2023 - Semi-automating abstract screening with a natural .pdf:C\:\\Users\\fgfra\\Zotero\\storage\\W69M6ZQ3\\Ng et al. - 2023 - Semi-automating abstract screening with a natural .pdf:application/pdf},
}

@article{natukundaUnsupervisedTitleAbstract2023,
	title = {Unsupervised title and abstract screening for systematic review: a retrospective case-study using topic modelling methodology},
	volume = {12},
	issn = {2046-4053},
	url = {https://systematicreviewsjournal.biomedcentral.com/articles/10.1186/s13643-022-02163-4},
	doi = {10.1186/s13643-022-02163-4},
	shorttitle = {Unsupervised title and abstract screening for systematic review},
	abstract = {Background:  The importance of systematic reviews in collating and summarising available research output on a particular topic cannot be over-emphasized. However, initial screening of retrieved literature is significantly time and labour intensive. Attempts at automating parts of the systematic review process have been made with varying degree of success partly due to being domain-specific, requiring vendor-specific software or manually labelled training data. Our primary objective was to develop statistical methodology for performing automated title and abstract screening for systematic reviews. Secondary objectives included (1) to retrospectively apply the automated screening methodology to previously manually screened systematic reviews and (2) to characterize the performance of the automated screening methodology scoring algorithm in a simulation study.
Methods:  We implemented a Latent Dirichlet Allocation-based topic model to derive representative topics from the retrieved documents’ title and abstract. The second step involves defining a score threshold for classifying the documents as relevant for full-text review or not. The score is derived based on a set of search keywords (often the database retrieval search terms). Two systematic review studies were retrospectively used to illustrate the methodology.
Results:  In one case study (helminth dataset), 69.83\% sensitivity compared to manual title and abstract screening was achieved. This is against a false positive rate of 22.63\% . For the second case study (Wilson disease dataset), a sensitivity of 54.02\% and specificity of 67.03\% were achieved.
Conclusions:  Unsupervised title and abstract screening has the potential to reduce the workload involved in conducting systematic review. While sensitivity of the methodology on the tested data is low, approximately 70\% specificity was achieved. Users ought to keep in mind that potentially low sensitivity might occur. One approach to mitigate this might be to incorporate additional targeted search keywords such as the indexing databases terms into the search term copora. Moreover, automated screening can be used as an additional screener to the manual screeners.},
	pages = {1},
	number = {1},
	journaltitle = {Systematic Reviews},
	shortjournal = {Syst Rev},
	author = {Natukunda, Agnes and Muchene, Leacky K.},
	urldate = {2024-07-25},
	date = {2023-01-03},
	langid = {english},
	keywords = {automatic\_abstract\_screening},
	file = {Natukunda e Muchene - 2023 - Unsupervised title and abstract screening for syst.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\XVNG39VW\\Natukunda e Muchene - 2023 - Unsupervised title and abstract screening for syst.pdf:application/pdf},
}

@article{brehmContrastCodingChoices2022,
	title = {Contrast coding choices in a decade of mixed models},
	volume = {125},
	issn = {0749596X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X22000213},
	doi = {10.1016/j.jml.2022.104334},
	abstract = {Contrast coding in regression models, including mixed-effect models, changes what the terms in the model mean. In particular, it determines whether or not model terms should be interpreted as main effects. This paper highlights how opaque descriptions of contrast coding have affected the field of psycholinguistics. We begin with a reproducible example in R using simulated data to demonstrate how incorrect conclusions can be made from mixed models; this also serves as a primer on contrast coding for statistical novices. We then present an analysis of 3384 papers from the field of psycholinguistics that we coded based upon whether a clear description of contrast coding was present. This analysis demonstrates that the majority of the psycholinguistic literature does not transparently describe contrast coding choices, posing an important challenge to reproducibility and repli­ cability in our field.},
	pages = {104334},
	journaltitle = {Journal of Memory and Language},
	shortjournal = {Journal of Memory and Language},
	author = {Brehm, Laurel and Alday, Phillip M.},
	urldate = {2024-07-25},
	date = {2022-08},
	langid = {english},
	keywords = {contrasts\_coding, letto},
	file = {Brehm e Alday - 2022 - Contrast coding choices in a decade of mixed model.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\FE6EW8VZ\\Brehm e Alday - 2022 - Contrast coding choices in a decade of mixed model.pdf:application/pdf},
}

@article{prasadSTUDYCOGNITIVEPSYCHOLOGY2023,
	title = {{THE} {STUDY} {OF} {COGNITIVE} {PSYCHOLOGY} {IN} {CONJUNCTION} {WITH} {ARTIFICIAL} {INTELLIGENCE}},
	volume = {15},
	issn = {2237-8049},
	url = {https://revistas.unilasalle.edu.br/index.php/conhecimento_diversidade/article/view/10788},
	doi = {10.18316/rcd.v15i36.10788},
	abstract = {The major purpose of this research is to provide a thorough review and analysis of the interplay between artificial intelligence ({AI}) and psychology. I talk about state-of-the-art computer programs that are able to simulate human cognition and behavior (such as Human-Computer Interfaces, models of the mind, and data mining programs). Applications may be broken down into several subcategories and have many different aspects. While developing artificially intelligent robots has been and continues to be the major goal of {AI} research and development, the widespread acceptance and usage of {AI} systems have resulted in a much broader transfer of technology. The article begins with a brief history of cognitive psychology, a discussion of its fundamental ideas and models, and a look at the ways in which the study is connected to artificial intelligence ({AI}). The second part of this article takes a closer look at the difficulties encountered by the field of human-computer interaction, along with its aims, duties, applications, and underlying psychological theories. Multiple scientific, pragmatic, and technical obstacles (complexity problems, disturbing coefficients, etc.) stand in the way of extending or overcoming these limits. We also demonstrate the potential use of mental modeling in the areas of diagnosis, manipulation, and education support in this work. Predictions may be made with the use of data mining, knowledge discovery, or expert systems (for instance, the prognoses of children with mental problems based on their settings). The article reviews the missing features and offers an overview of the coefficients used in the system. Finally, we discuss the application of expert systems and life simulation (applied mental model) in virtual reality to benefit autistic people and their loved ones.},
	pages = {270},
	number = {36},
	journaltitle = {Conhecimento \& Diversidade},
	shortjournal = {{RCD}},
	author = {Prasad, Kdv and Kalavakolanu, Sripathi},
	urldate = {2024-07-25},
	date = {2023-02-28},
	langid = {english},
	keywords = {psicologia\_e\_AI, non\_centra\_con\_la\_tesi},
	file = {Prasad e Kalavakolanu - 2023 - THE STUDY OF COGNITIVE PSYCHOLOGY IN CONJUNCTION W.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\3KFFX2HM\\Prasad e Kalavakolanu - 2023 - THE STUDY OF COGNITIVE PSYCHOLOGY IN CONJUNCTION W.pdf:application/pdf},
}

@article{woodUsingAutomationCombat2019,
	title = {Using automation to combat the replication crisis: A case study from controlled-rearing studies of newborn chicks},
	volume = {57},
	issn = {01636383},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0163638318301437},
	doi = {10.1016/j.infbeh.2019.101329},
	shorttitle = {Using automation to combat the replication crisis},
	abstract = {The accuracy of science depends on the precision of its methods. When fields produce precise measurements, the scientific method can generate remarkable gains in knowledge. When fields produce noisy measurements, however, the scientific method is not guaranteed to work – in fact, noisy measurements are now regarded as a leading cause of the replication crisis in psychology. Scientists should therefore strive to improve the precision of their methods, especially in fields with noisy measurements. Here, we show that automation can reduce measurement error by ∼60\% in one domain of developmental psychology: controlled-rearing studies of newborn chicks. Automated studies produce measurements that are 3–4 times more precise than nonautomated studies and produce effect sizes that are 3–4 times larger than non-automated studies. Automation also eliminates experimenter bias and allows replications to be performed quickly and easily. We suggest that automation can be a powerful tool for improving measurement precision, producing high powered experiments, and combating the replication crisis.},
	pages = {101329},
	journaltitle = {Infant Behavior and Development},
	shortjournal = {Infant Behavior and Development},
	author = {Wood, Samantha M.W. and Wood, Justin N.},
	urldate = {2024-07-25},
	date = {2019-11},
	langid = {english},
	keywords = {riproducibility\_in\_psy\_research, letto},
	file = {Wood e Wood - 2019 - Using automation to combat the replication crisis.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\LDYX95VV\\Wood e Wood - 2019 - Using automation to combat the replication crisis.pdf:application/pdf},
}

@article{roddMovingExperimentalPsychology2024,
	title = {Moving experimental psychology online: How to obtain high quality data when we can’t see our participants},
	volume = {134},
	issn = {0749596X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X23000712},
	doi = {10.1016/j.jml.2023.104472},
	shorttitle = {Moving experimental psychology online},
	abstract = {The past 10 years have seen rapid growth of online (web-based) data collection across the behavioural sciences. Despite the many important contributions of such studies, some researchers have concerns about the reduction in experimental control when research moves outside of laboratory conditions. This paper provides an accessible overview of the issues that can adversely affect data quality in online experiments, with particular focus on cognitive studies of memory and language. I provide checklists for researchers setting up such experiments to help improve data quality. These recommendations focus on three key aspects of experimental design: the technology choices made by researchers and participants, participant recruitment methods, and the performance of participants during experiments. I argue that ensuring high data quality for online experiments requires significant effort prior to data collection to maintain the credibility of our rapidly expanding evidence base. With such safeguards in place, online experiments will continue to provide important, paradigm-changing opportu­ nities across the behavioural sciences.},
	pages = {104472},
	journaltitle = {Journal of Memory and Language},
	shortjournal = {Journal of Memory and Language},
	author = {Rodd, Jennifer M.},
	urldate = {2024-07-25},
	date = {2024-02},
	langid = {english},
	keywords = {riproducibility\_in\_psy\_research, letto},
	file = {Rodd - 2024 - Moving experimental psychology online How to obta.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\A49YKHA6\\Rodd - 2024 - Moving experimental psychology online How to obta.pdf:application/pdf},
}

@article{ioannidisWhyMostPublished2005,
	title = {Why Most Published Research Findings Are False},
	volume = {2},
	issn = {1549-1676},
	url = {https://dx.plos.org/10.1371/journal.pmed.0020124},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {There is increasing concern that most current published research ﬁndings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientiﬁc ﬁeld. In this framework, a research ﬁnding is less likely to be true when the studies conducted in a ﬁeld are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater ﬂexibility in designs, deﬁnitions, outcomes, and analytical modes; when there is greater ﬁnancial and other interest and prejudice; and when more teams are involved in a scientiﬁc ﬁeld in chase of statistical signiﬁcance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientiﬁc ﬁelds, claimed research ﬁndings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research.},
	pages = {e124},
	number = {8},
	journaltitle = {{PLoS} Medicine},
	shortjournal = {{PLoS} Med},
	author = {Ioannidis, John P. A.},
	urldate = {2024-07-25},
	date = {2005-08-30},
	langid = {english},
	keywords = {riproducibility\_in\_psy\_research, letto},
	file = {Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\E7ALHQ4C\\Ioannidis - 2005 - Why Most Published Research Findings Are False.pdf:application/pdf},
}

@article{landersPrimerTheorydrivenWeb2016,
	title = {A primer on theory-driven web scraping: Automatic extraction of big data from the Internet for use in psychological research.},
	volume = {21},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000081},
	doi = {10.1037/met0000081},
	shorttitle = {A primer on theory-driven web scraping},
	abstract = {The term big data encompasses a wide range of approaches of collecting and analyzing data in ways that were not possible before the era of modern personal computing. One approach to big data of great potential to psychologists is web scraping, which involves the automated collection of information from webpages. Although web scraping can create massive big datasets with tens of thousands of variables, it can also be used to create modestly sized, more manageable datasets with tens of variables but hundreds of thousands of cases, well within the skillset of most psychologists to analyze, in a matter of hours. In this article, we demystify web scraping methods as currently used to examine research questions of interest to psychologists. First, we introduce an approach called theory-driven web scraping in which the choice to use web-based big data must follow substantive theory. Second, we introduce data source theories, a term used to describe the assumptions a researcher must make about a prospective big data source in order to meaningfully scrape data from it. Critically, researchers must derive specific hypotheses to be tested based upon their data source theory, and if these hypotheses are not empirically supported, plans to use that data source should be changed or eliminated. Third, we provide a case study and sample code in Python demonstrating how web scraping can be conducted to collect big data along with links to a web tutorial designed for psychologists. Fourth, we describe a 4-step process to be followed in web scraping projects. Fifth and finally, we discuss legal, practical and ethical concerns faced when conducting web scraping projects.},
	pages = {475--492},
	number = {4},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Landers, Richard N. and Brusso, Robert C. and Cavanaugh, Katelyn J. and Collmus, Andrew B.},
	urldate = {2024-07-25},
	date = {2016-12},
	langid = {english},
	keywords = {web\_scraping\_and\_psy},
	file = {Landers et al. - 2016 - A primer on theory-driven web scraping Automatic .pdf:C\:\\Users\\fgfra\\Zotero\\storage\\XJ9N9K4N\\Landers et al. - 2016 - A primer on theory-driven web scraping Automatic .pdf:application/pdf},
}

@article{vezzoliIntroductoryGuideConducting2023,
	title = {An introductory guide for conducting psychological research with big data.},
	volume = {28},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000513},
	doi = {10.1037/met0000513},
	abstract = {Big Data can bring enormous beneﬁts to psychology. However, many psychological researchers show skepticism in undertaking Big Data research. Psychologists often do not take Big Data into consideration while developing their research projects because they have difﬁculties imagining how Big Data could help in their speciﬁc ﬁeld of research, imagining themselves as “Big Data scientists,” or for lack of speciﬁc knowledge. This article provides an introductory guide for conducting Big Data research for psychologists who are considering using this approach and want to have a general idea of its processes. By taking the Knowledge Discovery from Database steps as the ﬁl rouge, we provide useful indications for ﬁnding data suitable for psychological investigations, describe how these data can be preprocessed, and list some techniques to analyze them and programming languages (R and Python) through which all these steps can be realized. In doing so, we explain the concepts with the terminology and take examples from psychology. For psychologists, familiarizing with the language of data science is important because it may appear difﬁcult and esoteric at ﬁrst approach. As Big Data research is often multidisciplinary, this overview helps build a general insight into the research steps and a common language, facilitating collaboration across different ﬁelds.},
	pages = {580--599},
	number = {3},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Vezzoli, Michela and Zogmaister, Cristina},
	urldate = {2024-07-25},
	date = {2023-06},
	langid = {english},
	keywords = {web\_scraping\_and\_psy},
	file = {Vezzoli e Zogmaister - 2023 - An introductory guide for conducting psychological.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\D6WSW2L2\\Vezzoli e Zogmaister - 2023 - An introductory guide for conducting psychological.pdf:application/pdf},
}

@article{adjeridBigDataPsychology2018,
	title = {Big data in psychology: A framework for research advancement.},
	volume = {73},
	issn = {1935-990X, 0003-066X},
	url = {https://doi.apa.org/doi/10.1037/amp0000190},
	doi = {10.1037/amp0000190},
	shorttitle = {Big data in psychology},
	abstract = {The potential for big data to provide value for psychology is significant. However, the pursuit of big data remains an uncertain and risky undertaking for the average psychological researcher. In this article, we address some of this uncertainty by discussing the potential impact of big data on the type of data available for psychological research, addressing the benefits and most significant challenges that emerge from these data, and organizing a variety of research opportunities for psychology. Our article yields two central insights. First, we highlight that big data research efforts are more readily accessible than many researchers realize, particularly with the emergence of open-source research tools, digital platforms, and instrumentation. Second, we argue that opportunities for big data research are diverse and differ both in their fit for varying research goals, as well as in the challenges they bring about. Ultimately, our outlook for researchers in psychology using and benefiting from big data is cautiously optimistic. Although not all big data efforts are suited for all researchers or all areas within psychology, big data research prospects are diverse, expanding, and promising for psychology and related disciplines.},
	pages = {899--917},
	number = {7},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Adjerid, Idris and Kelley, Ken},
	urldate = {2024-07-25},
	date = {2018-10},
	langid = {english},
	keywords = {web\_scraping\_and\_psy},
	file = {Adjerid e Kelley - 2018 - Big data in psychology A framework for research a.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\WXYH5TGD\\Adjerid e Kelley - 2018 - Big data in psychology A framework for research a.pdf:application/pdf},
}

@article{fillingerChallengesBigData2019,
	title = {Challenges of big data integration in the life sciences},
	volume = {411},
	issn = {1618-2642, 1618-2650},
	url = {http://link.springer.com/10.1007/s00216-019-02074-9},
	doi = {10.1007/s00216-019-02074-9},
	abstract = {Big data has been reported to be revolutionizing many areas of life, including science. It summarizes data that is unprecedentedly large, rapidly generated, heterogeneous, and hard to accurately interpret. This availability has also brought new challenges: How to properly annotate data to make it searchable? What are the legal and ethical hurdles when sharing data? How to store data securely, preventing loss and corruption? The life sciences are not the only disciplines that must align themselves with big data requirements to keep up with the latest developments. The large hadron collider, for instance, generates research data at a pace beyond any current biomedical research center. There are three recent major coinciding events that explain the emergence of big data in the context of research: the technological revolution for data generation, the development of tools for data analysis, and a conceptual change towards open science and data. The true potential of big data lies in pattern discovery in large datasets, as well as the formulation of new models and hypotheses. Confirmation of the existence of the Higgs boson, for instance, is one of the most recent triumphs of big data analysis in physics. Digital representations of biological systems have become more comprehensive. This, in combination with advances in machine learning, creates exciting new research possibilities. In this paper, we review the state of big data in bioanalytical research and provide an overview of the guidelines for its proper usage.},
	pages = {6791--6800},
	number = {26},
	journaltitle = {Analytical and Bioanalytical Chemistry},
	shortjournal = {Anal Bioanal Chem},
	author = {Fillinger, Sven and De La Garza, Luis and Peltzer, Alexander and Kohlbacher, Oliver and Nahnsen, Sven},
	urldate = {2024-07-25},
	date = {2019-10},
	langid = {english},
	file = {Fillinger et al. - 2019 - Challenges of big data integration in the life sci.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\H5EMXZYC\\Fillinger et al. - 2019 - Challenges of big data integration in the life sci.pdf:application/pdf},
}

@article{leeUnbalancedDataType2020,
	title = {Unbalanced data, type {II} error, and nonlinearity in predicting M\&A failure},
	volume = {109},
	issn = {0148-2963},
	doi = {10.1016/j.jbusres.2019.11.083},
	abstract = {The traditional forecasting methods in the M\&A data have three limitations: first, the outcome of M\&A deal is an event with a small probability of failure, second, the consequences of misclassifying failure as success are much more severe than those of misclassifying success as failure, and third, the nonlinear and complex nature of the relationship between predictors and M\&A outcome could limit the advantage of logistic regression. To overcome these limitations, we develop a forecasting model that combines two complementary approaches: a generalized logit model framework and a context-specific cost-sensitive function. Our empirical results demonstrate that the proposed approach provides excellent forecasts when compared with traditional forecasting methods. ({PsycInfo} Database Record (c) 2020 {APA}, all rights reserved)},
	pages = {271--287},
	journaltitle = {Journal of Business Research},
	shortjournal = {Journal of Business Research},
	author = {Lee, Kangbok and Joo, Sunghoon and Baik, Hyeoncheol and Han, Sumin and In, Joonhwan},
	date = {2020-03},
	note = {Publisher: Elsevier Science},
	keywords = {Artificial Neural Networks, Generalized logit model, Logistic Regression, Logit and Probit model, Machine learning, Machine Learning, Merger and acquisition, Mergers and Acquisitions, Neural network, Nonlinear Regression, Nonlinearity prediction, Prediction, Probability, Type {II} Errors, Unbalanced data},
}

@article{leeUnbalancedDataType2020a,
	title = {Unbalanced data, type {II} error, and nonlinearity in predicting M\&A failure},
	volume = {109},
	issn = {0148-2963},
	url = {https://www.sciencedirect.com/science/article/pii/S014829631930757X},
	doi = {10.1016/j.jbusres.2019.11.083},
	abstract = {The traditional forecasting methods in the M\&A data have three limitations: first, the outcome of M\&A deal is an event with a small probability of failure, second, the consequences of misclassifying failure as success are much more severe than those of misclassifying success as failure, and third, the nonlinear and complex nature of the relationship between predictors and M\&A outcome could limit the advantage of logistic regression. To overcome these limitations, we develop a forecasting model that combines two complementary approaches: a generalized logit model framework and a context-specific cost-sensitive function. Our empirical results demonstrate that the proposed approach provides excellent forecasts when compared with traditional forecasting methods.},
	pages = {271--287},
	journaltitle = {Journal of Business Research},
	shortjournal = {Journal of Business Research},
	author = {Lee, Kangbok and Joo, Sunghoon and Baik, Hyeoncheol and Han, Sumin and In, Joonhwan},
	urldate = {2024-07-26},
	date = {2020-03-01},
	keywords = {Generalized logit model, Logit and Probit model, Machine learning, Merger and acquisition, Neural network, Nonlinearity prediction, Unbalanced data},
	file = {ScienceDirect Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\VMRIKID4\\S014829631930757X.html:text/html},
}

@article{leeUnbalancedDataType2020b,
	title = {Unbalanced data, type {II} error, and nonlinearity in predicting M\&A failure},
	volume = {109},
	issn = {01482963},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S014829631930757X},
	doi = {10.1016/j.jbusres.2019.11.083},
	abstract = {The traditional forecasting methods in the M\&A data have three limitations: first, the outcome of M\&A deal is an event with a small probability of failure, second, the consequences of misclassifying failure as success are much more severe than those of misclassifying success as failure, and third, the nonlinear and complex nature of the relationship between predictors and M\&A outcome could limit the advantage of logistic regression. To overcome these limitations, we develop a forecasting model that combines two complementary approaches: a generalized logit model framework and a context-specific cost-sensitive function. Our empirical results demonstrate that the proposed approach provides excellent forecasts when compared with traditional forecasting methods.},
	pages = {271--287},
	journaltitle = {Journal of Business Research},
	shortjournal = {Journal of Business Research},
	author = {Lee, Kangbok and Joo, Sunghoon and Baik, Hyeoncheol and Han, Sumin and In, Joonhwan},
	urldate = {2024-07-26},
	date = {2020-03},
	langid = {english},
	keywords = {unbalanced\_data},
	file = {Lee et al. - 2020 - Unbalanced data, type II error, and nonlinearity i.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\8AQ2VHU2\\Lee et al. - 2020 - Unbalanced data, type II error, and nonlinearity i.pdf:application/pdf},
}

@article{marksAnalysisVarianceTechniques1974,
	title = {Analysis of variance techniques for unbalanced data},
	volume = {44},
	issn = {0034-6543},
	doi = {10.2307/1169980},
	abstract = {Suggests that the application of a linear model to unbalanced data requires a thorough understanding of the nature of the substantive problem, the data, and the methodology employed, and great care in framing and interpreting hypotheses. (25 ref) ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {351--364},
	number = {3},
	journaltitle = {Review of Educational Research},
	shortjournal = {Review of Educational Research},
	author = {Marks, Edmond},
	date = {1974},
	note = {Publisher: American Educational Research Assn},
	keywords = {Analysis of Variance, analysis of variance techniques for unbalanced data},
}

@article{marksAnalysisVarianceTechniques1974a,
	title = {Analysis of Variance Techniques for Unbalanced Data},
	volume = {44},
	issn = {0034-6543},
	url = {https://www.jstor.org/stable/1169980},
	doi = {10.2307/1169980},
	pages = {351--364},
	number = {3},
	journaltitle = {Review of Educational Research},
	author = {Marks, Edmond},
	urldate = {2024-07-26},
	date = {1974},
	note = {Publisher: [Sage Publications, Inc., American Educational Research Association]},
}

@article{hinchenMultipleRegressionUnbalanced1970,
	title = {Multiple Regression with Unbalanced Data},
	rights = {Copyright 1970 American Society for Quality Control, Inc.},
	issn = {0022-4065},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00224065.1970.11980404},
	journaltitle = {Journal of Quality Technology},
	author = {Hinchen, John D.},
	urldate = {2024-07-26},
	date = {1970-01-01},
	note = {Publisher: Taylor \& Francis},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\3CPMZADP\\00224065.1970.html:text/html},
}

@article{hinchenMultipleRegressionUnbalanced1970a,
	title = {Multiple Regression with Unbalanced Data},
	volume = {2},
	issn = {0022-4065},
	url = {https://doi.org/10.1080/00224065.1970.11980404},
	doi = {10.1080/00224065.1970.11980404},
	abstract = {Regression analysis has become a very popular method for analyzing existing plant and research data. The existence of correlations among the independent variables is recognized as a major pitfall in the proper interpretation of such data. This paper outlines the errors in interpretation that may result when specific degrees of correlation exist. The situation is examined for a two-variable case, fitting first and second degree models to data from sources of varying complexity. A computer was used to generate and analyze the data.},
	pages = {22--29},
	number = {1},
	journaltitle = {Journal of Quality Technology},
	author = {Hinchen, John D.},
	urldate = {2024-07-26},
	date = {1970-01-01},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00224065.1970.11980404},
	keywords = {Experimental Design, Multiple Regression Analysis},
}

@article{wolinsReviewAnalysisUnbalanced1983,
	title = {Review of Analysis of Unbalanced Data: A Pre-Program Introduction.},
	volume = {28},
	issn = {00107549},
	url = {http://access.portico.org/stable?au=phzkkd62j},
	doi = {10.1037/021750},
	shorttitle = {Review of Analysis of Unbalanced Data},
	pages = {883--884},
	number = {11},
	journaltitle = {Contemporary Psychology: A Journal of Reviews},
	author = {Wolins, Leroy},
	urldate = {2024-07-26},
	date = {1983-11},
}

@article{speedMethodsAnalysisLinear1978,
	title = {Methods of Analysis of Linear Models with Unbalanced Data},
	volume = {73},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2286530},
	doi = {10.2307/2286530},
	abstract = {The objective of this article is to review existing methods for analyzing experimental design models with unbalanced data and to relate them to existing computer programs. The methods are distinguished by the hypotheses associated with the sums of squares which are generated. The choice of a method should be based on the appropriateness of the hypothesis rather than on computational convenience or the orthogonality of the quadratic forms. The sums of squares are described using the R() notation as applied to the over-parameterized linear model, but the hypotheses are stated in terms of the full-rank cell means model. The zero-cell frequency situation is treated briefly.},
	pages = {105--112},
	number = {361},
	journaltitle = {Journal of the American Statistical Association},
	author = {Speed, F. M. and Hocking, R. R. and Hackney, O. P.},
	urldate = {2024-07-26},
	date = {1978},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	keywords = {unbalanced\_data},
	file = {JSTOR Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\G7D3EE9B\\Speed et al. - 1978 - Methods of Analysis of Linear Models with Unbalanc.pdf:application/pdf},
}

@article{shawAnovaUnbalancedData1993,
	title = {Anova for Unbalanced Data: An Overview},
	volume = {74},
	issn = {0012-9658},
	url = {https://www.jstor.org/stable/1939922},
	doi = {10.2307/1939922},
	shorttitle = {Anova for Unbalanced Data},
	abstract = {Ecological studies typically involve comparison of biological responses among a variety of environmental conditions. When the response variables have continuous distributions and the conditions are discrete, whether inherently or by design, then it is appropriate to analyze the data using analysis of variance ({ANOVA}). When data conform to a complete, balanced design (equal numbers of observations in each experimental treatment), it is straightforward to conduct an {ANOVA}, particularly with the aid of the numerous statistical computing packages that are available. Interpretation of an {ANOVA} of balanced data is also unambiguous. Unfortunately, for a variety of reasons, it is rare that a practicing ecologist embarks on an analysis of data that are completely balanced. Regardless of its cause, lack of balance necessitates care in the analysis and interpretation. In this paper, our aims is to provide an overview of the consequences of lack of balance and to give some guidelines to analyzing unbalanced data for models involving fixed effects. Our treatment is necessarily cursory and will not substitute for training available from a sequence of courses in mathematical statistics and linear models. It is intended to introduce the reader to the main issues and to the extensive statistical literature that deals with them.},
	pages = {1638--1645},
	number = {6},
	journaltitle = {Ecology},
	author = {Shaw, Ruth G. and Mitchell-Olds, Thomas},
	urldate = {2024-07-26},
	date = {1993},
	note = {Publisher: Ecological Society of America},
	keywords = {unbalanced\_data},
	file = {JSTOR Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\FNV3H6ZK\\Shaw e Mitchell-Olds - 1993 - Anova for Unbalanced Data An Overview.pdf:application/pdf},
}

@incollection{jebbExploratoryDataAnalysis2024,
	location = {Washington, {DC}, {US}},
	title = {Exploratory data analysis as a foundation of inductive research},
	isbn = {978-1-4338-4408-9 978-1-4338-4409-6},
	abstract = {In organizational science, a great deal of emphasis is placed on the importance of causal explanations, or theories. The starting point of theory can be said to lie in first identifying phenomena to explain. This scientific activity, phenomenon detection, is fundamental to scientific progress. This chapter introduces exploratory data analysis ({EDA}) for more effective phenomenon detection within organizational science. It begins with a formal introduction to {EDA} and a description of its history and underlying philosophy. The chapter then focuses on theoretical issues related to the role of {EDA} within organizational science and is divided into two major sections. The first addresses the relationship between {EDA} and research credibility. It discusses how {EDA}, when properly understood and practiced, encourages scientific replication. The second major section highlights one of the chief benefits of {EDA}: that it maximizes the value of the data. ({PsycInfo} Database Record (c) 2024 {APA}, all rights reserved)},
	pages = {509--530},
	booktitle = {Methodological issues and strategies in clinical research, 5th ed},
	publisher = {American Psychological Association},
	author = {Jebb, Andrew T. and Parrigon, Scott and Woo, Sang Eun},
	date = {2024},
	doi = {10.1037/0000409-022},
	keywords = {Credibility, Experimental Replication, Experimentation, Industrial and Organizational Psychology, Statistical Analysis},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\BM3RWD2Q\\2024-97080-022.html:text/html},
}

@article{fifeUnderstandingExploratoryConfirmatory2022,
	title = {Understanding the exploratory/confirmatory data analysis continuum: Moving beyond the 'replication crisis'},
	volume = {77},
	issn = {0003-066X},
	url = {https://search.ebscohost.com/login.aspx?direct=true&db=pdh&AN=2022-04805-001&site=ehost-live},
	doi = {10.1037/amp0000886},
	shorttitle = {Understanding the exploratory/confirmatory data analysis continuum},
	abstract = {In light of the 'replication crisis,' some advocate for stricter standards and greater transparency in research methods. These efforts push toward a data analysis approach called 'confirmatory data analysis' ({CDA}; see Wagenmakers et al., 2012). However, some (e.g., Baumeister, 2016; Goldin-Meadow, 2016) suggest that emphasizing {CDA} may restrict creativity and discovery. These scholars argued (sometimes inadvertently) for greater freedom to pursue 'exploratory data analysis' ({EDA}; see Tukey, 1977). Ironically and unfortunately, many who push against stricter {CDA} standards do not realize {EDA} exists, or misunderstand the philosophy and proper tools for exploration. In this article, the meaning, tools, philosophy, and ethics associated with {EDA}, {CDA}, and a relatively unknown but important approach called 'rough {CDA}' are clarified. Important distinctions are developed between {EDA}/rough {CDA}/{CDA} and other (some problematic) analysis activities including p-hacking, {HARKing}, and data mining, which are situated in a (graphical) framework that clarifies relationships and ethical boundaries with each. In short, the proper data analytic approach depends on (a) intentions and (b) transparency. Most psychological research is not at a maturity level to justify {CDA}; researchers have historically used tools mismatched to their research agenda. In the conclusion, recommendations are presented about how these typologies can be integrated into graduate training programs and how a cumulative research program can help psychology move beyond the replication crisis. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {453--466},
	number = {3},
	journaltitle = {American Psychologist},
	shortjournal = {American Psychologist},
	author = {Fife, Dustin A. and Rodgers, Joseph Lee},
	urldate = {2024-07-26},
	date = {2022-04},
	note = {Publisher: American Psychological Association},
	keywords = {Experimental Replication, Statistical Analysis, {CDA}, Crises, Ethics, exploratory data analysis, Graduate Psychology Education, Methodology, p-hacking, Philosophies, replication crisis, rough {CDA}, exploratory\_data\_analysis, letto},
	file = {EBSCO Full Text:C\:\\Users\\fgfra\\Zotero\\storage\\YKAI8FND\\Fife e Rodgers - 2022 - Understanding the exploratoryconfirmatory data an.pdf:application/pdf},
}

@article{jebbExploratoryDataAnalysis2017,
	title = {Exploratory data analysis as a foundation of inductive research},
	volume = {27},
	issn = {1053-4822},
	url = {https://www.sciencedirect.com/science/article/pii/S1053482216300353},
	doi = {10.1016/j.hrmr.2016.08.003},
	abstract = {Across academic disciplines, scientific progress is maximized when there is a balance between deductive and inductive approaches. To promote this balance in organizational science, rigorous inductive research aimed at phenomenon detection must be further encouraged. To this end, the present article discusses the logic and methods of exploratory data analysis ({EDA}), the mode of analysis concerned with discovery, exploration, and empirically detecting phenomena in data. We begin by first describing the historical and conceptual background of {EDA}. We then discuss two issues related to {EDA} and its relationship to scientific credibility. First, we argue that {EDA} fosters a replication-based science by requiring cross-validation and by emphasizing the natural uncertainty of data patterns. Second, we clarify that {EDA} is distinguishable from other exploratory practices that are considered scientifically questionable (e.g., “p-hacking”, “data fishing” and “data-dredging”). In the following section of the paper, we present a final argument for {EDA}: that it helps maximize the value of data. To illustrate this point, we present several graphical methods for detecting data patterns and provide references to further techniques for the interested reader.},
	pages = {265--276},
	number = {2},
	journaltitle = {Human Resource Management Review},
	shortjournal = {Human Resource Management Review},
	author = {Jebb, Andrew T. and Parrigon, Scott and Woo, Sang Eun},
	urldate = {2024-07-26},
	date = {2017-06-01},
	keywords = {Exploratory data analysis, Inductive, Phenomenon detection, Replication, Visualization},
	file = {ScienceDirect Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\E64DEIUY\\S1053482216300353.html:text/html},
}

@article{jebbExploratoryDataAnalysis2017a,
	title = {Exploratory data analysis as a foundation of inductive research},
	volume = {27},
	issn = {10534822},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053482216300353},
	doi = {10.1016/j.hrmr.2016.08.003},
	abstract = {Across academic disciplines, scientiﬁc progress is maximized when there is a balance between deductive and inductive approaches. To promote this balance in organizational science, rigorous inductive research aimed at phenomenon detection must be further encouraged. To this end, the present article discusses the logic and methods of exploratory data analysis ({EDA}), the mode of analysis concerned with discovery, exploration, and empirically detecting phenomena in data. We begin by ﬁrst describing the historical and conceptual background of {EDA}. We then discuss two issues related to {EDA} and its relationship to scientiﬁc credibility. First, we argue that {EDA} fosters a replication-based science by requiring cross-validation and by emphasizing the natural uncertainty of data patterns. Second, we clarify that {EDA} is distinguishable from other exploratory practices that are considered scientiﬁcally questionable (e.g., “p-hacking”, “data ﬁshing” and “data-dredging”). In the following section of the paper, we present a ﬁnal argument for {EDA}: that it helps maximize the value of data. To illustrate this point, we present several graphical methods for detecting data patterns and provide references to further techniques for the interested reader.},
	pages = {265--276},
	number = {2},
	journaltitle = {Human Resource Management Review},
	shortjournal = {Human Resource Management Review},
	author = {Jebb, Andrew T. and Parrigon, Scott and Woo, Sang Eun},
	urldate = {2024-07-26},
	date = {2017-06},
	langid = {english},
	keywords = {exploratory\_data\_analysis},
	file = {Jebb et al. - 2017 - Exploratory data analysis as a foundation of induc.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\P553IU6E\\Jebb et al. - 2017 - Exploratory data analysis as a foundation of induc.pdf:application/pdf},
}

@online{APAPsycNetRecord,
	title = {{APA} {PsycNet} Record Access},
	url = {https://psycnet.apa.org/recordAccess/institutional/2014-21924-001?returnUrl=https%253A%252F%252Fpsycnet.apa.org%252FdoiLanding%253Fdoi%253D10.1037%25252Fa0036850},
	urldate = {2024-07-26},
	file = {APA PsycNet Record Access:C\:\\Users\\fgfra\\Zotero\\storage\\BHAZIZWV\\2014-21924-001.html:text/html},
}

@article{harpoleHowBandwidthSelection2014a,
	title = {How bandwidth selection algorithms impact exploratory data analysis using kernel density estimation.},
	volume = {19},
	issn = {1939-1463, 1082-989X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0036850},
	doi = {10.1037/a0036850},
	abstract = {Exploratory data analysis ({EDA}) can reveal important features of underlying distributions, and these features often have an impact on inferences and conclusions drawn from data. Graphical analysis is central to {EDA}, and graphical representations of distributions often benefit from smoothing. A viable method of estimating and graphing the underlying density in {EDA} is kernel density estimation ({KDE}). This article provides an introduction to {KDE} and examines alternative methods for specifying the smoothing bandwidth in terms of their ability to recover the true density. We also illustrate the comparison and use of {KDE} methods with 2 empirical examples. Simulations were carried out in which we compared 8 bandwidth selection methods (Sheather–Jones plug-in [{SJDP}], normal rule of thumb, Silverman’s rule of thumb, least squares cross-validation, biased cross-validation, and 3 adaptive kernel estimators) using 5 true density shapes (standard normal, positively skewed, bimodal, skewed bimodal, and standard lognormal) and 9 sample sizes (15, 25, 50, 75, 100, 250, 500, 1,000, 2,000). Results indicate that, overall, {SJDP} outperformed all methods. However, for smaller sample sizes (25 to 100) either biased cross-validation or Silverman’s rule of thumb was recommended, and for larger sample sizes the adaptive kernel estimator with {SJDP} was recommended. Information is provided about implementing the recommendations in the R computing language.},
	pages = {428--443},
	number = {3},
	journaltitle = {Psychological Methods},
	shortjournal = {Psychological Methods},
	author = {Harpole, Jared K. and Woods, Carol M. and Rodebaugh, Thomas L. and Levinson, Cheri A. and Lenze, Eric J.},
	urldate = {2024-07-26},
	date = {2014-09},
	langid = {english},
	file = {Harpole et al. - 2014 - How bandwidth selection algorithms impact explorat.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\QNQ3HM3R\\Harpole et al. - 2014 - How bandwidth selection algorithms impact explorat.pdf:application/pdf},
}

@article{leeOrdinaryLeastSquares2012,
	title = {Ordinary Least Squares Estimation of Parameters in Exploratory Factor Analysis With Ordinal Data},
	volume = {47},
	issn = {0027-3171},
	url = {https://doi.org/10.1080/00273171.2012.658340},
	doi = {10.1080/00273171.2012.658340},
	abstract = {Exploratory factor analysis ({EFA}) is often conducted with ordinal data (e.g., items with 5-point responses) in the social and behavioral sciences. These ordinal variables are often treated as if they were continuous in practice. An alternative strategy is to assume that a normally distributed continuous variable underlies each ordinal variable. The {EFA} model is specified for these underlying continuous variables rather than the observed ordinal variables. Although these underlying continuous variables are not observed directly, their correlations can be estimated from the ordinal variables. These correlations are referred to as polychoric correlations. This article is concerned with ordinary least squares ({OLS}) estimation of parameters in {EFA} with polychoric correlations. Standard errors and confidence intervals for rotated factor loadings and factor correlations are presented. {OLS} estimates and the associated standard error estimates and confidence intervals are illustrated using personality trait ratings from 228 college students. Statistical properties of the proposed procedure are explored using a Monte Carlo study. The empirical illustration and the Monte Carlo study showed that (a) {OLS} estimation of {EFA} is feasible with large models, (b) point estimates of rotated factor loadings are unbiased, (c) point estimates of factor correlations are slightly negatively biased with small samples, and (d) standard error estimates and confidence intervals perform satisfactorily at moderately large samples.},
	pages = {314--339},
	number = {2},
	journaltitle = {Multivariate Behavioral Research},
	author = {Lee, Chun-Ting and Zhang, Guangjian and Edwards, Michael C.},
	urldate = {2024-07-26},
	date = {2012-03-30},
	pmid = {26734852},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00273171.2012.658340},
}

@article{leeOrdinaryLeastSquares2012a,
	title = {Ordinary Least Squares Estimation of Parameters in Exploratory Factor Analysis With Ordinal Data},
	volume = {47},
	issn = {0027-3171},
	url = {https://doi.org/10.1080/00273171.2012.658340},
	doi = {10.1080/00273171.2012.658340},
	abstract = {Exploratory factor analysis ({EFA}) is often conducted with ordinal data (e.g., items with 5-point responses) in the social and behavioral sciences. These ordinal variables are often treated as if they were continuous in practice. An alternative strategy is to assume that a normally distributed continuous variable underlies each ordinal variable. The {EFA} model is specified for these underlying continuous variables rather than the observed ordinal variables. Although these underlying continuous variables are not observed directly, their correlations can be estimated from the ordinal variables. These correlations are referred to as polychoric correlations. This article is concerned with ordinary least squares ({OLS}) estimation of parameters in {EFA} with polychoric correlations. Standard errors and confidence intervals for rotated factor loadings and factor correlations are presented. {OLS} estimates and the associated standard error estimates and confidence intervals are illustrated using personality trait ratings from 228 college students. Statistical properties of the proposed procedure are explored using a Monte Carlo study. The empirical illustration and the Monte Carlo study showed that (a) {OLS} estimation of {EFA} is feasible with large models, (b) point estimates of rotated factor loadings are unbiased, (c) point estimates of factor correlations are slightly negatively biased with small samples, and (d) standard error estimates and confidence intervals perform satisfactorily at moderately large samples.},
	pages = {314--339},
	number = {2},
	journaltitle = {Multivariate Behavioral Research},
	author = {Lee, Chun-Ting and Zhang, Guangjian and Edwards, Michael C.},
	urldate = {2024-07-26},
	date = {2012-03-30},
	pmid = {26734852},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/00273171.2012.658340},
}

@incollection{behrensExploratoryDataAnalysis2000,
	location = {Washington, {DC}, {US}},
	title = {Exploratory data analysis},
	isbn = {978-1-55798-652-8},
	abstract = {Discusses Exploratory Data Analysis ({EDA}) developed by John Tukey with the aim of understanding data in the broadest possible set of circumstances in psychological research. ({PsycInfo} Database Record (c) 2022 {APA}, all rights reserved)},
	pages = {303--305},
	booktitle = {Encyclopedia of Psychology, Vol. 3.},
	publisher = {American Psychological Association},
	author = {Behrens, John T.},
	date = {2000},
	doi = {10.1037/10518-112},
	keywords = {Data Processing},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\WXSH43VU\\2004-12701-112.html:text/html},
}

@article{armstrongLinearProgrammingExploratory1980,
	title = {Linear Programming in Exploratory Data Analysis},
	volume = {5},
	issn = {0362-9791},
	url = {https://www.jstor.org/stable/1164904},
	doi = {10.2307/1164904},
	abstract = {It has long been popular to utilize the least squares estimation procedure for fitting the multiple linear regression model to observed data. In this paper, two useful alternatives to least squares (L$_{\textrm{2}}$ norm) estimation in exploratory data analysis are examined: least absolute value estimation (L$_{\textrm{1}}$ norm) and Chebychev (L$_{\textrm{∞}}$ norm) estimation. Formulating the L$_{\textrm{1}}$ norm and L$_{\textrm{∞}}$ norm problems as linear programming problems offers several advantages, including efficient solution methods using special-purpose computer codes. An example is provided in which the three procedures are used to fit a line, both with and without an outlier present in the data.},
	pages = {293--307},
	number = {4},
	journaltitle = {Journal of Educational Statistics},
	author = {Armstrong, Ronald D. and Frome, Edward L. and Sklar, Michael G.},
	urldate = {2024-07-26},
	date = {1980},
	note = {Publisher: [Sage Publications, Inc., American Educational Research Association, American Statistical Association]},
}

@article{goodPhilosophyExploratoryData1983,
	title = {The Philosophy of Exploratory Data Analysis},
	volume = {50},
	issn = {0031-8248},
	url = {https://www.jstor.org/stable/188015},
	abstract = {This paper attempts to define Exploratory Data Analysis ({EDA}) more precisely than usual, and to produce the beginnings of a philosophy of this topical and somewhat novel branch of statistics. A data set is, roughly speaking, a collection of k-tuples for some k. In both descriptive statistics and in {EDA}, these k-tuples, or functions of them, are represented in a manner matched to human and computer abilities with a view to finding patterns that are not "kinkera". A kinkus is a pattern that has a negligible probability of being even partly potentially explicable. A potentially explicable pattern is one for which there probably exists a hypothesis of adequate "explicativity", which is another technical probabilistic concept. A pattern can be judged to be probably potentially explicable even if we cannot find an explanation. The theory of probability understood here is one of partially ordered (interval-valued), subjective (personal) probabilities. Among other topics relevant to a philosophy of {EDA} are the "reduction" of data; Francis Bacon's philosophy of science; the automatic formulation of hypotheses; successive deepening of hypotheses; neurophysiology; and rationality of type {II}.},
	pages = {283--295},
	number = {2},
	journaltitle = {Philosophy of Science},
	author = {Good, I. J.},
	urldate = {2024-07-26},
	date = {1983},
	note = {Publisher: [The University of Chicago Press, Philosophy of Science Association]},
	keywords = {exploratory\_data\_analysis, letto},
	file = {JSTOR Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\JIA7L2MX\\Good - 1983 - The Philosophy of Exploratory Data Analysis.pdf:application/pdf},
}

@article{bujaStatisticalInferenceExploratory2009,
	title = {Statistical Inference for Exploratory Data Analysis and Model Diagnostics},
	volume = {367},
	issn = {1364-503X},
	url = {https://www.jstor.org/stable/40485732},
	abstract = {We propose to furnish visual statistical methods with an inferential framework and protocol, modelled on confirmatory statistical testing. In this framework, plots take on the role of test statistics, and human cognition the role of statistical tests. Statistical significance of ' discoveries' is measured by having the human viewer compare the plot of the real dataset with collections of plots of simulated datasets. A simple but rigorous protocol that provides inferential validity is modelled after the 'lineup' popular from criminal legal procedures. Another protocol modelled after the 'Rorschach' inkblot test, well known from (pop-) psychology, will help analysts acclimatize to random variability before being exposed to the plot of the real data. The proposed protocols will be useful for exploratory data analysis, with reference datasets simulated by using a null assumption that structure is absent. The framework is also useful for model diagnostics in which case reference datasets are simulated from the model in question. This latter point follows up on previous proposals. Adopting the protocols will mean an adjustment in working procedures for data analysts, adding more rigour, and teachers might find that incorporating these protocols into the curriculum improves their students' statistical thinking.},
	pages = {4361--4383},
	number = {1906},
	journaltitle = {Philosophical Transactions: Mathematical, Physical and Engineering Sciences},
	author = {Buja, Andreas and Cook, Dianne and Hofmann, Heike and Lawrence, Michael and Lee, Eun-Kyung and Swayne, Deborah F. and Wlckham, Hadley},
	urldate = {2024-07-26},
	date = {2009},
	note = {Publisher: Royal Society},
	keywords = {exploratory\_data\_analysis, letto},
	file = {JSTOR Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\W6TBR8BF\\Buja et al. - 2009 - Statistical Inference for Exploratory Data Analysi.pdf:application/pdf},
}

@article{floraOldNewIdeas2012,
	title = {Old and New Ideas for Data Screening and Assumption Testing for Exploratory and Confirmatory Factor Analysis},
	volume = {3},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2012.00055/full},
	doi = {10.3389/fpsyg.2012.00055},
	abstract = {{\textless}p{\textgreater}We provide a basic review of the data screening and assumption testing issues relevant to exploratory and confirmatory factor analysis along with practical advice for conducting analyses that are sensitive to these concerns. Historically, factor analysis was developed for explaining the relationships among many continuous test scores, which led to the expression of the common factor model as a multivariate linear regression model with observed, continuous variables serving as dependent variables, and unobserved factors as the independent, explanatory variables. Thus, we begin our paper with a review of the assumptions for the common factor model and data screening issues as they pertain to the factor analysis of continuous observed variables. In particular, we describe how principles from regression diagnostics also apply to factor analysis. Next, because modern applications of factor analysis frequently involve the analysis of the individual items from a single test or questionnaire, an important focus of this paper is the factor analysis of items. Although the traditional linear factor model is well-suited to the analysis of continuously distributed variables, commonly used item types, including Likert-type items, almost always produce dichotomous or ordered categorical variables. We describe how relationships among such items are often not well described by product-moment correlations, which has clear ramifications for the traditional linear factor analysis. An alternative, non-linear factor analysis using polychoric correlations has become more readily available to applied researchers and thus more popular. Consequently, we also review the assumptions and data-screening issues involved in this method. Throughout the paper, we demonstrate these procedures using an historic data set of nine cognitive ability variables.{\textless}/p{\textgreater}},
	journaltitle = {Frontiers in Psychology},
	shortjournal = {Front. Psychol.},
	author = {Flora, David B. and {LaBrish}, Cathy and Chalmers, R. P.},
	urldate = {2024-07-26},
	date = {2012-03-01},
	note = {Publisher: Frontiers},
	keywords = {assumption testing, confirmatory factor analysis, data screening, exploratory factor analysis, item factor analysis, regression diagnostics, Structural Equation Modeling, exploratory\_data\_analysis, letto},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\GQT54UKY\\Flora et al. - 2012 - Old and New Ideas for Data Screening and Assumptio.pdf:application/pdf},
}

@article{clydeBayesianAdaptiveSampling2011,
	title = {Bayesian Adaptive Sampling for Variable Selection and Model Averaging},
	volume = {20},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.09049},
	doi = {10.1198/jcgs.2010.09049},
	pages = {80--101},
	number = {1},
	journaltitle = {Journal of Computational and Graphical Statistics},
	shortjournal = {Journal of Computational and Graphical Statistics},
	author = {Clyde, Merlise A. and Ghosh, Joyee and Littman, Michael L.},
	urldate = {2024-07-26},
	date = {2011-01},
	langid = {english},
}

@article{clydeBayesianAdaptiveSampling2011a,
	title = {Bayesian Adaptive Sampling for Variable Selection and Model Averaging},
	volume = {20},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.09049},
	doi = {10.1198/jcgs.2010.09049},
	pages = {80--101},
	number = {1},
	journaltitle = {Journal of Computational and Graphical Statistics},
	shortjournal = {Journal of Computational and Graphical Statistics},
	author = {Clyde, Merlise A. and Ghosh, Joyee and Littman, Michael L.},
	urldate = {2024-07-26},
	date = {2011-01},
	langid = {english},
	keywords = {Bayesian\_adaptive\_sampling},
	file = {Clyde et al. - 2011 - Bayesian Adaptive Sampling for Variable Selection .pdf:C\:\\Users\\fgfra\\Zotero\\storage\\G6LAYS8Y\\Clyde et al. - 2011 - Bayesian Adaptive Sampling for Variable Selection .pdf:application/pdf},
}

@article{nottAdaptiveSamplingBayesian2005,
	title = {Adaptive Sampling for Bayesian Variable Selection},
	volume = {92},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/20441233},
	abstract = {Our paper proposes adaptive Monte Carlo sampling schemes for Bayesian variable selection in linear regression that improve on standard Markov chain methods. We do so by considering Metropolis-Hastings proposals that make use of accumulated information about the posterior distribution obtained during sampling. Adaptation needs to be done carefully to ensure that sampling is from the correct ergodic distribution. We give conditions for the validity of an adaptive sampling scheme in this problem, and for simulating from a distribution on a finite state space in general, and suggest a class of adaptive proposal densities which uses best linear prediction to approximate the Gibbs sampler. Our sampling scheme is computationally much faster per iteration than the Gibbs sampler, and when this is taken into account the efficiency gains when using our sampling scheme compared to alternative approaches are substantial in terms of precision of estimation of posterior quantities of interest for a given amount of computation time. We compare our method with other sampling schemes for examples involving both real and simulated data. The methodology developed in the paper can be extended to variable selection in more general problems.},
	pages = {747--763},
	number = {4},
	journaltitle = {Biometrika},
	author = {Nott, David J. and Kohn, Robert},
	urldate = {2024-07-26},
	date = {2005},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	keywords = {Bayesian\_adaptive\_sampling},
	file = {JSTOR Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\TYDJERM9\\Nott e Kohn - 2005 - Adaptive Sampling for Bayesian Variable Selection.pdf:application/pdf},
}

@article{martsolfAdaptiveSamplingRecruiting2006,
	title = {Adaptive Sampling: Recruiting a Diverse Community Sample of Survivors of Sexual Violence},
	volume = {23},
	issn = {0737-0016},
	url = {https://doi.org/10.1207/s15327655jchn2303_4},
	doi = {10.1207/s15327655jchn2303_4},
	shorttitle = {Adaptive Sampling},
	abstract = {Accessing vulnerable and hard-to-reach populations is a significant challenge for nurse researchers. Adaptive sampling is a procedure that has been used effectively in community-based research to recruit rare or hidden populations. Structured community assessment, as practiced by community health nurses, can be used to enhance adaptive sampling procedures to recruit research participants. This article1 describes adaptive sampling techniques, discusses how the techniques can be enhanced with a structured nursing community assessment, and describes how adaptive sampling was used successfully by nurse researchers to obtain a diverse and vulnerable community sample for a grounded-theory study of women's and men's responses to sexual violence.},
	pages = {169--182},
	number = {3},
	journaltitle = {Journal of Community Health Nursing},
	author = {Martsolf, Donna S. and Courey, Tamra J. and Chapman, Terri R. and Draucker, Claire B. and Mims, Barbara L.},
	urldate = {2024-07-26},
	date = {2006-08-01},
	pmid = {16863402},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1207/s15327655jchn2303\_4},
}

@article{bendaResponseStimulusAdaptive2007,
	title = {From response to stimulus: adaptive sampling in sensory physiology},
	volume = {17},
	issn = {0959-4388},
	url = {https://www.sciencedirect.com/science/article/pii/S095943880700092X},
	doi = {10.1016/j.conb.2007.07.009},
	series = {Sensory systems},
	shorttitle = {From response to stimulus},
	abstract = {Sensory systems extract behaviorally relevant information from a continuous stream of complex high-dimensional input signals. Understanding the detailed dynamics and precise neural code, even of a single neuron, is therefore a non-trivial task. Automated closed-loop approaches that integrate data analysis in the experimental design ease the investigation of sensory systems in three directions: First, adaptive sampling speeds up the data acquisition and thus increases the yield of an experiment. Second, model-driven stimulus exploration improves the quality of experimental data needed to discriminate between alternative hypotheses. Third, information-theoretic data analyses open up novel ways to search for those stimuli that are most efficient in driving a given neuron in terms of its firing rate or coding quality. Examples from different sensory systems show that, in all three directions, substantial progress can be achieved once rapid online data analysis, adaptive sampling, and computational modeling are tightly integrated into experiments.},
	pages = {430--436},
	number = {4},
	journaltitle = {Current Opinion in Neurobiology},
	shortjournal = {Current Opinion in Neurobiology},
	author = {Benda, Jan and Gollisch, Tim and Machens, Christian K and Herz, Andreas {VM}},
	urldate = {2024-07-26},
	date = {2007-08-01},
	file = {ScienceDirect Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\8LAMC7YS\\S095943880700092X.html:text/html},
}

@article{bendaResponseStimulusAdaptive2007a,
	title = {From response to stimulus: adaptive sampling in sensory physiology},
	volume = {17},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {09594388},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S095943880700092X},
	doi = {10.1016/j.conb.2007.07.009},
	shorttitle = {From response to stimulus},
	pages = {430--436},
	number = {4},
	journaltitle = {Current Opinion in Neurobiology},
	shortjournal = {Current Opinion in Neurobiology},
	author = {Benda, Jan and Gollisch, Tim and Machens, Christian K and Herz, Andreas Vm},
	urldate = {2024-07-26},
	date = {2007-08},
	langid = {english},
	keywords = {Bayesian\_adaptive\_sampling},
	file = {Benda et al. - 2007 - From response to stimulus adaptive sampling in se.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\4Q2YJJK9\\Benda et al. - 2007 - From response to stimulus adaptive sampling in se.pdf:application/pdf},
}

@article{engemannAutomatedModelSelection2015,
	title = {Automated model selection in covariance estimation and spatial whitening of {MEG} and {EEG} signals},
	volume = {108},
	issn = {1053-8119},
	url = {https://www.sciencedirect.com/science/article/pii/S1053811914010325},
	doi = {10.1016/j.neuroimage.2014.12.040},
	abstract = {Magnetoencephalography and electroencephalography (M/{EEG}) measure non-invasively the weak electromagnetic fields induced by post-synaptic neural currents. The estimation of the spatial covariance of the signals recorded on M/{EEG} sensors is a building block of modern data analysis pipelines. Such covariance estimates are used in brain–computer interfaces ({BCI}) systems, in nearly all source localization methods for spatial whitening as well as for data covariance estimation in beamformers. The rationale for such models is that the signals can be modeled by a zero mean Gaussian distribution. While maximizing the Gaussian likelihood seems natural, it leads to a covariance estimate known as empirical covariance ({EC}). It turns out that the {EC} is a poor estimate of the true covariance when the number of samples is small. To address this issue the estimation needs to be regularized. The most common approach downweights off-diagonal coefficients, while more advanced regularization methods are based on shrinkage techniques or generative models with low rank assumptions: probabilistic {PCA} ({PPCA}) and factor analysis ({FA}). Using cross-validation all of these models can be tuned and compared based on Gaussian likelihood computed on unseen data. We investigated these models on simulations, one electroencephalography ({EEG}) dataset as well as magnetoencephalography ({MEG}) datasets from the most common {MEG} systems. First, our results demonstrate that different models can be the best, depending on the number of samples, heterogeneity of sensor types and noise properties. Second, we show that the models tuned by cross-validation are superior to models with hand-selected regularization. Hence, we propose an automated solution to the often overlooked problem of covariance estimation of M/{EEG} signals. The relevance of the procedure is demonstrated here for spatial whitening and source localization of {MEG} signals.},
	pages = {328--342},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Engemann, Denis A. and Gramfort, Alexandre},
	urldate = {2024-07-26},
	date = {2015-03-01},
	keywords = {Covariance estimation, Electroencephalography ({EEG}), Factor analysis ({FA}), Magnetoencephalography ({MEG}), Model selection, Neuroimaging, Principal component analysis ({PCA}), Statistical learning, Whitening},
	file = {ScienceDirect Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\QIF8EVS4\\S1053811914010325.html:text/html},
}

@article{tuLearningBinaryFactor2014,
	title = {Learning binary factor analysis with automatic model selection},
	volume = {134},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231214001246},
	doi = {10.1016/j.neucom.2012.12.069},
	series = {Special issue on the 2011 Sino-foreign-interchange Workshop on Intelligence Science and Intelligent Data Engineering ({IScIDE} 2011)},
	abstract = {Binary Factor Analysis ({BFA}) uncovers the independent binary information sources from observations with wide applications. {BFA} learning hierarchically nests three levels of inverse problems, i.e., inference of binary code for each observation, parameter estimation and model selection. Under Bayesian Ying-Yang ({BYY}) framework, the first level becomes an intractable Binary Quadratic Programming ({BQP}) problem, while model selection can be conducted automatically during parameter learning. We conduct extensive experiments to reveal that the performance order of four {BQP} methods is reversed from making {BQP} optimization to making {BYY} automatic model selection, which implies that learning is not merely optimization. Moreover, the {BFA} learning algorithm is further developed with priors over parameters to improve the performance. Finally, based on {BFA}, we empirically compare {BYY} with Variational Bayes ({VB}) and Bayesian information criterion ({BIC}).},
	pages = {149--158},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Tu, Shikui and Xu, Lei},
	urldate = {2024-07-26},
	date = {2014-06-25},
	keywords = {Automatic model selection, Bayesian Ying-Yang, Binary Factor Analysis, Variational Bayes, automat\_model\_selection},
	file = {ScienceDirect Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\G7SMBVFC\\S0925231214001246.html:text/html;Versione inviata:C\:\\Users\\fgfra\\Zotero\\storage\\V7W7A9Y3\\Tu e Xu - 2014 - Learning binary factor analysis with automatic mod.pdf:application/pdf},
}

@article{zhouAutomaticModelSelection2011,
	title = {The automatic model selection and variable kernel width for {RBF} neural networks},
	volume = {74},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231211004243},
	doi = {10.1016/j.neucom.2011.07.011},
	abstract = {The Orthogonal Least Squares ({OLS}) algorithm has been extensively used in basis selection for {RBF} networks, but it is unable to perform model selection automatically because the tolerance ρ must be specified manually. This introduces noise and it is difficult to implement in the parametric complexity of real-time system. Therefore, a generic criterion that detects the optimum number of its basis functions is proposed. In this paper, not only the Bayesian Information Criterion ({BIC}) method, used for fitness calculation, is incorporated into the basis function selection process of the {OLS} algorithm for assigning its appropriate number, but also a new method is developed to optimize the widths of the Gaussian functions in order to improve the generalization performance. The augmented algorithm is employed to the Radial Basis Function Neural Networks ({RBFNN}) for known and unknown noise nonlinear dynamic systems and its performance is compared with the standard {OLS}; experimental results show that both the efficacy of {BIC} for fitness calculation and the importance of proper choice of basis function widths are significant.},
	pages = {3628--3637},
	number = {17},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Zhou, Peng and Li, Dehua and Wu, Hong and Cheng, Feng},
	urldate = {2024-07-26},
	date = {2011-10-01},
	keywords = {Bayesian information criterion, Kernel width, Orthogonal least squares algorithm, Radial basis function network},
	file = {ScienceDirect Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\3N2Y584I\\S0925231211004243.html:text/html},
}

@article{bollenAutomatingSelectionModelImplied2004,
	title = {Automating the Selection of Model-Implied Instrumental Variables},
	volume = {32},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124103260341},
	doi = {10.1177/0049124103260341},
	abstract = {Recently, interest has grown in the use of instrumental variables ({IVs}) in estimating factor analysis and latent variable models such as structural equations models. Bollen (1996) suggested a two-stage least squares (2SLS) technique that makes use of model-implied {IVs} in estimating the measurement and latent variable models. Model-implied instrumental variables are the observed variables in the model that can serve as instrumental variables in a given equation. One difficulty inhibiting the practical use of the 2SLS estimator is identifying the model-implied {IVs}. The authors provide a simple procedure that identifies the model-implied {IVs} and a computer algorithm that can easily be implemented to automate the selection of {IVs} for simultaneous equations, factor analysis, and latent variable models.},
	pages = {425--452},
	number = {4},
	journaltitle = {Sociological Methods \& Research},
	author = {Bollen, Kenneth A. and Bauer, Daniel J.},
	urldate = {2024-07-26},
	date = {2004-05-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
}

@online{AutomatedSupportModel,
	title = {Automated Support for Model Selection Using Analytic Hierarchy Process - {ProQuest}},
	url = {https://www.proquest.com/docview/869951232?parentSessionId=Sc942aASUgCf35rx0dXhQwntcocchk4tWgGbkwoSl9I%3D&sourcetype=Dissertations%20&%20Theses},
	urldate = {2024-07-26},
	file = {Automated Support for Model Selection Using Analytic Hierarchy Process - ProQuest:C\:\\Users\\fgfra\\Zotero\\storage\\N5ZHGUGQ\\869951232.html:text/html},
}

@article{zhouAutomaticModelSelection2011a,
	title = {The automatic model selection and variable kernel width for {RBF} neural networks},
	volume = {74},
	issn = {09252312},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231211004243},
	doi = {10.1016/j.neucom.2011.07.011},
	abstract = {The Orthogonal Least Squares ({OLS}) algorithm has been extensively used in basis selection for {RBF} networks, but it is unable to perform model selection automatically because the tolerance r must be speciﬁed manually. This introduces noise and it is difﬁcult to implement in the parametric complexity of real-time system. Therefore, a generic criterion that detects the optimum number of its basis functions is proposed. In this paper, not only the Bayesian Information Criterion ({BIC}) method, used for ﬁtness calculation, is incorporated into the basis function selection process of the {OLS} algorithm for assigning its appropriate number, but also a new method is developed to optimize the widths of the Gaussian functions in order to improve the generalization performance. The augmented algorithm is employed to the Radial Basis Function Neural Networks ({RBFNN}) for known and unknown noise nonlinear dynamic systems and its performance is compared with the standard {OLS}; experimental results show that both the efﬁcacy of {BIC} for ﬁtness calculation and the importance of proper choice of basis function widths are signiﬁcant.},
	pages = {3628--3637},
	number = {17},
	journaltitle = {Neurocomputing},
	shortjournal = {Neurocomputing},
	author = {Zhou, Peng and Li, Dehua and Wu, Hong and Cheng, Feng},
	urldate = {2024-07-26},
	date = {2011-10},
	langid = {english},
	keywords = {automat\_model\_selection},
	file = {Zhou et al. - 2011 - The automatic model selection and variable kernel .pdf:C\:\\Users\\fgfra\\Zotero\\storage\\GUYUQDVM\\Zhou et al. - 2011 - The automatic model selection and variable kernel .pdf:application/pdf},
}

@article{engemannAutomatedModelSelection2015a,
	title = {Automated model selection in covariance estimation and spatial whitening of {MEG} and {EEG} signals},
	volume = {108},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811914010325},
	doi = {10.1016/j.neuroimage.2014.12.040},
	abstract = {Magnetoencephalography and electroencephalography (M/{EEG}) measure non-invasively the weak electromagnetic ﬁelds induced by post-synaptic neural currents. The estimation of the spatial covariance of the signals recorded on M/{EEG} sensors is a building block of modern data analysis pipelines. Such covariance estimates are used in brain–computer interfaces ({BCI}) systems, in nearly all source localization methods for spatial whitening as well as for data covariance estimation in beamformers. The rationale for such models is that the signals can be modeled by a zero mean Gaussian distribution. While maximizing the Gaussian likelihood seems natural, it leads to a covariance estimate known as empirical covariance ({EC}). It turns out that the {EC} is a poor estimate of the true covariance when the number of samples is small. To address this issue the estimation needs to be regularized. The most common approach downweights off-diagonal coefﬁcients, while more advanced regularization methods are based on shrinkage techniques or generative models with low rank assumptions: probabilistic {PCA} ({PPCA}) and factor analysis ({FA}). Using cross-validation all of these models can be tuned and compared based on Gaussian likelihood computed on unseen data.},
	pages = {328--342},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Engemann, Denis A. and Gramfort, Alexandre},
	urldate = {2024-07-26},
	date = {2015-03},
	langid = {english},
	keywords = {automat\_model\_selection},
	file = {Engemann e Gramfort - 2015 - Automated model selection in covariance estimation.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\UGIFUBI2\\Engemann e Gramfort - 2015 - Automated model selection in covariance estimation.pdf:application/pdf},
}

@article{bollenAutomatingSelectionModelImplied2004a,
	title = {Automating the Selection of Model-Implied Instrumental Variables},
	volume = {32},
	rights = {http://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {0049-1241, 1552-8294},
	url = {http://journals.sagepub.com/doi/10.1177/0049124103260341},
	doi = {10.1177/0049124103260341},
	abstract = {An {SAS}/{IML} macro that implements the algorithm is in the appendix.},
	pages = {425--452},
	number = {4},
	journaltitle = {Sociological Methods \& Research},
	shortjournal = {Sociological Methods \& Research},
	author = {Bollen, Kenneth A. and Bauer, Daniel J.},
	urldate = {2024-07-26},
	date = {2004-05},
	langid = {english},
	keywords = {automat\_model\_selection},
	file = {Bollen e Bauer - 2004 - Automating the Selection of Model-Implied Instrume.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\3YMEIU8T\\Bollen e Bauer - 2004 - Automating the Selection of Model-Implied Instrume.pdf:application/pdf},
}

@article{lopez-rubioAutomaticModelSelection2009,
	title = {Automatic Model Selection by Cross-Validation for Probabilistic {PCA}},
	volume = {30},
	issn = {1573-773X},
	url = {https://doi.org/10.1007/s11063-009-9113-5},
	doi = {10.1007/s11063-009-9113-5},
	abstract = {The Mixture of Probabilistic Principal Components Analyzers ({MPPCA}) is a multivariate analysis technique which defines a Gaussian probabilistic model at each unit. The numbers of units and principal directions in each unit are not learned in the original approach. Variational Bayesian approaches have been proposed for this purpose, which rely on assumptions on the probability distributions of the {MPPCA} parameters. Here we present a different way to solve this problem, where cross-validation and simulated annealing are combined to guide the search for an optimal model selection, providing a structured strategy to escape from suboptimal configurations. This allows to learn the model architecture without the need of any assumptions other than those of the basic {PPCA} framework. Experimental results are presented, which show the probability density estimation and missing value imputation features of the proposal.},
	pages = {113--132},
	number = {2},
	journaltitle = {Neural Processing Letters},
	shortjournal = {Neural Process Lett},
	author = {López-Rubio, Ezequiel and Ortiz-de-Lazcano-Lobato, Juan Miguel},
	urldate = {2024-07-26},
	date = {2009-10-01},
	langid = {english},
	keywords = {Cross-validation, Dimensionality reduction, Missing value imputation, Probabilistic principal components analysis ({PPCA}), Probability density estimation, Simulated annealing, automat\_model\_selection},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\R99F9822\\López-Rubio e Ortiz-de-Lazcano-Lobato - 2009 - Automatic Model Selection by Cross-Validation for .pdf:application/pdf},
}

@article{luSemisupervisedLearningAlgorithm2008,
	title = {A Semi-supervised Learning Algorithm on Gaussian Mixture with Automatic Model Selection},
	volume = {27},
	issn = {1573-773X},
	url = {https://doi.org/10.1007/s11063-007-9059-4},
	doi = {10.1007/s11063-007-9059-4},
	abstract = {In Gaussian mixture modeling, it is crucial to select the number of Gaussians for a sample set, which becomes much more difficult when the overlap in the mixture is larger. Under regularization theory, we aim to solve this problem using a semi-supervised learning algorithm through incorporating pairwise constraints into entropy regularized likelihood ({ERL}) learning which can make automatic model selection for Gaussian mixture. The simulation experiments further demonstrate that the presented semi-supervised learning algorithm (i.e., the constrained {ERL} learning algorithm) can automatically detect the number of Gaussians with a good parameter estimation, even when two or more actual Gaussians in the mixture are overlapped at a high degree. Moreover, the constrained {ERL} learning algorithm leads to some promising results when applied to iris data classification and image database categorization.},
	pages = {57--66},
	number = {1},
	journaltitle = {Neural Processing Letters},
	shortjournal = {Neural Process Lett},
	author = {Lu, Zhiwu and Peng, Yuxin},
	urldate = {2024-07-26},
	date = {2008-02-01},
	langid = {english},
	keywords = {Model selection, Entropy regularization, Gaussian mixture, Pairwise constraints, Semi-supervised learning, automat\_model\_selection},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\KLF98EN4\\Lu e Peng - 2008 - A Semi-supervised Learning Algorithm on Gaussian M.pdf:application/pdf},
}

@article{karstoftApplicabilityAutomatedModel2020,
	title = {Applicability of an Automated Model and Parameter Selection in the Prediction of Screening-Level {PTSD} in Danish Soldiers Following Deployment: Development Study of Transferable Predictive Models Using Automated Machine Learning},
	volume = {8},
	issn = {2291-9694},
	url = {http://medinform.jmir.org/2020/7/e17119/},
	doi = {10.2196/17119},
	shorttitle = {Applicability of an Automated Model and Parameter Selection in the Prediction of Screening-Level {PTSD} in Danish Soldiers Following Deployment},
	abstract = {Background: Posttraumatic stress disorder ({PTSD}) is a relatively common consequence of deployment to war zones. Early postdeployment screening with the aim of identifying those at risk for {PTSD} in the years following deployment will help deliver interventions to those in need but have so far proved unsuccessful.
Objective: This study aimed to test the applicability of automated model selection and the ability of automated machine learning prediction models to transfer across cohorts and predict screening-level {PTSD} 2.5 years and 6.5 years after deployment.
Methods: Automated machine learning was applied to data routinely collected 6-8 months after return from deployment from 3 different cohorts of Danish soldiers deployed to Afghanistan in 2009 (cohort 1, N=287 or N=261 depending on the timing of the outcome assessment), 2010 (cohort 2, N=352), and 2013 (cohort 3, N=232).
Results: Models transferred well between cohorts. For screening-level {PTSD} 2.5 and 6.5 years after deployment, random forest models provided the highest accuracy as measured by area under the receiver operating characteristic curve ({AUC}): 2.5 years, {AUC}=0.77, 95\% {CI} 0.71-0.83; 6.5 years, {AUC}=0.78, 95\% {CI} 0.73-0.83. Linear models performed equally well. Military rank, hyperarousal symptoms, and total level of {PTSD} symptoms were highly predictive.
Conclusions: Automated machine learning provided validated models that can be readily implemented in future deployment cohorts in the Danish Defense with the aim of targeting postdeployment support interventions to those at highest risk for developing {PTSD}, provided the cohorts are deployed on similar missions.},
	pages = {e17119},
	number = {7},
	journaltitle = {{JMIR} Medical Informatics},
	shortjournal = {{JMIR} Med Inform},
	author = {Karstoft, Karen-Inge and Tsamardinos, Ioannis and Eskelund, Kasper and Andersen, Søren Bo and Nissen, Lars Ravnborg},
	urldate = {2024-07-26},
	date = {2020-07-22},
	langid = {english},
	keywords = {automat\_model\_selection, risorsa\_indirettamente\_consultata},
	file = {Karstoft et al. - 2020 - Applicability of an Automated Model and Parameter .pdf:C\:\\Users\\fgfra\\Zotero\\storage\\D3R2VYDX\\Karstoft et al. - 2020 - Applicability of an Automated Model and Parameter .pdf:application/pdf},
}

@article{gelmanExploratoryDataAnalysis2004a,
	title = {Exploratory Data Analysis for Complex Models},
	volume = {13},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/106186004X11435},
	doi = {10.1198/106186004X11435},
	pages = {755--779},
	number = {4},
	journaltitle = {Journal of Computational and Graphical Statistics},
	shortjournal = {Journal of Computational and Graphical Statistics},
	author = {Gelman, Andrew},
	urldate = {2024-07-26},
	date = {2004-12},
	langid = {english},
	keywords = {automat\_model\_selection},
	file = {Gelman - 2004 - Exploratory Data Analysis for Complex Models.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\WDDJIBC5\\Gelman - 2004 - Exploratory Data Analysis for Complex Models.pdf:application/pdf},
}

@article{gangiEfficientOptimizationApproach2019,
	title = {An efficient optimization approach for best subset selection in linear regression, with application to model selection and fitting in autoregressive time-series},
	volume = {74},
	issn = {0926-6003, 1573-2894},
	url = {http://link.springer.com/10.1007/s10589-019-00134-5},
	doi = {10.1007/s10589-019-00134-5},
	abstract = {In this paper we consider two relevant optimization problems: the problem of selecting the best sparse linear regression model and the problem of optimally identifying the parameters of auto-regressive models based on time series data. Usually these problems, which although different are indeed related, are solved through a sequence of separate steps, alternating between choosing a subset of features and then ﬁnding a best ﬁt regression. In this paper we propose to model both problems as mixed integer non linear optimization ones and propose numerical procedures based on state of the art optimization tools in order to solve both of them. The proposed approach has the advantage of considering both model selection as well as parameter estimation as a single optimization problem. Numerical experiments performed on widely available datasets as well as on synthetic ones conﬁrm the high quality of our approach, both in terms of the quality of the resulting models and in terms of {CPU} time.},
	pages = {919--948},
	number = {3},
	journaltitle = {Computational Optimization and Applications},
	shortjournal = {Comput Optim Appl},
	author = {Gangi, Leonardo Di and Lapucci, M. and Schoen, F. and Sortino, A.},
	urldate = {2024-07-26},
	date = {2019-12},
	langid = {english},
	keywords = {automat\_model\_selection},
	file = {Gangi et al. - 2019 - An efficient optimization approach for best subset.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\3PPWKYT9\\Gangi et al. - 2019 - An efficient optimization approach for best subset.pdf:application/pdf},
}

@article{niAutomaticModelSelection2009,
	title = {Automatic model selection for partially linear models},
	volume = {100},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X09001171},
	doi = {10.1016/j.jmva.2009.06.009},
	abstract = {We propose and study a unified procedure for variable selection in partially linear models. A new type of double-penalized least squares is formulated, using the smoothing spline to estimate the nonparametric part and applying a shrinkage penalty on parametric components to achieve model parsimony. Theoretically we show that, with proper choices of the smoothing and regularization parameters, the proposed procedure can be as efficient as the oracle estimator [J. Fan, R. Li, Variable selection via nonconcave penalized likelihood and its oracle properties, Journal of American Statistical Association 96 (2001) 1348–1360]. We also study the asymptotic properties of the estimator when the number of parametric effects diverges with the sample size. Frequentist and Bayesian estimates of the covariance and confidence intervals are derived for the estimators. One great advantage of this procedure is its linear mixed model ({LMM}) representation, which greatly facilitates its implementation by using standard statistical software. Furthermore, the {LMM} framework enables one to treat the smoothing parameter as a variance component and hence conveniently estimate it together with other regression coefficients. Extensive numerical studies are conducted to demonstrate the effective performance of the proposed procedure.},
	pages = {2100--2111},
	number = {9},
	journaltitle = {Journal of Multivariate Analysis},
	shortjournal = {Journal of Multivariate Analysis},
	author = {Ni, Xiao and Zhang, Hao Helen and Zhang, Daowen},
	urldate = {2024-07-26},
	date = {2009-10},
	langid = {english},
	keywords = {automat\_model\_selection},
	file = {Ni et al. - 2009 - Automatic model selection for partially linear mod.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\4ADAGQKK\\Ni et al. - 2009 - Automatic model selection for partially linear mod.pdf:application/pdf},
}

@article{giovagnoliBayesianDesignAdaptive2021,
	title = {The Bayesian Design of Adaptive Clinical Trials},
	volume = {18},
	issn = {1661-7827},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7826635/},
	doi = {10.3390/ijerph18020530},
	abstract = {This paper presents a brief overview of the recent literature on adaptive design of clinical trials from a Bayesian perspective for statistically not so sophisticated readers. Adaptive designs are attracting a keen interest in several disciplines, from a theoretical viewpoint and also—potentially—from a practical one, and Bayesian adaptive designs, in particular, have raised high expectations in clinical trials. The main conceptual tools are highlighted here, with a mention of several trial designs proposed in the literature that use these methods, including some of the registered Bayesian adaptive trials to this date. This review aims at complementing the existing ones on this topic, pointing at further interesting reading material.},
	pages = {530},
	number = {2},
	journaltitle = {International Journal of Environmental Research and Public Health},
	shortjournal = {Int J Environ Res Public Health},
	author = {Giovagnoli, Alessandra},
	urldate = {2024-07-26},
	date = {2021-01},
	pmid = {33435249},
	pmcid = {PMC7826635},
	keywords = {Bayesian\_adaptive\_sampling},
	file = {PubMed Central Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\PSKRITYU\\Giovagnoli - 2021 - The Bayesian Design of Adaptive Clinical Trials.pdf:application/pdf},
}

@article{todeschiniDetectingBadRegression2004,
	title = {Detecting “bad” regression models: multicriteria fitness functions in regression analysis},
	volume = {515},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {00032670},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S000326700301612X},
	doi = {10.1016/j.aca.2003.12.010},
	shorttitle = {Detecting “bad” regression models},
	abstract = {Regression models with good fitting but no predictive ability are sometimes chance correlations and often show some pathological features such as multicollinearity, overfitting, and inclusion of noisy/spurious variables. This problem is well known and of the utmost importance. The present paper proposes some criteria that are to be fulfilled as conditions for model acceptability, the aim being to recognize linear regression models with pathology. These criteria have been thought of in order to face the following problems: • model instability due to outliers and influential objects; • predictor multicollinearity; • redundancy in explanatory variables; • overfitting due to chance factors. A multicriteria fitness function based on the maximization of the Q2 statistics under a set of tests is proposed here. This new fitness function can also be used in model searching by variable selection approaches in order to obtain a final optimal population of models. Computations on the Selwood data set are reported to illustrate the use of this multicriteria fitness function in model searching.},
	pages = {199--208},
	number = {1},
	journaltitle = {Analytica Chimica Acta},
	shortjournal = {Analytica Chimica Acta},
	author = {Todeschini, Roberto and Consonni, Viviana and Mauri, Andrea and Pavan, Manuela},
	urldate = {2024-07-26},
	date = {2004-07},
	langid = {english},
	keywords = {malpractices\_on\_regression\_analisys, letto},
	file = {Todeschini et al. - 2004 - Detecting “bad” regression models multicriteria f.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\K4W4GCGI\\Todeschini et al. - 2004 - Detecting “bad” regression models multicriteria f.pdf:application/pdf},
}

@article{schmidtAutomatedDataAnalysis2024,
	title = {Automated data analysis of unstructured grey literature in health research: A mapping review},
	volume = {15},
	issn = {1759-2879, 1759-2887},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1692},
	doi = {10.1002/jrsm.1692},
	shorttitle = {Automated data analysis of unstructured grey literature in health research},
	abstract = {The amount of grey literature and ‘softer’ intelligence from social media or websites is vast. Given the long lead-times of producing high-quality peer-reviewed health information, this is causing a demand for new ways to provide prompt input for secondary research. To our knowledge, this is the first review of automated data extraction methods or tools for health-related grey literature and soft data, with a focus on (semi)automating horizon scans, health technology assessments ({HTA}), evidence maps, or other literature reviews. We searched six databases to cover both health- and computer-science literature. After deduplication, 10\% of the search results were screened by two reviewers, the remainder was single-screened up to an estimated 95\% sensitivity; screening was stopped early after screening an additional 1000 results with no new includes. All full texts were retrieved, screened, and extracted by a single reviewer and 10\% were checked in duplicate. We included 84 papers covering automation for health-related social media, internet fora, news, patents, government agencies and charities, or trial registers. From each paper, we extracted data about important functionalities for users of the tool or method; information about the level of support and reliability; and about practical challenges and research gaps. Poor availability of code, data, and usable tools leads to low transparency regarding performance and duplication of work. Financial implications, scalability, integration into downstream workflows, and meaningful evaluations should be carefully planned before starting to develop a tool, given the vast amounts of data and opportunities those tools offer to expedite research.},
	pages = {178--197},
	number = {2},
	journaltitle = {Research Synthesis Methods},
	shortjournal = {Research Synthesis Methods},
	author = {Schmidt, Lena and Mohamed, Saleh and Meader, Nick and Bacardit, Jaume and Craig, Dawn},
	urldate = {2024-07-26},
	date = {2024-03},
	langid = {english},
	keywords = {automat\_data\_analysis},
	file = {Schmidt et al. - 2024 - Automated data analysis of unstructured grey liter.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\XD5EKASU\\Schmidt et al. - 2024 - Automated data analysis of unstructured grey liter.pdf:application/pdf},
}

@article{zachariouIronsmithAutomatedPipeline2022,
	title = {Ironsmith: An automated pipeline for {QSM}-based data analyses},
	volume = {249},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S105381192101106X},
	doi = {10.1016/j.neuroimage.2021.118835},
	shorttitle = {Ironsmith},
	pages = {118835},
	journaltitle = {{NeuroImage}},
	shortjournal = {{NeuroImage}},
	author = {Zachariou, Valentinos and Bauer, Christopher E. and Powell, David K. and Gold, Brian T.},
	urldate = {2024-07-26},
	date = {2022-04},
	langid = {english},
	keywords = {automat\_data\_analysis},
	file = {Zachariou et al. - 2022 - Ironsmith An automated pipeline for QSM-based dat.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\DVM7JKLL\\Zachariou et al. - 2022 - Ironsmith An automated pipeline for QSM-based dat.pdf:application/pdf},
}

@article{liAutomaticVariableSelection2013,
	title = {Automatic variable selection for longitudinal generalized linear models},
	volume = {61},
	rights = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947312004495},
	doi = {10.1016/j.csda.2012.12.015},
	abstract = {We consider the problem of variable selection for the generalized linear models ({GLMs}) with longitudinal data. An automatic variable selection procedure is developed using smooth-threshold generalized estimating equations ({SGEE}). The proposed procedure automatically eliminates inactive predictors by setting the corresponding parameters to be zero, and simultaneously estimates the nonzero regression coefficients by solving the {SGEE}. The proposed method shares some of the desired features of existing variable selection methods: the resulting estimator enjoys the oracle property; the proposed procedure avoids the convex optimization problem and is flexible and easy to implement. Moreover, we propose a penalized weighted deviance criterion for a data-driven choice of the tuning parameters. Simulation studies are carried out to assess the performance of {SGEE}, and a real dataset is analyzed for further illustration.},
	pages = {174--186},
	journaltitle = {Computational Statistics \& Data Analysis},
	shortjournal = {Computational Statistics \& Data Analysis},
	author = {Li, Gaorong and Lian, Heng and Feng, Sanying and Zhu, Lixing},
	urldate = {2024-07-26},
	date = {2013-05},
	langid = {english},
	keywords = {automat\_data\_analysis},
	file = {Li et al. - 2013 - Automatic variable selection for longitudinal gene.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\GJZJJCQQ\\Li et al. - 2013 - Automatic variable selection for longitudinal gene.pdf:application/pdf},
}

@inproceedings{bobkovVerificationModelEvolution2019,
	title = {Verification of a model of the evolution of the organizational structure of retail enterprises, by using regression analysis and automatic linear modeling},
	url = {https://msed.vse.cz/msed_2019/sbornik/toc.html},
	doi = {10.18267/pr.2019.los.186.16},
	abstract = {The purpose of the study, examined in this article, is to create an econometric model that allows quantifying the growth of the organization's efficiency in the process of its development. The retail sector of the Czech Republic was chosen as the object of analysis. The financial and economic indicators of these organizations were obtained from the database Albertina Gold Edition of Bisnode Česká republika, a.s. for 2014. The computer program {IBM} {SPSS} Statistics was used for regression analysis in order to conduct the research. The analysis of the obtained results showed that the increase in the efficiency of retail organizations in the process of their development can be described by a linear model with acceptable accuracy. The main factors determining the increase in the efficiency of retail organizations were identified and the authors confirmed the model of the evolution of the organizational structure of commercial organizations. The analysis of the constructed model revealed the factors having the greatest impact on the growth of the efficiency of retail trade organizations.},
	eventtitle = {International Days of Statistics and Economics 2019},
	booktitle = {International Days of Statistics and Economics 2019},
	publisher = {Libuše Macáková, {MELANDRIUM}},
	author = {Bobkov, Aleksandr and Denisov, Igor and Kuchmaeva, Oxana and Tsenina, Ekaterina and Velinov, Emil},
	urldate = {2024-07-26},
	date = {2019},
	langid = {english},
	keywords = {automat\_data\_analysis},
	file = {Bobkov et al. - 2019 - Verification of a model of the evolution of the or.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\YFMMEMH2\\Bobkov et al. - 2019 - Verification of a model of the evolution of the or.pdf:application/pdf},
}

@article{kamperAutoMoGAutomatedDatadriven2021,
	title = {{AutoMoG}: Automated data-driven Model Generation of multi-energy systems using piecewise-linear regression},
	volume = {145},
	issn = {0098-1354},
	url = {https://www.sciencedirect.com/science/article/pii/S0098135420306852},
	doi = {10.1016/j.compchemeng.2020.107162},
	shorttitle = {{AutoMoG}},
	abstract = {Operational optimization of multi-energy systems requires a mathematical model that is accurate and computationally efficient. A model can be generated in a data-driven way if measured data is available. Commonly, data is then used to model each component of the multi-energy system independently. However, independent modeling of each component may lead to models that are unnecessarily complicated and, thus, inefficient in practice. In this work, we propose the method {AutoMoG} for Automated data-driven Model Generation of multi-energy systems using piecewise-linear regression. {AutoMoG} provides Mixed-Integer Linear Programming models of multi-energy systems. To accurately model the overall multi-energy system, {AutoMoG} balances the errors caused by each component. Model accuracy is measured in terms of operating cost. In a case study, {AutoMoG} provides a multi-energy system model with less linear sections than single-component regression Still, {AutoMoG} retains high accuracy. Thereby, {AutoMoG} enables efficient data-driven modeling as the basis for multi-energy system optimization.},
	pages = {107162},
	journaltitle = {Computers \& Chemical Engineering},
	shortjournal = {Computers \& Chemical Engineering},
	author = {Kämper, Andreas and Leenders, Ludger and Bahl, Björn and Bardow, André},
	urldate = {2024-07-26},
	date = {2021-02-01},
	keywords = {Regression analysis, Energy system optimization, Information criterion, Mixed-integer linear programming, automat\_data\_analysis},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\WHZ2GIU2\\Kämper et al. - 2021 - AutoMoG Automated data-driven Model Generation of.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\5ULV3ZAE\\S0098135420306852.html:text/html},
}

@inproceedings{veselinovicMethodAutomaticGeneration1996,
	location = {Atlanta, {GA}, {USA}},
	title = {A method for automatic generation of piecewise linear models},
	volume = {3},
	isbn = {978-0-7803-3073-3},
	url = {http://ieeexplore.ieee.org/document/541471/},
	doi = {10.1109/ISCAS.1996.541471},
	abstract = {This paper presents a method to automate generation of piecewise linear ({PL}) models which approximate some nonlinear multi-variable scalar functions with a given relative error. The method allows to tune the models for any given relative error and assures that any intermediate function simplification will not affect the overall accuracy. Estimation of the accuracy of the {PL} model and a possible trade-off between the size of the model and its accuracy is done as well. The given relative error is also used to estimate contributionof each factor in the overall function and to simplify the function neglecting the factors whose contribution is smaller than the given error. This allows for the final model to be the smallest possible, for the given accuracy.},
	eventtitle = {1996 {IEEE} International Symposium on Circuits and Systems. Circuits and Systems Connecting the World. {ISCAS} 96},
	pages = {24--27},
	booktitle = {1996 {IEEE} International Symposium on Circuits and Systems. Circuits and Systems Connecting the World. {ISCAS} 96},
	publisher = {{IEEE}},
	author = {Veselinovic, P. and Leenaerts, D.},
	urldate = {2024-07-26},
	date = {1996},
	langid = {english},
	keywords = {automat\_data\_analysis},
	file = {Veselinovic e Leenaerts - 1996 - A method for automatic generation of piecewise lin.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\RBP5VP2W\\Veselinovic e Leenaerts - 1996 - A method for automatic generation of piecewise lin.pdf:application/pdf},
}

@article{smithStepAwayStepwise2018,
	title = {Step away from stepwise},
	volume = {5},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-018-0143-6},
	doi = {10.1186/s40537-018-0143-6},
	abstract = {Stepwise regression is a popular data-mining tool that uses statistical significance to select the explanatory variables to be used in a multiple-regression model.},
	pages = {32},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {Journal of Big Data},
	author = {Smith, Gary},
	urldate = {2024-07-26},
	date = {2018-09-15},
	keywords = {Big Data, Data mining, Stepwise regression, stepwise\_regression, non\_centra\_con\_la\_tesi},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\9M7DMLS7\\Smith - 2018 - Step away from stepwise.pdf:application/pdf;Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\8HXFDGTA\\s40537-018-0143-6.html:text/html},
}

@article{wangStepwiseRegressionAll2016,
	title = {Stepwise Regression and All Possible Subsets Regression in Education},
	abstract = {Stepwise methods are quite common to be reported in empirically based journal articles (Huberty, 1994). However, many researchers using stepwise methods failed to realize that software packages had been programmed in error. The purpose of this study is to introduce the procedure of stepwise regression and used experiments and Venn diagrams to illustrate the three main problems of stepwise regression: wrong degree of freedom, capitalization on sampling, and error R 2 not optimized. Meanwhile, the study also depicted an alternative method: All-possible-subsets regression and used an experiment to illustrate how to use it in real complex study. Finally, some matters needing attention when using both automatic procedures are discussed.},
	author = {Wang, Ke and Chen, Zhuo},
	date = {2016-01-01},
	keywords = {stepwise\_regression, non\_centra\_con\_la\_tesi},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\CEIKE9HD\\Wang e Chen - 2016 - Stepwise Regression and All Possible Subsets Regre.pdf:application/pdf},
}

@inproceedings{lewisStepwiseHierarchicalRegression2007,
	title = {Stepwise versus hierarchical regression: Pros and cons},
	shorttitle = {Stepwise versus hierarchical regression},
	abstract = {Multiple regression is commonly used in social and behavioral data analysis (Fox, 1991; Huberty, 1989).
In multiple regression contexts, researchers are very often interested in determining the “best” predictors in the analysis. This focus may stem from a need to identify those predictors that are supportive of theory.
Alternatively, the researcher may simply be interested in explaining the most variability in the dependent variable with the fewest possible predictors, perhaps as part of a cost analysis. Two approaches to determining the quality of predictors are (1) stepwise regression and (2) hierarchical regression.
This paper will explore the advantages and disadvantages of these methods and use a small {SPSS} dataset for illustration purposes.},
	author = {Lewis, Mitzi},
	date = {2007-02-07},
	keywords = {stepwise\_regression, non\_centra\_con\_la\_tesi},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\8CMI6TVY\\Lewis - 2007 - Stepwise versus hierarchical regression Pros and .pdf:application/pdf},
}

@article{zhangResearchConstructionRealization2022,
	title = {Research on the Construction and Realization of Data Pipeline in Machine Learning Regression Prediction},
	volume = {2022},
	rights = {Copyright © 2022 Hua Zhang et al.},
	issn = {1563-5147},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/7924335},
	doi = {10.1155/2022/7924335},
	abstract = {The data set used by machine learning usually contains missing value and text type data, and sometimes, it is necessary to combine the attributes in the data set. The data set must be cleaned and converted before the machine learning model can be generated. This is frequently a chain of events. The entire processing procedure will be time-consuming and inconvenient. This article examines the data pipeline and recommends that it be used to process all data. We carry out automation and use k-fold cross-validation to evaluate the performance of the model. Experiments demonstrate that it can lower the regression prediction model’s root mean square error and enhance prediction accuracy.},
	pages = {7924335},
	number = {1},
	journaltitle = {Mathematical Problems in Engineering},
	author = {Zhang, Hua and Zheng, Guoxun and Xu, Jun and Yao, Xuekun},
	urldate = {2024-07-26},
	date = {2022},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1155/2022/7924335},
	keywords = {pipeline\_regression, letto},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\TUXNCC7C\\Zhang et al. - 2022 - Research on the Construction and Realization of Da.pdf:application/pdf;Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\2RHK2245\\7924335.html:text/html},
}

@article{wangFunctionalDataAnalysis2016,
	title = {Functional Data Analysis},
	volume = {3},
	issn = {2326-8298, 2326-831X},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-statistics-041715-033624},
	doi = {10.1146/annurev-statistics-041715-033624},
	abstract = {With the advance of modern technology, more and more data are being recorded continuously during a time interval or intermittently at several discrete time points. These are both examples of functional data, which has become a commonly encountered type of data. Functional data analysis ({FDA}) encompasses the statistical methodology for such data. Broadly interpreted, {FDA} deals with the analysis and theory of data that are in the form of functions. This paper provides an overview of {FDA}, starting with simple statistical notions such as mean and covariance functions, then covering some core techniques, the most popular of which is functional principal component analysis ({FPCA}). {FPCA} is an important dimension reduction tool, and in sparse data situations it can be used to impute functional data that are sparsely observed. Other dimension reduction approaches are also discussed. In addition, we review another core technique, functional linear regression, as well as clustering and classification of functional data. Beyond linear and single- or multiple- index methods, we touch upon a few nonlinear approaches that are promising for certain applications. They include additive and other nonlinear functional regression models and models that feature time warping, manifold learning, and empirical differential equations. The paper concludes with a brief discussion of future directions.},
	pages = {257--295},
	issue = {Volume 3, 2016},
	journaltitle = {Annual Review of Statistics and Its Application},
	author = {Wang, Jane-Ling and Chiou, Jeng-Min and Müller, Hans-Georg},
	urldate = {2024-07-28},
	date = {2016-06-01},
	langid = {english},
	note = {Publisher: Annual Reviews},
	keywords = {letto, functional\_data\_analysis},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\YXEGD795\\Wang et al. - 2016 - Functional Data Analysis.pdf:application/pdf;Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\BX8KQN2W\\annurev-statistics-041715-033624.html:text/html},
}

@article{behrensPrinciplesProceduresExploratory1997,
	title = {Principles and procedures of exploratory data analysis},
	volume = {2},
	issn = {1939-1463},
	doi = {10.1037/1082-989X.2.2.131},
	abstract = {Exploratory data analysis ({EDA}) is a well-established statistical tradition that provides conceptual and computational tools for discovering patterns to foster hypothesis development and refinement. These tools and attitudes complement the use of significance and hypothesis tests used in confirmatory data analysis ({CDA}). Although {EDA} complements rather than replaces {CDA}, use of {CDA} without {EDA} is seldom warranted. Even when well-specified theories are held, {EDA} helps one interpret the results of {CDA} and may reveal unexpected or misleading patterns in the data. This article introduces the central heuristics and computational tools of {EDA} and contrasts it with {CDA} and exploratory statistics in general. {EDA} techniques are illustrated using previously published psychological data. Changes in statistical training and practice are recommended to incorporate these tools. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {131--160},
	number = {2},
	journaltitle = {Psychological Methods},
	author = {Behrens, John T.},
	date = {1997},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Statistical Data, Statistical Analysis, Hypothesis Testing, letto, Statistical Significance, Theory Formulation},
	file = {1997-06270-001.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\A7E4WN67\\1997-06270-001.pdf:application/pdf;Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\64BKFXTI\\1997-06270-001.html:text/html},
}

@book{tukeyExploratoryDataAnalysis1977,
	title = {Exploratory Data Analysis},
	isbn = {978-0-201-07616-5},
	abstract = {Scratching down numbers (stem-and-leaf); Schematic summaries (pictures and numbers); Easy re-expression; Effective comparison (including well-chosen expresion); Plots of relationship; Straightening out plots (using three points); Smoothing sequences; Optional sections for chapter 7; Parallel and wandering schematic plots; Delineations of batches of points; Using two-way analyses; Making two-way analyses; Advances fits; Three-way fits; Looking in two or more ways at batches of points; Counted fractions; Better smoothing; Counts in bin after bin; Product-ratio plots; Shapes of distribution; Mathematical distributions; Postscript.},
	pagetotal = {714},
	publisher = {Addison-Wesley Publishing Company},
	author = {Tukey, John Wilder},
	date = {1977},
	langid = {english},
	note = {Google-Books-{ID}: {UT}9dAAAAIAAJ},
	keywords = {Mathematics / General, Mathematics / Probability \& Statistics / General, risorsa\_indirettamente\_consultata, book},
}

@article{opensciencecollaborationEstimatingReproducibilityPsychological2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	url = {https://www.science.org/doi/10.1126/science.aac4716},
	doi = {10.1126/science.aac4716},
	abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
	pages = {aac4716},
	number = {6251},
	journaltitle = {Science},
	author = {{Open Science Collaboration}},
	urldate = {2024-07-29},
	date = {2015-08-28},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {risorsa\_indirettamente\_consultata},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\7W9SB6LE\\Open Science Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf:application/pdf},
}

@article{kruschkeBayesianNewStatistics2018,
	title = {The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective},
	volume = {25},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-016-1221-4},
	doi = {10.3758/s13423-016-1221-4},
	shorttitle = {The Bayesian New Statistics},
	abstract = {In the practice of data analysis, there is a conceptual distinction between hypothesis testing, on the one hand, and estimation with quantified uncertainty on the other. Among frequentists in psychology, a shift of emphasis from hypothesis testing to estimation has been dubbed “the New Statistics” (Cumming 2014). A second conceptual distinction is between frequentist methods and Bayesian methods. Our main goal in this article is to explain how Bayesian methods achieve the goals of the New Statistics better than frequentist methods. The article reviews frequentist and Bayesian approaches to hypothesis testing and to estimation with confidence or credible intervals. The article also describes Bayesian approaches to meta-analysis, randomized controlled trials, and power analysis.},
	pages = {178--206},
	number = {1},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Kruschke, John K. and Liddell, Torrin M.},
	urldate = {2024-07-29},
	date = {2018-02-01},
	langid = {english},
	keywords = {risorsa\_indirettamente\_consultata, Bayes factor, Bayesian inference, Confidence interval, Credible interval, Effect size, Equivalence testing, Highest density interval, Meta-analysis, Null hypothesis significance testing, Power analysis, Randomized controlled trial, Region of practical equivalence},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\MWG5KSGY\\Kruschke e Liddell - 2018 - The Bayesian New Statistics Hypothesis testing, e.pdf:application/pdf},
}

@article{batesFittingLinearMixedEffects2015,
	title = {Fitting Linear Mixed-Effects Models Using lme4},
	volume = {67},
	rights = {Copyright (c) 2015 Douglas Bates, Martin Mächler, Ben Bolker, Steve Walker},
	issn = {1548-7660},
	url = {https://doi.org/10.18637/jss.v067.i01},
	doi = {10.18637/jss.v067.i01},
	abstract = {Maximum likelihood or restricted maximum likelihood ({REML}) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled {REML} criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or {REML} criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
	pages = {1--48},
	journaltitle = {Journal of Statistical Software},
	author = {Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
	urldate = {2024-07-30},
	date = {2015-10-07},
	langid = {english},
	keywords = {risorsa\_indirettamente\_consultata, Cholesky decomposition, linear mixed models, penalized least squares, sparse matrix methods},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\RLIQIJNY\\Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf:application/pdf},
}

@article{simmonsFalsePositivePsychologyUndisclosed2011,
	title = {False-Positive Psychology: Undisclosed Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
	volume = {22},
	issn = {0956-7976},
	url = {https://doi.org/10.1177/0956797611417632},
	doi = {10.1177/0956797611417632},
	shorttitle = {False-Positive Psychology},
	abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists’ nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
	pages = {1359--1366},
	number = {11},
	journaltitle = {Psychological Science},
	shortjournal = {Psychol Sci},
	author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
	urldate = {2024-07-30},
	date = {2011-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	keywords = {risorsa\_indirettamente\_consultata},
	file = {SAGE PDF Full Text:C\:\\Users\\fgfra\\Zotero\\storage\\2E4NL39R\\Simmons et al. - 2011 - False-Positive Psychology Undisclosed Flexibility.pdf:application/pdf},
}

@article{munafoManifestoReproducibleScience2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	doi = {10.1038/s41562-016-0021},
	abstract = {Improving the reliability and efficiency of scientific research will increase the credibility of the published scientific literature and accelerate discovery. Here we argue for the adoption of measures to optimize key elements of the scientific process: Methods, reporting and dissemination, reproducibility, evaluation and incentives. There is some evidence from both simulations and empirical studies supporting the likely effectiveness of these measures, but their broad adoption by researchers, institutions, funders and journals will require iterative evaluation and improvement. We discuss the goals of these measures, and how they can be implemented, in the hope that this will facilitate action toward improving the transparency, reproducibility and efficiency of scientific research.},
	number = {1},
	journaltitle = {Nature Human Behaviour},
	author = {Munafò, M.R. and Nosek, B.A. and Bishop, D.V.M. and Button, K.S. and Chambers, C.D. and Percie Du Sert, N. and Simonsohn, U. and Wagenmakers, E.-J. and Ware, J.J. and Ioannidis, J.P.A.},
	date = {2017},
	keywords = {risorsa\_indirettamente\_consultata},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\I7TCY4BS\\Munafò et al. - 2017 - A manifesto for reproducible science.pdf:application/pdf;Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\3TE3P6NE\\display.html:text/html},
}

@article{lokenMeasurementErrorReplication2017,
	title = {Measurement error and the replication crisis},
	volume = {355},
	url = {https://www.science.org/doi/10.1126/science.aal3618},
	doi = {10.1126/science.aal3618},
	pages = {584--585},
	number = {6325},
	journaltitle = {Science},
	author = {Loken, Eric and Gelman, Andrew},
	urldate = {2024-07-30},
	date = {2017-02-10},
	note = {Publisher: American Association for the Advancement of Science},
	keywords = {risorsa\_indirettamente\_consultata},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\TLT95LIQ\\Loken e Gelman - 2017 - Measurement error and the replication crisis.pdf:application/pdf},
}

@article{cohenNonDeterministicAlgorithms1979,
	title = {Non-Deterministic Algorithms},
	volume = {11},
	issn = {03600300},
	doi = {10.1145/356770.356773},
	abstract = {Primitive commands representing the concepts of choice, failure, and success are used to describe non-deterministic algorithms for solving a variety of problems. First, the role of the primitives is explained in a manner appealing to the reader's intuition. Then, a solution to the classical 8-queens problem is presented as a non-deterministic program, and its implementation is described. Two examples follow, showing the usefulness of the primitives in computer-aided problem solving: the first is a simple question-answering program; the other is a parser for a context-sensitive language. Finally, a brief survey of current and related work is presented which includes: additional desirable primitives, implementation, correctness, efficiency, and theoretical implications.},
	pages = {79--94},
	number = {2},
	journaltitle = {{ACM} Computing Surveys},
	author = {Cohen, Jacques},
	date = {1979-06},
	keywords = {letto, Algorithms, backtracking, Command \& control systems, Computer science, efficiency., Electronic data processing, implementation, interpreters, language constructs, problem solving, Problem solving, Programming languages},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\GENXVZ9X\\Cohen - 1979 - Non-Deterministic Algorithms.pdf:application/pdf},
}

@article{khanIterativeNondeterministicAlgorithms2013,
	title = {Iterative non-deterministic algorithms in on-shore wind farm design: A brief survey},
	volume = {19},
	issn = {1364-0321},
	url = {https://www.sciencedirect.com/science/article/pii/S1364032112006521},
	doi = {10.1016/j.rser.2012.11.040},
	shorttitle = {Iterative non-deterministic algorithms in on-shore wind farm design},
	abstract = {Wind farm layout design is a complex optimization problem consisting of number of design objectives and constraints. Different variations of this problem have been solved using several optimization techniques. Iterative heuristics are well-known optimization techniques that have been applied to a variety of complex optimization problems. This paper briefly outlines the design issues and constraints involved in the wind farm layout design, computational complexity of the problem, and single-objective and multi-objective aspects of the problem. The main focus of the paper is a brief survey of all iterative non-deterministic algorithms that have been applied to solve the wind farm layout design problem.},
	pages = {370--384},
	journaltitle = {Renewable and Sustainable Energy Reviews},
	shortjournal = {Renewable and Sustainable Energy Reviews},
	author = {Khan, Salman A. and Rehman, Shafiqur},
	urldate = {2024-07-30},
	date = {2013-03-01},
	keywords = {letto, Genetic algorithms, Iterative heuristics, Optimization methods, Swarm intelligence, Wind farm layout design},
	file = {Khan e Rehman - 2013 - Iterative non-deterministic algorithms in on-shore.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\GE66E9TB\\Khan e Rehman - 2013 - Iterative non-deterministic algorithms in on-shore.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\FNYHZ9QA\\S1364032112006521.html:text/html},
}

@article{olzaDevelopmentValidationPredictive2023,
	title = {Development and validation of predictive models for unplanned hospitalization in the Basque Country: analyzing the variability of non-deterministic algorithms},
	volume = {23},
	issn = {1472-6947},
	url = {https://doi.org/10.1186/s12911-023-02226-z},
	doi = {10.1186/s12911-023-02226-z},
	shorttitle = {Development and validation of predictive models for unplanned hospitalization in the Basque Country},
	abstract = {The progressive ageing in developed countries entails an increase in multimorbidity. Population-wide predictive models for adverse health outcomes are crucial to address these growing healthcare needs. The main objective of this study is to develop and validate a population-based prognostic model to predict the probability of unplanned hospitalization in the Basque Country, through comparing the performance of a logistic regression model and three families of machine learning models.},
	pages = {152},
	number = {1},
	journaltitle = {{BMC} Medical Informatics and Decision Making},
	shortjournal = {{BMC} Medical Informatics and Decision Making},
	author = {Olza, Alexander and Millán, Eduardo and Rodríguez-Álvarez, María Xosé},
	urldate = {2024-07-30},
	date = {2023-08-05},
	keywords = {letto, Hospitalization, Non-deterministic algorithms, Predictive models},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\VAIVWD7V\\Olza et al. - 2023 - Development and validation of predictive models fo.pdf:application/pdf;Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\AT6PNXEU\\s12911-023-02226-z.html:text/html},
}

@article{bucklessContrastCodingRefinement1990,
	title = {Contrast Coding: A Refinement of {ANOVA} in Behavioral Analysis},
	volume = {65},
	issn = {0001-4826},
	url = {https://www.proquest.com/docview/1301316132/citation/87437134C2BD4134PQ/1?sourcetype=Scholarly%20Journals},
	shorttitle = {Contrast Coding},
	abstract = {As behavioral accounting research has progressed, research designs have become more complex. New topic areas are initially investigated using single-factor designs, which may result in significant effects and a substantial amount of explained variation. In subsequent investigations, explained variation is increased by employing factorial designs and/or multiple-level explanatory variables. Traditionally, {ANOVA} is the statistical analysis approach employed to test research hypotheses. A limitation of {ANOVA} is that it only detects significant differences among cell means, but does not indicate the functional form of the relationship among cell means. We propose that researchers employ contrast coding—a refinement of {ANOVA}—to test research hypotheses. Contrast coding requires researchers to specify a priori the functional form of the relationship among cell means. This article demonstrates that contrast coding provides greater statistical power than the conventional {ANOVA} without increasing Type I error rates. Examples from recent accounting literature illustrate when the use of contrast coding is most advantageous. The examples include a factorial design and a single-factor, multiple-level experiment. Formulas for calculating the net benefit of employing contrast coding and the significance of the net benefit are presented. The examples and analysis support the use of contrast coding with certain research designs. The primary benefit of the a priori specification required by contrast coding is greater statistical power.},
	number = {4},
	journaltitle = {The Accounting Review},
	author = {Buckless, Frank A. and Ravenscroft, Sue P.},
	urldate = {2024-07-31},
	date = {1990-10-01},
	note = {Num Pages: 13
Place: Menasha, Wis., United States
Publisher: American Accounting Association},
	keywords = {contrasts\_coding, letto},
	file = {Buckless e Ravenscroft - 1990 - Contrast Coding A Refinement of ANOVA in Behavior.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\HYRX9CE4\\Buckless e Ravenscroft - 1990 - Contrast Coding A Refinement of ANOVA in Behavior.pdf:application/pdf},
}

@book{cohenAppliedMultipleRegression2002,
	location = {New York},
	edition = {3},
	title = {Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences},
	isbn = {978-0-203-77444-1},
	abstract = {This classic text on multiple regression is noted for its nonmathematical, applied, and data-analytic approach. Readers profit from its verbal-conceptual exposition and frequent use of examples.
The applied emphasis provides clear illustrations of the principles and provides worked examples of the types of applications that are possible. Researchers learn how to specify regression models that directly address their research questions. An overview of the fundamental ideas of multiple regression and a review of bivariate correlation and regression and other elementary statistical concepts provide a strong foundation for understanding the rest of the text. The third edition features an increased emphasis on graphics and the use of confidence intervals and effect size measures, and an accompanying website with data for most of the numerical examples along with the computer code for {SPSS}, {SAS}, and {SYSTAT}, at www.psypress.com/9780805822236 . ;
Applied Multiple Regression serves as both a textbook for graduate students and as a reference tool for researchers in psychology, education, health sciences, communications, business, sociology, political science, anthropology, and economics. An introductory knowledge of statistics is required. Self-standing chapters minimize the need for researchers to refer to previous chapters.},
	pagetotal = {536},
	publisher = {Routledge},
	author = {Cohen, Jacob and Cohen, Patricia and West, Stephen G. and Aiken, Leona S.},
	date = {2002-08-01},
	doi = {10.4324/9780203774441},
	keywords = {to\_read},
}

@article{cohenThingsHaveLearned1990a,
	title = {Things I Have Learned (So Far)},
	abstract = {This is an account of what I have learned (so far) about the application of statistics to psychology and the other sociobiomedical sciences. It includes the principles "less is more" (fewer variables, more highly targeted issues, sharp rounding off), "simple is better" (graphic representation, unit weighting for linear composites), and "some things you learn aren't so." I have learned to avoid the many misconceptions that surround Fisherian null hypothesis testing. I have also learned the importance of power analysis and the determination of just how big (rather than how statistically significant) are the effects that we study. Finally, I have learned that there is no royal road to statistical induction, that the informed judgment of the investigator is the crucial element in the interpretation of data, and that things take time.},
	journaltitle = {American Psychologist},
	author = {Cohen, Jacob},
	date = {1990},
	langid = {english},
	file = {Cohen - 1990 - Things I Have Learned (So Far).pdf:C\:\\Users\\fgfra\\Zotero\\storage\\NYA88YTL\\Cohen - 1990 - Things I Have Learned (So Far).pdf:application/pdf},
}

@article{cohenThingsHaveLearned1990,
	title = {Things I Have Learned (So Far)},
	abstract = {This is an account of what I have learned (so far) about the application of statistics to psychology and the other sociobiomedical sciences. It includes the principles "less is more" (fewer variables, more highly targeted issues, sharp rounding off), "simple is better" (graphic representation, unit weighting for linear composites), and "some things you learn aren't so." I have learned to avoid the many misconceptions that surround Fisherian null hypothesis testing. I have also learned the importance of power analysis and the determination of just how big (rather than how statistically significant) are the effects that we study. Finally, I have learned that there is no royal road to statistical induction, that the informed judgment of the investigator is the crucial element in the interpretation of data, and that things take time.},
	journaltitle = {American Psychologist},
	author = {Cohen, Jacob},
	date = {1990},
	langid = {english},
	file = {Cohen - 1990 - Things I Have Learned (So Far).pdf:C\:\\Users\\fgfra\\Zotero\\storage\\DUG7P8Z8\\Cohen - 1990 - Things I Have Learned (So Far).pdf:application/pdf},
}

@article{richardsCleaningDataGuess2012,
	title = {Cleaning data: guess the olympian},
	volume = {34},
	rights = {© 2012 The Authors. Teaching Statistics © 2012 Teaching Statistics Trust},
	issn = {1467-9639},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9639.2011.00495.x},
	doi = {10.1111/j.1467-9639.2011.00495.x},
	shorttitle = {Cleaning data},
	abstract = {This article tackles the problem of what should be done with real textual data that are contaminated by errors of recording, particularly when the data contain words that are misspelt, unintentionally or otherwise.},
	pages = {31--37},
	number = {1},
	journaltitle = {Teaching Statistics},
	author = {Richards, Kate and Davies, Neville},
	urldate = {2024-08-01},
	date = {2012},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9639.2011.00495.x},
	keywords = {Problem solving, Decision making, Teaching},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\SLTJKX7Y\\Richards e Davies - 2012 - Cleaning data guess the olympian.pdf:application/pdf;Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\5D6MYCPU\\j.1467-9639.2011.00495.html:text/html},
}

@book{mitcriticaldataSecondaryAnalysisElectronic2016,
	location = {Cham},
	title = {Secondary Analysis of Electronic Health Records},
	volume = {Chapter 12},
	rights = {https://creativecommons.org/licenses/by-nc/4.0},
	isbn = {978-3-319-43740-8 978-3-319-43742-2},
	url = {http://link.springer.com/10.1007/978-3-319-43742-2},
	publisher = {Springer International Publishing},
	author = {{Mit Critical Data}},
	urldate = {2024-08-01},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-43742-2},
	keywords = {letto, capitolo\_12, data\_pre\_processing},
	file = {Mit Critical Data - 2016 - Secondary Analysis of Electronic Health Records.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\JQN5YS7W\\Mit Critical Data - 2016 - Secondary Analysis of Electronic Health Records.pdf:application/pdf},
}

@article{shiAutomatedDataCleaning2021,
	title = {An automated data cleaning method for Electronic Health Records by incorporating clinical knowledge},
	volume = {21},
	issn = {1472-6947},
	url = {https://doi.org/10.1186/s12911-021-01630-7},
	doi = {10.1186/s12911-021-01630-7},
	abstract = {The use of Electronic Health Records ({EHR}) data in clinical research is incredibly increasing, but the abundancy of data resources raises the challenge of data cleaning. It can save time if the data cleaning can be done automatically. In addition, the automated data cleaning tools for data in other domains often process all variables uniformly, meaning that they cannot serve well for clinical data, as there is variable-specific information that needs to be considered. This paper proposes an automated data cleaning method for {EHR} data with clinical knowledge taken into consideration.},
	pages = {267},
	number = {1},
	journaltitle = {{BMC} Medical Informatics and Decision Making},
	shortjournal = {{BMC} Medical Informatics and Decision Making},
	author = {Shi, Xi and Prins, Charlotte and Van Pottelbergh, Gijs and Mamouris, Pavlos and Vaes, Bert and De Moor, Bart},
	urldate = {2024-08-01},
	date = {2021-09-17},
	keywords = {Automated method, Clinical decision support, Data cleaning},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\UA8KR7HJ\\Shi et al. - 2021 - An automated data cleaning method for Electronic H.pdf:application/pdf;Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\ETMPTPQ3\\s12911-021-01630-7.html:text/html},
}

@article{osborneDataCleaningTesting2013,
	title = {Is data cleaning and the testing of assumptions relevant in the 21st century?},
	volume = {4},
	issn = {1664-1078},
	doi = {10.3389/fpsyg.2013.00370},
	pages = {370},
	journaltitle = {Frontiers in Psychology},
	shortjournal = {Front Psychol},
	author = {Osborne, Jason W.},
	date = {2013},
	pmid = {23805118},
	pmcid = {PMC3691503},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\BB5J6CYA\\Osborne - 2013 - Is data cleaning and the testing of assumptions re.pdf:application/pdf},
}

@article{vandenbroeckDataCleaningDetecting2005,
	title = {Data cleaning: detecting, diagnosing, and editing data abnormalities},
	volume = {2},
	issn = {1549-1676},
	doi = {10.1371/journal.pmed.0020267},
	shorttitle = {Data cleaning},
	abstract = {In this policy forum the authors argue that data cleaning is an essential part of the research process, and should be incorporated into study design.},
	pages = {e267},
	number = {10},
	journaltitle = {{PLoS} medicine},
	shortjournal = {{PLoS} Med},
	author = {Van den Broeck, Jan and Cunningham, Solveig Argeseanu and Eeckels, Roger and Herbst, Kobus},
	date = {2005-10},
	pmid = {16138788},
	pmcid = {PMC1198040},
	keywords = {Epidemiologic Studies, Ethics, Professional, Guidelines as Topic, Humans, Peer Review, Reproducibility of Results, Statistics as Topic},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\IP7JT3FS\\Van den Broeck et al. - 2005 - Data cleaning detecting, diagnosing, and editing .pdf:application/pdf},
}

@article{baileyIntroducingRELAXAutomated2023,
	title = {Introducing {RELAX}: An automated pre-processing pipeline for cleaning {EEG} data - Part 1: Algorithm and application to oscillations},
	volume = {149},
	issn = {1388-2457},
	url = {https://www.sciencedirect.com/science/article/pii/S1388245723000275},
	doi = {10.1016/j.clinph.2023.01.017},
	shorttitle = {Introducing {RELAX}},
	abstract = {Objective
Electroencephalographic ({EEG}) data are often contaminated with non-neural artifacts which can confound experimental results. Current artifact cleaning approaches often require costly manual input. Our aim was to provide a fully automated {EEG} cleaning pipeline that addresses all artifact types and improves measurement of {EEG} outcomes
Methods
We developed {RELAX} (the Reduction of Electroencephalographic Artifacts). {RELAX} cleans continuous data using Multi-channel Wiener filtering [{MWF}] and/or wavelet enhanced independent component analysis [{wICA}] applied to artifacts identified by {ICLabel} [{wICA}\_ICLabel]). Several versions of {RELAX} were compared using three datasets (N = 213, 60 and 23 respectively) against six commonly used pipelines across a range of artifact cleaning metrics, including measures of remaining blink and muscle activity, and the variance explained by experimental manipulations after cleaning.
Results
{RELAX} with {MWF} and {wICA}\_ICLabel showed amongst the best performance at cleaning blink and muscle artifacts while preserving neural signal. {RELAX} with {wICA}\_ICLabel only may perform better at differentiating alpha oscillations between working memory conditions.
Conclusions
{RELAX} provides automated, objective and high-performing {EEG} cleaning, is easy to use, and freely available on {GitHub}.
Significance
We recommend {RELAX} for data cleaning across {EEG} studies to reduce artifact confounds, improve outcome measurement and improve inter-study consistency.},
	pages = {178--201},
	journaltitle = {Clinical Neurophysiology},
	shortjournal = {Clinical Neurophysiology},
	author = {Bailey, N. W. and Biabani, M. and Hill, A. T. and Miljevic, A. and Rogasch, N. C. and {McQueen}, B. and Murphy, O. W. and Fitzgerald, P. B.},
	urldate = {2024-08-01},
	date = {2023-05-01},
	keywords = {Artifact reduction, Blinks, Electroencephalography, Muscle, Neural oscillations, Pre-processing},
	file = {ScienceDirect Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\48GIE9UV\\S1388245723000275.html:text/html;Versione inviata:C\:\\Users\\fgfra\\Zotero\\storage\\4DXPMJYW\\Bailey et al. - 2023 - Introducing RELAX An automated pre-processing pip.pdf:application/pdf},
}

@inproceedings{gharatkarReviewPreprocessingUsing2017,
	title = {Review preprocessing using data cleaning and stemming technique},
	url = {https://ieeexplore.ieee.org/abstract/document/8276011},
	doi = {10.1109/ICIIECS.2017.8276011},
	abstract = {The opinions and experiences of other people constitute an important source of information in our everyday life. For example, we ask our friends which dentist, restaurant, or Smartphone they would recommend to us. Nowadays, online customer reviews have become an invaluable resource to answer such questions. Besides helping consumers to make more informed purchase decisions, online reviews are also of great value to vendors, as they represent unsolicited and genuine customer feedback that is conveniently available at virtually no costs. However, for popular products there often exist several thousands of reviews so that manual analysis is not an option. The reviews need cleaning before analysis the proposed system provide the architecture that cleans the unwanted data and provides quality data for analysis. The existing systems provides basic data cleaning steps such as tokenization, stop word removal, {URLs} removal, special character removal and hash tag removal which all are core steps in data cleaning. The proposed system implements all core steps and perform additional steps like slang word replacement, spell checker and stemming which help to produce quality data for sentiment analysis.},
	eventtitle = {2017 International Conference on Innovations in Information, Embedded and Communication Systems ({ICIIECS})},
	pages = {1--4},
	booktitle = {2017 International Conference on Innovations in Information, Embedded and Communication Systems ({ICIIECS})},
	author = {Gharatkar, Sandesh and Ingle, Aakash and Naik, Tanmay and Save, Ashwini},
	urldate = {2024-08-01},
	date = {2017-03},
	keywords = {Data mining, Cleaning, Communication systems, Data Cleaning, Dictionaries, Noise measurement, Preprocessing, review processing, Sentiment analysis, Stemming, Technological innovation},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\fgfra\\Zotero\\storage\\EBTLGJXD\\8276011.html:text/html;IEEE Xplore Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\HKGTE5SH\\Gharatkar et al. - 2017 - Review preprocessing using data cleaning and stemm.pdf:application/pdf},
}

@article{joshiDataPreprocessingTechniques2021a,
	title = {Data Preprocessing: The Techniques for Preparing Clean and Quality Data for Data Analytics Process},
	volume = {13},
	issn = {23208481, 09746471},
	url = {https://www.computerscijournal.org/vol13no23/data-preprocessing-the-techniques-for-preparing-clean-and-quality-data-for-data-analytics-process/},
	doi = {10.13005/ojcst13.0203.03},
	shorttitle = {Data Preprocessing},
	abstract = {The model and pattern for real time data mining have an important role for decision making. The meaningful real time data mining is basically depends on the quality of data while row or rough data available at warehouse. The data available at warehouse can be in any format, it may huge or it may unstructured. These kinds of data require some process to enhance the efficiency of data analysis. The process to make it ready to use is called data preprocessing. There can be many activities for data preprocessing such as data transformation, data cleaning, data integration, data optimization and data conversion which are use to converting the rough data to quality data. The data preprocessing techniques are the vital step for the data mining. The analyzed result will be good as far as data quality is good. This paper is about the different data preprocessing techniques which can be use for preparing the quality data for the data analysis for the available rough data.},
	pages = {78--81},
	number = {203},
	journaltitle = {Oriental journal of computer science and technology},
	shortjournal = {Orient. J. Comp. Sci. and Technol},
	author = {Joshi, Ashish P. and Patel, Biraj V.},
	urldate = {2024-08-01},
	date = {2021-01-30},
	langid = {english},
	file = {Joshi e Patel - 2021 - Data Preprocessing The Techniques for Preparing C.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\FSV94FFH\\Joshi e Patel - 2021 - Data Preprocessing The Techniques for Preparing C.pdf:application/pdf},
}

@book{russellHistoryWesternPhilosophy1946,
	title = {History of Western Philosophy},
	publisher = {Routledge},
	author = {Russell, Bertrand},
	date = {1946},
	keywords = {risorsa\_indirettamente\_consultata},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\6J3TPNVU\\RUSHOW.html:text/html},
}

@article{laceyVisualisingProblemsBalancing2015a,
	title = {Visualising the problems with balancing lithium–sulfur batteries by “mapping” internal resistance},
	volume = {51},
	issn = {1359-7345, 1364-548X},
	url = {https://xlink.rsc.org/?DOI=C5CC07167D},
	doi = {10.1039/C5CC07167D},
	abstract = {Stability and failure mechanisms in optimised lithium–sulfur cells are studied with a new, versatile and convenient approach to tracking changes in resistance with automated data analysis.
          , 
            Frequent and continuous determination of battery internal resistance by a simple current-interrupt method enables the visualisation of cell behaviour through the creation of resistance “maps”, showing changes in resistance as a function of both capacity and cycle number. This new approach is applied here for the investigation of cell failure in the lithium–sulfur system with Li electrode excesses optimised towards practically relevant specifications.},
	pages = {16502--16505},
	number = {92},
	journaltitle = {Chemical Communications},
	shortjournal = {Chem. Commun.},
	author = {Lacey, Matthew J. and Edström, Kristina and Brandell, Daniel},
	urldate = {2024-08-01},
	date = {2015},
	langid = {english},
	file = {Lacey et al. - 2015 - Visualising the problems with balancing lithium–su.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\WH6K5YRN\\Lacey et al. - 2015 - Visualising the problems with balancing lithium–su.pdf:application/pdf},
}

@online{BayesianDataAnalysisa,
	title = {Bayesian Data Analysis},
	url = {https://web.p.ebscohost.com/ehost/ebookviewer/ebook/bmxlYmtfXzE3NjMyNDRfX0FO0?sid=425de7d8-9a3a-49e6-bee3-49c3f3421dab%40redis&vid=9&format=EB&rid=1},
	urldate = {2024-08-02},
	file = {Bayesian Data Analysis:C\:\\Users\\fgfra\\Zotero\\storage\\DX4FV9IU\\bmxlYmtfXzE3NjMyNDRfX0FO0.html:text/html},
}

@article{mcshaneAbandonStatisticalSignificance2019,
	title = {Abandon Statistical Significance},
	volume = {73},
	doi = {10.1080/00031305.2018.1527253},
	abstract = {We discuss problems the null hypothesis significance testing ({NHST}) paradigm poses for replication and more broadly in the biomedical and social sciences as well as how these problems remain unresolved by proposals involving modified p-value thresholds, confidence intervals, and Bayes factors. We then discuss our own proposal, which is to abandon statistical significance. We recommend dropping the {NHST} paradigm--and the p-value thresholds intrinsic to it--as the default statistical paradigm for research, publication, and discovery in the biomedical and social sciences. Specifically, we propose that the p-value be demoted from its threshold screening role and instead, treated continuously, be considered along with currently subordinate factors (e.g., related prior evidence, plausibility of mechanism, study design and data quality, real world costs and benefits, novelty of finding, and other factors that vary by research domain) as just one among many pieces of evidence. We have no desire to "ban" p-values or other purely statistical measures. Rather, we believe that such measures should not be thresholded and that, thresholded or not, they should not take priority over the currently subordinate factors. We also argue that it seldom makes sense to calibrate evidence as a function of p-values or other purely statistical measures. We offer recommendations for how our proposal can be implemented in the scientific publication process as well as in statistical decision making more broadly.},
	pages = {235--245},
	number = {1},
	journaltitle = {Grantee Submission},
	author = {{McShane}, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
	date = {2019-01-01},
	keywords = {Statistics, Statistical Significance, Social Sciences, Biomedicine, Replication (Evaluation), Scientific Research},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\ZCDUNZ6E\\McShane et al. - 2019 - Abandon Statistical Significance.pdf:application/pdf},
}

@article{rubinTypeErrorRates2024,
	title = {Type I Error Rates are Not Usually Inflated},
	rights = {© 2024. This work is published under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the {ProQuest} Terms and Conditions, you may use this content in accordance with the terms of the License.},
	url = {https://www.proquest.com/docview/2900744976?pq-origsite=primo&parentSessionId=UVHsTE4OBQbV1axja8bt9497yuukdsQM8BEQeRdbrOM%3D&sourcetype=Working%20Papers},
	doi = {10.31222/osf.io/3kv2b},
	abstract = {The inflation of Type I error rates is thought to be one of the causes of the replication crisis. Questionable research practices such as p-hacking are thought to inflate Type I error rates above their nominal level, leading to unexpectedly high levels of false positives in the literature and, consequently, unexpectedly low replication rates. In this article, I offer an alternative view. I argue that questionable and other research practices do not usually inflate relevant Type I error rates. I begin with an introduction to Type I error rates that distinguishes them from theoretical errors. I then illustrate my argument with respect to model misspecification, multiple testing, selective inference, forking paths, exploratory analyses, p-hacking, optional stopping, double dipping, and {HARKing}. In each case, I demonstrate that relevant Type I error rates are not usually inflated above their nominal level, and in the rare cases that they are, the inflation is easily identified and resolved. I conclude that the replication crisis may be explained, at least in part, by researchers' misinterpretation of statistical errors and their underestimation of theoretical errors.},
	journaltitle = {{arXiv}.org},
	author = {Rubin, Mark},
	urldate = {2024-08-02},
	date = {2024-01-03},
	note = {Place: Ithaca, United States
Publisher: Cornell University Library, {arXiv}.org
Section: Statistics},
	keywords = {Methodology, Replication},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\WM5YU3UK\\Rubin - 2024 - Type I Error Rates are Not Usually Inflated.pdf:application/pdf},
}

@article{gelmanGardenForkingPaths2013,
	title = {The garden of forking paths: Why multiple comparisons can be a problem, even when there is no “ﬁshing expedition” or “p-hacking” and the research hypothesis was posited ahead of time},
	abstract = {Researcher degrees of freedom can lead to a multiple comparisons problem, even in settings where researchers perform only a single analysis on their data. The problem is there can be a large number of potential comparisons when the details of data analysis are highly contingent on data, without the researcher having to perform any conscious procedure of ﬁshing or examining multiple p-values. We discuss in the context of several examples of published papers where data-analysis decisions were theoretically-motivated based on previous literature, but where the details of data selection and analysis were not pre-speciﬁed and, as a result, were contingent on data.},
	author = {Gelman, Andrew and Loken, Eric},
	date = {2013-11-14},
	langid = {english},
	file = {Gelman e Loken - The garden of forking paths Why multiple comparis.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\D57RV4AN\\Gelman e Loken - The garden of forking paths Why multiple comparis.pdf:application/pdf},
}

@article{lindquistZenArtMultiple2015,
	title = {Zen and the art of multiple comparisons},
	volume = {77},
	issn = {1534-7796},
	doi = {10.1097/PSY.0000000000000148},
	abstract = {{OBJECTIVE}: The need for appropriate multiple comparisons correction when performing statistical inference is not a new problem. However, it has come to the forefront in many new modern data-intensive disciplines. For example, researchers in areas such as imaging and genetics are routinely required to simultaneously perform thousands of statistical tests. Ignoring this multiplicity can cause severe problems with false positives, thereby introducing nonreproducible results into the literature.
{METHODS}: This article serves as an introduction to hypothesis testing and multiple comparisons for practical research applications, with a particular focus on its use in the analysis of functional magnetic resonance imaging data.
{RESULTS}: We discuss hypothesis testing and a variety of principled techniques for correcting for multiple tests. We also illustrate potential pitfalls problems that can occur if the multiple comparisons issue is not dealt with properly. We conclude, by discussing effect size estimation, an issue often linked with the multiple comparisons problem.
{CONCLUSIONS}: Failure to properly account for multiple comparisons will ultimately lead to heightened risks for false positives and erroneous conclusions.},
	pages = {114--125},
	number = {2},
	journaltitle = {Psychosomatic Medicine},
	shortjournal = {Psychosom Med},
	author = {Lindquist, Martin A. and Mejia, Amanda},
	date = {2015},
	pmid = {25647751},
	pmcid = {PMC4333023},
	keywords = {Humans, Statistics as Topic, Data Interpretation, Statistical, False Positive Reactions, Functional Neuroimaging, Magnetic Resonance Imaging, Research Design, Sample Size},
	file = {Versione accettata:C\:\\Users\\fgfra\\Zotero\\storage\\MS37UFLN\\Lindquist e Mejia - 2015 - Zen and the art of multiple comparisons.pdf:application/pdf},
}

@article{gordonEvaluationFreelyAvailable2022,
	title = {Evaluation of freely available data profiling tools for health data research application: a functional evaluation review},
	volume = {12},
	issn = {2044-6055},
	doi = {10.1136/bmjopen-2021-054186},
	shorttitle = {Evaluation of freely available data profiling tools for health data research application},
	abstract = {{OBJECTIVES}: To objectively evaluate freely available data profiling software tools using healthcare data.
{DESIGN}: Data profiling tools were evaluated for their capabilities using publicly available information and data sheets. From initial assessment, several underwent further detailed evaluation for application on healthcare data using a synthetic dataset of 1000 patients and associated data using a common health data model, and tools scored based on their functionality with this dataset.
{SETTING}: Improving the quality of healthcare data for research use is a priority. Profiling tools can assist by evaluating datasets across a range of quality dimensions. Several freely available software packages with profiling capabilities are available but healthcare organisations often have limited data engineering capability and expertise.
{PARTICIPANTS}: 28 profiling tools, 8 undergoing evaluation on synthetic dataset of 1000 patients.
{RESULTS}: Of 28 potential profiling tools initially identified, 8 showed high potential for applicability with healthcare datasets based on available documentation, of which two performed consistently well for these purposes across multiple tasks including determination of completeness, consistency, uniqueness, validity, accuracy and provision of distribution metrics.
{CONCLUSIONS}: Numerous freely available profiling tools are serviceable for potential use with health datasets, of which at least two demonstrated high performance across a range of technical data quality dimensions based on testing with synthetic health dataset and common data model. The appropriate tool choice depends on factors including underlying organisational infrastructure, level of data engineering and coding expertise, but there are freely available tools helping profile health datasets for research use and inform curation activity.},
	pages = {e054186},
	number = {5},
	journaltitle = {{BMJ} open},
	shortjournal = {{BMJ} Open},
	author = {Gordon, Ben and Fennessy, Clara and Varma, Susheel and Barrett, Jake and {McCondochie}, Enez and Heritage, Trevor and Duroe, Oenone and Jeffery, Richard and Rajamani, Vishnu and Earlam, Kieran and Banda, Victor and Sebire, Neil},
	date = {2022-05-09},
	pmid = {35534084},
	pmcid = {PMC9086620},
	keywords = {Humans, Delivery of Health Care, health informatics, information management, information technology, Software, data\_profiling},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\PBECV55F\\Gordon et al. - 2022 - Evaluation of freely available data profiling tool.pdf:application/pdf},
}

@article{ruddleTasksVisualizationsUsed2024,
	title = {Tasks and Visualizations Used for Data Profiling: A Survey and Interview Study},
	volume = {30},
	issn = {1941-0506},
	doi = {10.1109/TVCG.2023.3234337},
	shorttitle = {Tasks and Visualizations Used for Data Profiling},
	abstract = {The use of good-quality data to inform decision making is entirely dependent on robust processes to ensure it is fit for purpose. Such processes vary between organisations, and between those tasked with designing and following them. In this article we report on a survey of 53 data analysts from many industry sectors, 24 of whom also participated in in-depth interviews, about computational and visual methods for characterizing data and investigating data quality. The paper makes contributions in two key areas. The first is to data science fundamentals, because our lists of data profiling tasks and visualization techniques are more comprehensive than those published elsewhere. The second concerns the application question "what does good profiling look like to those who routinely perform it?", which we answer by highlighting the diversity of profiling tasks, unusual practice and exemplars of visualization, and recommendations about formalizing processes and creating rulebooks.},
	pages = {3400--3412},
	number = {7},
	journaltitle = {{IEEE} transactions on visualization and computer graphics},
	shortjournal = {{IEEE} Trans Vis Comput Graph},
	author = {Ruddle, Roy A. and Cheshire, James and Fernstad, Sara Johansson},
	date = {2024-07},
	pmid = {37018563},
	keywords = {data\_profiling},
	file = {Full text:C\:\\Users\\fgfra\\Zotero\\storage\\9BEZ7XGM\\Ruddle et al. - 2024 - Tasks and Visualizations Used for Data Profiling .pdf:application/pdf},
}

@inproceedings{coutoNewTrendsBig2022,
	location = {Cham},
	title = {New Trends in Big Data Profiling},
	isbn = {978-3-031-10461-9},
	doi = {10.1007/978-3-031-10461-9_55},
	abstract = {A known challenge related to big data is that data ingestion occurs continuously and at high speed, and the data profile can quickly vary because of this dynamism. Data profiling can range from simple summaries to more complex statistics, which is essential for understanding the data. For a data scientist, it is essential to know the profile of the data to be handled, and this information needs to be updated according to the new data that is continuously arriving. This paper reviews the literature about how data profiling is being used in big data ecosystems. We search in eight relevant web databases to map the papers that present big data profiling trends. We focus on categorizing and reviewing the current progress on big data profiling for the leading tools, scenarios, datasets, metadata, and information extracted. Finally, we explore some potential future issues and challenges in big data profiling research.},
	pages = {808--825},
	booktitle = {Intelligent Computing},
	publisher = {Springer International Publishing},
	author = {Couto, Júlia Colleoni and Damasio, Juliana and Bordini, Rafael and Ruiz, Duncan},
	editor = {Arai, Kohei},
	date = {2022},
	langid = {english},
	keywords = {data\_profiling, Big data, Data lakes, Data profiling},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\DZV5FKGQ\\Couto et al. - 2022 - New Trends in Big Data Profiling.pdf:application/pdf},
}

@inproceedings{abedjanDataProfilingTutorial2017,
	location = {New York, {NY}, {USA}},
	title = {Data Profiling: A Tutorial},
	isbn = {978-1-4503-4197-4},
	url = {https://dl.acm.org/doi/10.1145/3035918.3054772},
	doi = {10.1145/3035918.3054772},
	series = {{SIGMOD} '17},
	shorttitle = {Data Profiling},
	abstract = {is to understand the dataset at hand and its metadata. The process of metadata discovery is known as data profiling. Profiling activities range from ad-hoc approaches, such as eye-balling random subsets of the data or formulating aggregation queries, to systematic inference of structural information and statistics of a dataset using dedicated profiling tools. In this tutorial, we highlight the importance of data profiling as part of any data-related use-case, and we discuss the area of data profiling by classifying data profiling tasks and reviewing the state-of-the-art data profiling systems and techniques. In particular, we discuss hard problems in data profiling, such as algorithms for dependency discovery and profiling algorithms for dynamic data and streams. We also pay special attention to visualizing and interpreting the results of data profiling. We conclude with directions for future research in the area of data profiling. This tutorial is based on our survey on profiling relational data [2].},
	pages = {1747--1751},
	booktitle = {Proceedings of the 2017 {ACM} International Conference on Management of Data},
	publisher = {Association for Computing Machinery},
	author = {Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix},
	urldate = {2024-08-09},
	date = {2017},
	keywords = {data\_profiling},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\MWH2J9P8\\Abedjan et al. - 2017 - Data Profiling A Tutorial.pdf:application/pdf},
}

@inbook{abedjanDataProfilingTools2019,
	location = {Cham},
	title = {Data Profiling Tools},
	rights = {https://www.springer.com/tdm},
	isbn = {978-3-031-00737-8 978-3-031-01865-7},
	url = {https://link.springer.com/10.1007/978-3-031-01865-7_8},
	pages = {97--101},
	booktitle = {Data Profiling},
	publisher = {Springer International Publishing},
	author = {Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix and Papenbrock, Thorsten},
	bookauthor = {Abedjan, Ziawasch and Golab, Lukasz and Naumann, Felix and Papenbrock, Thorsten},
	urldate = {2024-08-09},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-031-01865-7_8},
	note = {Series Title: Synthesis Lectures on Data Management},
	keywords = {data\_profiling},
	file = {Abedjan et al. - 2019 - Data Profiling Tools.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\SHLPINWS\\Abedjan et al. - 2019 - Data Profiling Tools.pdf:application/pdf},
}

@software{rcoreteamLanguageEnvironmentStatistical2024,
	location = {Vienna, Austria},
	title = {R: A Language and Environment for Statistical Computing},
	url = {https://www.R-project.org/},
	version = {4.4.1},
	author = {R Core Team},
	date = {2024},
	keywords = {software\_and\_packages},
}

@software{vanrossumPythonReferenceManual2009,
	title = {Python 3 Reference Manual},
	isbn = {1-4414-1269-7},
	author = {Van Rossum, Guido and Drake, Fred L.},
	date = {2009},
	keywords = {software\_and\_packages},
}

@article{quininoUsingCoefficientDetermination2013,
	title = {Using the coefficient of determination R2 to test the significance of multiple linear regression},
	volume = {35},
	issn = {0141982X},
	url = {https://search.ebscohost.com/login.aspx?direct=true&db=eue&AN=86462613&site=ehost-live},
	doi = {10.1111/j.1467-9639.2012.00525.x},
	abstract = {This article proposes the use of the coefficient of determination as a statistic for hypothesis testing in multiple linear regression based on distributions acquired by beta sampling.},
	pages = {84--88},
	number = {2},
	journaltitle = {Teaching Statistics},
	author = {Quinino, Roberto C. and Reis, Edna A. and Bessegato, Lupércio F.},
	urldate = {2024-08-17},
	date = {2013},
	keywords = {Regression analysis, Beta distribution, Coefficients (Statistics), Statistical hypothesis testing, Statistical models},
	file = {EBSCO Full Text:C\:\\Users\\fgfra\\Zotero\\storage\\2RJ8GF22\\Quinino et al. - 2013 - Using the coefficient of determination R2 to test .pdf:application/pdf},
}

@article{gargCoefficientDeterminationMultiple2014,
	title = {Coefficient of determination for multiple measurement error models},
	volume = {126},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X14000141},
	doi = {10.1016/j.jmva.2014.01.006},
	abstract = {The coefficient of determination (R2) is used for judging the goodness of fit in a linear regression model. It is the square of the multiple correlation coefficient between the study and explanatory variables based on the sample values. It gives valid results only when the observations are correctly observed without any measurement error. The conventional R2 provides invalid results in the presence of measurement errors in the data because the sample R2 becomes an inconsistent estimator of its population counterpart which is the square of the population multiple correlation coefficient between the study and explanatory variables. The goodness of fit statistics based on the variants of R2 for multiple measurement error models have been proposed in this paper. These variants are based on the utilization of the two forms of additional information from outside the sample. The two forms are the known covariance matrix of measurement errors associated with the explanatory variables and the known reliability matrix associated with the explanatory variables. The asymptotic properties of the conventional R2 and the proposed variants of R2 like goodness of fit statistics have been studied analytically and numerically.},
	pages = {137--152},
	journaltitle = {Journal of Multivariate Analysis},
	shortjournal = {Journal of Multivariate Analysis},
	author = {Garg, G. and {Shalabh}},
	urldate = {2024-08-17},
	date = {2014-04-01},
	keywords = {Linear regression, Coefficient of determination (), Measurement error, Non-normal distribution, Ultrastructural model},
	file = {ScienceDirect Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\VP2H96RN\\S0047259X14000141.html:text/html;Versione inviata:C\:\\Users\\fgfra\\Zotero\\storage\\72XBSP8R\\Cheng et al. - 2014 - Coefficient of determination for multiple measurem.pdf:application/pdf},
}

@article{batesFittingLinearMixedEffects2015a,
	title = {Fitting Linear Mixed-Effects Models Using \textbf{lme4}},
	volume = {67},
	issn = {1548-7660},
	url = {http://www.jstatsoft.org/v67/i01/},
	doi = {10.18637/jss.v067.i01},
	abstract = {Maximum likelihood or restricted maximum likelihood ({REML}) estimates of the parameters in linear mixed-eﬀects models can be determined using the lmer function in the lme4 package for R. As for most model-ﬁtting functions in R, the model is described in an lmer call by a formula, in this case including both ﬁxed- and random-eﬀects terms. The formula and data together determine a numerical representation of the model from which the proﬁled deviance or the proﬁled {REML} criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the proﬁled deviance or {REML} criterion, and the structure of classes or types that represents such a model. Suﬃcient detail is included to allow specialization of these structures by users who wish to write functions to ﬁt specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
	number = {1},
	journaltitle = {Journal of Statistical Software},
	shortjournal = {J. Stat. Soft.},
	author = {Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
	urldate = {2024-08-24},
	date = {2015},
	langid = {english},
	keywords = {mixed\_effects},
	file = {Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\ARNESTLT\\Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf:application/pdf},
}

@article{barrRandomEffectsStructure2013,
	title = {Random effects structure for confirmatory hypothesis testing: Keep it maximal},
	volume = {68},
	issn = {0749-596X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/},
	doi = {10.1016/j.jml.2012.11.001},
	shorttitle = {Random effects structure for confirmatory hypothesis testing},
	abstract = {Linear mixed-effects models ({LMEMs}) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using {LMEMs} for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that {LMEMs} generalize best when they include the maximal random effects structure justified by the design. The generalization performance of {LMEMs} including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only {LMEMs} used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal {LMEMs} should be the ‘gold standard’ for confirmatory hypothesis testing in psycholinguistics and beyond.},
	pages = {10.1016/j.jml.2012.11.001},
	number = {3},
	journaltitle = {Journal of memory and language},
	shortjournal = {J Mem Lang},
	author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
	urldate = {2024-09-06},
	date = {2013-04},
	pmid = {24403724},
	pmcid = {PMC3881361},
	keywords = {mixed\_effects, random\_effects},
	file = {PubMed Central Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\P8DFDQS4\\Barr et al. - 2013 - Random effects structure for confirmatory hypothes.pdf:application/pdf},
}

@online{ScriptEnciclopedia,
	title = {Script - Enciclopedia},
	url = {https://www.treccani.it/enciclopedia/script/},
	abstract = {Nell'Enciclopedia Treccani troverai tutto quello che devi sapere su script. Entra subito su Treccani.it, il portale del sapere.},
	titleaddon = {Treccani},
	urldate = {2024-09-08},
	langid = {italian},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\C5M38DVI\\script.html:text/html},
}

@book{gelmanRegressionOtherStories2020,
	title = {Regression and Other Stories},
	isbn = {978-1-139-16187-9},
	url = {https://www.cambridge.org/highereducation/books/regression-and-other-stories/DD20DD6C9057118581076E54E40C372C},
	abstract = {Most textbooks on regression focus on theory and the simplest of examples. Real statistical problems, however, are complex and subtle. This is not a book about the theory of regression. It is about using regression to solve real problems of comparison, estimation, prediction, and causal inference. Unlike other books, it focuses on practical issues such as sample size and missing data and a wide range of goals and techniques. It jumps right in to methods and computer code you can use immediately. Real examples, real stories from the authors' experience demonstrate what regression can do and its limitations, with practical advice for understanding assumptions and implementing methods for experiments and observational studies. They make a smooth transition to logistic regression and {GLM}. The emphasis is on computation in R and Stan rather than derivations, with code available online. Graphics and presentation aid understanding of the models and model fitting.},
	publisher = {Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer and Vehtari, Aki},
	urldate = {2024-09-08},
	date = {2020-07-23},
	langid = {english},
	doi = {10.1017/9781139161879},
	keywords = {regression},
	file = {Gelman et al. - 2020 - Regression and Other Stories.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\7B846BNS\\Gelman et al. - 2020 - Regression and Other Stories.pdf:application/pdf;Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\AQJ386D5\\DD20DD6C9057118581076E54E40C372C.html:text/html},
}

@article{gelmanBayesianMeasuresExplained2006,
	title = {Bayesian Measures of Explained Variance and Pooling in Multilevel (Hierarchical) Models},
	volume = {48},
	issn = {0040-1706, 1537-2723},
	url = {http://www.tandfonline.com/doi/abs/10.1198/004017005000000517},
	doi = {10.1198/004017005000000517},
	pages = {241--251},
	number = {2},
	journaltitle = {Technometrics},
	shortjournal = {Technometrics},
	author = {Gelman, Andrew and Pardoe, Iain},
	urldate = {2024-09-08},
	date = {2006-05},
	langid = {english},
	file = {Gelman e Pardoe - 2006 - Bayesian Measures of Explained Variance and Poolin.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\Y5HZYZVH\\Gelman e Pardoe - 2006 - Bayesian Measures of Explained Variance and Poolin.pdf:application/pdf},
}

@article{oziliAcceptableRsquareEmpirical2023,
	title = {The Acceptable R-square in Empirical Modelling for Social Science Research},
	abstract = {This article examines the acceptable R-square in social science empirical modelling with particular focus on why a low R-square model is acceptable in empirical social science research. The paper shows that a low R-square model is not necessarily bad. This is because the goal of most social science research modelling is not to predict human behaviour. Rather, the goal is often to assess whether specific predictors or explanatory variables have a significant effect on the dependent variable. Therefore, a low R-square of at least 0.1 (or 10 percent) is acceptable on the condition that some or most of the predictors or explanatory variables are statistically significant. If this condition is not met, the low R-square model cannot be accepted. A high R-square model is also acceptable provided that there is no spurious causation in the model and there is no multicollinearity among the explanatory variables.},
	author = {Ozili, Peterson K},
	date = {2023},
	langid = {english},
	file = {Ozili - 2023 - The Acceptable R-square in Empirical Modelling for.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\V6WWQUTN\\Ozili - 2023 - The Acceptable R-square in Empirical Modelling for.pdf:application/pdf},
}

@article{robertwallRegressionAnalysisAdjusted2020,
	title = {Regression Analysis and Adjusted R 2},
	volume = {114},
	issn = {0145-482X},
	url = {https://search.ebscohost.com/login.aspx?direct=true&db=cul&AN=145141606&site=ehost-live},
	doi = {10.1177/0145482X20939786},
	pages = {332--333},
	number = {4},
	journaltitle = {Journal of Visual Impairment \& Blindness},
	shortjournal = {J {VIS} {IMPAIRMENT} {BLINDNESS}},
	author = {Robert Wall, Emerson},
	urldate = {2024-09-08},
	date = {2020-08-07},
	note = {Place: Thousand Oaks, California
Publisher: Sage Publications Inc.},
	keywords = {Models, Statistical, Multiple Regression, Predictive Validity, Regression, Variable},
	file = {EBSCO Full Text:C\:\\Users\\fgfra\\Zotero\\storage\\H39L8X9W\\Robert Wall - 2020 - Regression Analysis and Adjusted R 2.pdf:application/pdf},
}

@article{jewsburyCattellHornCarroll2017a,
	title = {The Cattell–Horn–Carroll Model of Cognition for Clinical Assessment},
	volume = {35},
	issn = {0734-2829},
	url = {https://doi.org/10.1177/0734282916651360},
	doi = {10.1177/0734282916651360},
	abstract = {The Cattell–Horn–Carroll ({CHC}) model is a comprehensive model of the major dimensions of individual differences that underlie performance on cognitive tests. Studies evaluating the generality of the {CHC} model across test batteries, age, gender, and culture were reviewed and found to be overwhelmingly supportive. However, less research is available to evaluate the {CHC} model for clinical assessment. The {CHC} model was shown to provide good to excellent fit in nine high-quality data sets involving popular neuropsychological tests, across a range of clinically relevant populations. Executive function tests were found to be well represented by the {CHC} constructs, and a discrete executive function factor was found not to be necessary. The {CHC} model could not be simplified without significant loss of fit. The {CHC} model was supported as a paradigm for cognitive assessment, across both healthy and clinical populations and across both nonclinical and neuropsychological tests. The results have important implications for theoretical modeling of cognitive abilities, providing further evidence for the value of the {CHC} model as a basis for a common taxonomy across test batteries and across areas of assessment.},
	pages = {547--567},
	number = {6},
	journaltitle = {Journal of Psychoeducational Assessment},
	author = {Jewsbury, Paul A. and Bowden, Stephen C. and Duff, Kevin},
	urldate = {2024-09-17},
	date = {2017-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
}

@online{CattellHornCarroll,
	title = {The Cattell–Horn–Carroll Model of Cognition for Clinical Assessment - Paul A. Jewsbury, Stephen C. Bowden, Kevin Duff, 2017},
	url = {https://journals.sagepub.com/doi/10.1177/0734282916651360},
	urldate = {2024-09-17},
	file = {The Cattell–Horn–Carroll Model of Cognition for Clinical Assessment - Paul A. Jewsbury, Stephen C. Bowden, Kevin Duff, 2017:C\:\\Users\\fgfra\\Zotero\\storage\\89NHBZ5W\\0734282916651360.html:text/html},
}

@article{jewsburyCattellHornCarroll2017,
	title = {The Cattell–Horn–Carroll Model of Cognition for Clinical Assessment},
	volume = {35},
	issn = {0734-2829},
	url = {https://doi.org/10.1177/0734282916651360},
	doi = {10.1177/0734282916651360},
	abstract = {The Cattell?Horn?Carroll ({CHC}) model is a comprehensive model of the major dimensions of individual differences that underlie performance on cognitive tests. Studies evaluating the generality of the {CHC} model across test batteries, age, gender, and culture were reviewed and found to be overwhelmingly supportive. However, less research is available to evaluate the {CHC} model for clinical assessment. The {CHC} model was shown to provide good to excellent fit in nine high-quality data sets involving popular neuropsychological tests, across a range of clinically relevant populations. Executive function tests were found to be well represented by the {CHC} constructs, and a discrete executive function factor was found not to be necessary. The {CHC} model could not be simplified without significant loss of fit. The {CHC} model was supported as a paradigm for cognitive assessment, across both healthy and clinical populations and across both nonclinical and neuropsychological tests. The results have important implications for theoretical modeling of cognitive abilities, providing further evidence for the value of the {CHC} model as a basis for a common taxonomy across test batteries and across areas of assessment.},
	pages = {547--567},
	number = {6},
	journaltitle = {Journal of Psychoeducational Assessment},
	author = {Jewsbury, Paul A. and Bowden, Stephen C. and Duff, Kevin},
	urldate = {2024-09-17},
	date = {2017-09-01},
	note = {Publisher: {SAGE} Publications Inc},
}

@article{jewsburyCattellHornCarroll2017b,
	title = {The Cattell–Horn–Carroll Model of Cognition for Clinical Assessment},
	volume = {35},
	rights = {http://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {0734-2829, 1557-5144},
	url = {http://journals.sagepub.com/doi/10.1177/0734282916651360},
	doi = {10.1177/0734282916651360},
	abstract = {The Cattell–Horn–Carroll ({CHC}) model is a comprehensive model of the major dimensions of individual differences that underlie performance on cognitive tests. Studies evaluating the generality of the {CHC} model across test batteries, age, gender, and culture were reviewed and found to be overwhelmingly supportive. However, less research is available to evaluate the {CHC} model for clinical assessment. The {CHC} model was shown to provide good to excellent fit in nine high-quality data sets involving popular neuropsychological tests, across a range of clinically relevant populations. Executive function tests were found to be well represented by the {CHC} constructs, and a discrete executive function factor was found not to be necessary. The {CHC} model could not be simplified without significant loss of fit. The {CHC} model was supported as a paradigm for cognitive assessment, across both healthy and clinical populations and across both nonclinical and neuropsychological tests. The results have important implications for theoretical modeling of cognitive abilities, providing further evidence for the value of the {CHC} model as a basis for a common taxonomy across test batteries and across areas of assessment.},
	pages = {547--567},
	number = {6},
	journaltitle = {Journal of Psychoeducational Assessment},
	author = {Jewsbury, Paul A. and Bowden, Stephen C. and Duff, Kevin},
	urldate = {2024-09-17},
	date = {2017-09},
	langid = {english},
	note = {Publisher: {SAGE} Publications},
}

@article{jewsburyCattellHornCarroll2017c,
	title = {The Cattell–Horn–Carroll Model of Cognition for Clinical Assessment},
	volume = {35},
	issn = {0734-2829},
	url = {https://doi.org/10.1177/0734282916651360},
	doi = {10.1177/0734282916651360},
	abstract = {The Cattell–Horn–Carroll ({CHC}) model is a comprehensive model of the major dimensions of individual differences that underlie performance on cognitive tests. Studies evaluating the generality of the {CHC} model across test batteries, age, gender, and culture were reviewed and found to be overwhelmingly supportive. However, less research is available to evaluate the {CHC} model for clinical assessment. The {CHC} model was shown to provide good to excellent fit in nine high-quality data sets involving popular neuropsychological tests, across a range of clinically relevant populations. Executive function tests were found to be well represented by the {CHC} constructs, and a discrete executive function factor was found not to be necessary. The {CHC} model could not be simplified without significant loss of fit. The {CHC} model was supported as a paradigm for cognitive assessment, across both healthy and clinical populations and across both nonclinical and neuropsychological tests. The results have important implications for theoretical modeling of cognitive abilities, providing further evidence for the value of the {CHC} model as a basis for a common taxonomy across test batteries and across areas of assessment.},
	pages = {547--567},
	number = {6},
	journaltitle = {Journal of Psychoeducational Assessment},
	author = {Jewsbury, Paul A. and Bowden, Stephen C. and Duff, Kevin},
	urldate = {2024-09-17},
	date = {2017-09-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {Jewsbury et al. - 2017 - The Cattell–Horn–Carroll Model of Cognition for Cl.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\3A4UJUKP\\Jewsbury et al. - 2017 - The Cattell–Horn–Carroll Model of Cognition for Cl.pdf:application/pdf},
}

@incollection{schneiderCattellHornCarroll2018,
	location = {New York, {NY}, {US}},
	title = {The Cattell–Horn–Carroll theory of cognitive abilities},
	isbn = {978-1-4625-3578-1},
	abstract = {The Cattell–Horn–Carroll ({CHC}) theory of cognitive abilities is a comprehensive taxonomy of abilities embedded in multiple overlapping theories of cognition. It provides a common framework and nomenclature for intelligence researchers to communicate their findings. The basic idea of {CHC} theory is that intelligence is both multidimensional and functionally integrated. In {CHC} theory, the dimensions of ability have a hierarchical structure, meaning that some have a broader scope than others. At the bottom of the hierarchy are specific abilities, which are tied to a specific task or test. Specific abilities are the only abilities that can be measured directly. Narrow abilities are clusters of highly correlated specific abilities. Broad abilities are clusters of narrow abilities that are mutually more correlated with each other than with abilities in other broad-ability clusters. The chapter defines each of the constructs in {CHC} theory in terms that clinicians will find useful. It provides some guidance as to which constructs are more central to the theory or have more validity data available. The chapter proposes a number of additions, deletions, and rearrangements in the list of {CHC} theory abilities. It presents a model of cognition that incorporates the {CHC} broad abilities as parameters of information processing. ({PsycInfo} Database Record (c) 2024 {APA}, all rights reserved)},
	pages = {73--163},
	booktitle = {Contemporary intellectual assessment: Theories, tests, and issues, 4th ed},
	publisher = {The Guilford Press},
	author = {Schneider, W. Joel and {McGrew}, Kevin S.},
	date = {2018},
	keywords = {Cognitive Ability, Experimenters, Information Processing Model, Intelligence, Psychological Theories, Taxonomies},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\YT7LBJWK\\2018-36604-003.html:text/html},
}

@collection{goldsteinEncyclopediaChildBehavior2011,
	location = {Boston, {MA}},
	title = {Encyclopedia of Child Behavior and Development},
	rights = {http://www.springer.com/tdm},
	isbn = {978-0-387-77579-1 978-0-387-79061-9},
	url = {http://link.springer.com/10.1007/978-0-387-79061-9},
	publisher = {Springer {US}},
	editor = {Goldstein, Sam and Naglieri, Jack A.},
	urldate = {2024-09-17},
	date = {2011},
	langid = {english},
	doi = {10.1007/978-0-387-79061-9},
	file = {Goldstein e Naglieri - 2011 - Encyclopedia of Child Behavior and Development.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\C4JXCVXL\\Goldstein e Naglieri - 2011 - Encyclopedia of Child Behavior and Development.pdf:application/pdf},
}

@article{jewsburyCattellHornCarroll2017d,
	title = {The Cattell–Horn–Carroll Model of Cognition for Clinical Assessment},
	volume = {35},
	rights = {http://journals.sagepub.com/page/policies/text-and-data-mining-license},
	issn = {0734-2829, 1557-5144},
	url = {http://journals.sagepub.com/doi/10.1177/0734282916651360},
	doi = {10.1177/0734282916651360},
	abstract = {The Cattell–Horn–Carroll ({CHC}) model is a comprehensive model of the major dimensions of individual differences that underlie performance on cognitive tests. Studies evaluating the generality of the {CHC} model across test batteries, age, gender, and culture were reviewed and found to be overwhelmingly supportive. However, less research is available to evaluate the {CHC} model for clinical assessment. The {CHC} model was shown to provide good to excellent fit in nine high-quality data sets involving popular neuropsychological tests, across a range of clinically relevant populations. Executive function tests were found to be well represented by the {CHC} constructs, and a discrete executive function factor was found not to be necessary. The {CHC} model could not be simplified without significant loss of fit. The {CHC} model was supported as a paradigm for cognitive assessment, across both healthy and clinical populations and across both nonclinical and neuropsychological tests. The results have important implications for theoretical modeling of cognitive abilities, providing further evidence for the value of the {CHC} model as a basis for a common taxonomy across test batteries and across areas of assessment.},
	pages = {547--567},
	number = {6},
	journaltitle = {Journal of Psychoeducational Assessment},
	author = {Jewsbury, Paul A. and Bowden, Stephen C. and Duff, Kevin},
	urldate = {2024-09-17},
	date = {2017-09},
	langid = {english},
	note = {Publisher: {SAGE} Publications},
	file = {Jewsbury et al. - 2017 - The Cattell–Horn–Carroll Model of Cognition for Cl.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\VSWV9UFL\\Jewsbury et al. - 2017 - The Cattell–Horn–Carroll Model of Cognition for Cl.pdf:application/pdf},
}

@online{ProvaOraleDellesame,
	title = {La prova orale dell'esame di Stato di psicologia: Legislazione e deontologia professionale (Italian Edition) - Damato, Titti: 9781709745263 - {AbeBooks}},
	url = {https://www.abebooks.com/9781709745263/prova-orale-dellesame-Stato-psicologia-1709745266/plp},
	shorttitle = {La prova orale dell'esame di Stato di psicologia},
	abstract = {La prova orale dell'esame di Stato di psicologia: Legislazione e deontologia professionale (Italian Edition) by Damato, Titti - {ISBN} 10: 1709745266 - {ISBN} 13: 9781709745263 - Independently published - 2020 - Softcover},
	urldate = {2024-09-21},
	langid = {english},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\YV4RS43G\\plp.html:text/html},
}

@book{MACCHICASSIAVALENZA2012,
	title = {V. {MACCHI} {CASSIA}, E. {VALENZA}, F. {SIMION}, Lo sviluppo della mente umana. Dalle teorie classiche ai nuovi orientamenti},
	url = {https://www.mulino.it/isbn/9788815239013?forcedLocale=it&fbrefresh=CAN_BE_ANYTHING},
	abstract = {Il volume offre un panorama aggiornato dei modelli teorici che la psicologia ha elaborato per spiegare le cause e i meccanismi che determinano lo sviluppo della mente umana. Dopo aver affrontato nella prima parte le teorie classiche, le autrici passano in rassegna, nella seconda parte, i modelli più recenti relativi allo sviluppo cognitivo, sottolineandone gli aspetti interdisciplinari. È così illustrato con chiarezza il fondamentale ruolo che questo filone di ricerca svolge nell’indagine sull’architettura e sul funzionamento della mente umana.Indice del volume: Prefazione. - I. Le domande centrali delle teorie dello sviluppo cognitivo. - Parte prima: Le teorie classiche. - {II}. Il comportamentismo. - {III}. Il costruttivismo di Piaget. - {IV}. La teoria dell’elaborazione dell’informazione e il cognitivismo. - V. Lo studio delle competenze percettive e cognitive nella prima infanzia. - {VI}. L’approccio innatista-modulare. - Parte seconda: I nuovi orientamenti. - {VII}. Il contributo delle neuroscienze cognitive dello sviluppo. - {VIII}. L’approccio connessionista. - {IX}. Il neurocostruttivismo. - Riferimenti bibliografici. - Indice analitico.Viola Macchi Cassia insegna Sociologia della famiglia nell’Università di Torino. Eloisa Valenza è professore associato e insegna Psicologia dello sviluppo e Psicologia dello sviluppo cognitivo presso l’Università degli studi di Padova. Francesca Simion è professore ordinario e insegna Psicologia dello sviluppo cognitivo e Neuroscienze cognitive dello sviluppo presso l’Università degli studi di Padova. Per il Mulino ha curato insieme a L. Camaioni «Metodi di ricerca in psicologia dello sviluppo» (1990).Le autrici hanno pubblicato per il Mulino il volume «Lo sviluppo cognitivo. Dalle teorie classiche ai nuovi orientamenti» (2004).},
	urldate = {2024-09-23},
	date = {2012-07-19},
	langid = {italian},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\JLHZJDX2\\9788815239013.html:text/html},
}

@online{PSICOLOGIASVILUPPOED,
	title = {{PSICOLOGIA} {DELLO} {SVILUPPO} 4/{ED} {CON} {CONNECT}},
	url = {https://www.mheducation.it/psicologia-dello-sviluppo-4-ed-con-connect-9788838697074-italy},
	abstract = {Famoso in tutto il mondo per l’approccio cronologico alla disciplina, il testo di Santrock et al., curato dalla Professoressa Rollo dell’Università degli Studi di Parma, è arrivato con grande successo alla quarta edizione italiana. L’opera, fin dalla prima edizione, ha subito una profonda revisione per adattare i contenuti originali alle realtà specifiche del nostro Paese e per integrare le ricerche di base svolte dalla psicologia dello sviluppo con le loro implicazioni operativo-applicative nell’educazione.Il manuale, organizzato per domini psicologici e all’interno di essi per periodo di sviluppo, si snoda attraverso cinque parti che ripercorrono gli argomenti fondamentali dell’insegnamento della materia. Gli autori affrontano i principali temi dello sviluppo delineandone gli aspetti metodologici e teorici, classici e contemporanei, attraverso uno stile semplice e originale nel metodo di approccio, nel livello di difficoltà, nei contenuti e negli ausili didattici impiegati.In questa nuova edizione si è data enfasi al concetto di inclusione che ha implicato prima di tutto una profonda revisione terminologica. Inoltre, si è ulteriormente approfondito il contributo di recenti filoni di ricerca neuropsicologica, con particolare riferimento ai principi teorici delle neuroscienze cognitive e sociali dello sviluppo e alle importanti implicazioni applicative, come i disturbi dello spettro autistico. Infine, al termine di ciascun capitolo è stata introdotta una mappa concettuale per facilitarne la sintesi al lettore ed è stata operata una profonda revisione e un puntuale aggiornamento delle voci bibliografiche.},
	urldate = {2024-09-23},
	langid = {italian},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\IQPPNNCC\\psicologia-dello-sviluppo-4-ed-con-connect-9788838697074-italy.html:text/html},
}

@book{camanniSeNonDovessi2023,
	title = {Se non dovessi tornare},
	publisher = {Mondadori},
	author = {Camanni, Enrico},
	date = {2023-02-06},
	keywords = {romanzo},
}

@online{marcuseOneDimensionalManStudies1964,
	title = {One-Dimensional Man: Studies in the Ideology of Advanced Industrial Society},
	author = {Marcuse, Herbert},
	date = {1964},
	keywords = {saggi},
	file = {One-Dimensional Man\: Studies in the Ideology of Advanced Industrial Society \: Marcuse, Herbert\: Amazon.it\: Libri:C\:\\Users\\fgfra\\Zotero\\storage\\9IBCCWG7\\0807014176.html:text/html},
}

@book{golemanFocusHiddenDriver2015,
	location = {New York},
	edition = {Reprint edizione},
	title = {Focus: The Hidden Driver of Excellence},
	isbn = {978-0-06-211496-9},
	shorttitle = {Focus},
	abstract = {New York Times Bestseller“A powerful guide for taking control of our attention.” —Tony Schwartz, author of The Power of Full Engagement and {CEO} of The Energy {ProjectFrom} the author of the \#1 international bestseller Emotional Intelligence, comes a groundbreaking look at today’s scarcest resource and the secret to high performance and fulfillment: {attentionFor} more than two decades, psychologist and journalist Daniel Goleman has been scouting the leading edge of the human sciences for what’s new, surprising, and important. In Focus, he delves into the science of attention in all its varieties, presenting a long-overdue discussion of this little-noticed and underrated mental asset that matters enormously for how we navigate life. Attention works much like a muscle: use it poorly and it can wither; work it well and it grows. In an era of unstoppable distractions, Goleman persuasively argues that now more than ever we must learn to sharpen focus if we are to contend with, let alone thrive in, a complex world. Goleman analyzes attention research as a threesome: inner, other, and outer focus. Goleman shows why high-performers need all three kinds of focus, as demonstrated by rich case studies from fields as diverse as competitive sports, education, the arts, and business. Those who excel rely on what Goleman calls smart practice that helps them improve habits, add new skills, and sustain excellence. Combining cutting-edge research with practical findings, Focus reveals what distinguishes experts from amateurs and stars from average performers.},
	pagetotal = {311},
	publisher = {{HarperCollins}},
	author = {Goleman, Daniel},
	date = {2015-05-05},
	keywords = {saggio, psicologia},
}

@book{pastoreAnalisiDatiPsicologia2015a,
	location = {Bologna},
	title = {Analisi dei dati in psicologia: con applicazioni in R},
	isbn = {978-88-15-25898-4},
	series = {Manuali},
	shorttitle = {Analisi dei dati in psicologia},
	publisher = {il Mulino},
	author = {Pastore, Massimiliano},
	date = {2015},
}

@book{cohenStatisticalPowerAnalysis1988,
	location = {New York},
	edition = {2},
	title = {Statistical Power Analysis for the Behavioral Sciences},
	isbn = {978-0-203-77158-7},
	abstract = {Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes:  * a chapter covering power analysis in set correlation and multivariate methods; * a chapter considering effect size, psychometric reliability, and the efficacy of "qualifying" dependent variables and; * expanded power and sample size tables for multiple regression/correlation.},
	pagetotal = {567},
	publisher = {Routledge},
	author = {Cohen, Jacob},
	date = {1988-07-01},
	doi = {10.4324/9780203771587},
}

@book{ziliakCultStatisticalSignificance2008,
	title = {The Cult of Statistical Significance: How the Standard Error Costs Us Jobs, Justice, and Lives},
	isbn = {978-0-472-07007-7 978-0-472-05007-9 978-0-472-02610-4},
	url = {https://www.fulcrum.org/concern/monographs/cr56n193q},
	shorttitle = {The Cult of Statistical Significance},
	abstract = {{\textless}{DIV}{\textgreater}{\textless}P style="{MARGIN}: 0in 0in 0pt; {LINE}-{HEIGHT}: 150\%"{\textgreater}"{McCloskey} and Ziliak have been pushing this very elementary, very correct, very important argument through several articles over several years and for reasons I cannot fathom it is still resisted. If it takes a book to get it across, I hope this book will do it. It ought to."{\textless}/P{\textgreater}{\textless}P style="{MARGIN}: 0in 0in 0pt; {LINE}-{HEIGHT}: 150\%"{\textgreater}—Thomas Schelling, Distinguished University Professor, School of Public Policy, University of Maryland, and 2005 Nobel Prize Laureate in Economics{\textless}/P{\textgreater}{\textless}P style="{MARGIN}: 0in 0in 0pt; {LINE}-{HEIGHT}: 150\%"{\textgreater} {\textless}/P{\textgreater}{\textless}P style="{MARGIN}: 0in 0in 0pt; {LINE}-{HEIGHT}: 150\%"{\textgreater}"With humor, insight, piercing logic and a nod to history, Ziliak and {McCloskey} show how economists—and other scientists—suffer from a mass delusion about statistical analysis. The quest for statistical significance that pervades science today is a deeply flawed substitute for thoughtful analysis. . . . Yet few participants in the scientific bureaucracy have been willing to admit what Ziliak and {McCloskey} make clear: the emperor has no clothes."{\textless}/P{\textgreater}{\textless}P style="{MARGIN}: 0in 0in 0pt; {LINE}-{HEIGHT}: 150\%"{\textgreater}—Kenneth Rothman, Professor of Epidemiology, Boston University School of Health{\textless}/P{\textgreater}{\textless}P style="{MARGIN}: 0in 0in 0pt; {LINE}-{HEIGHT}: 150\%; mso-layout-grid-align: none"{\textgreater} {\textless}/P{\textgreater}{\textless}P style="{MARGIN}: 0in 0in 0pt; {LINE}-{HEIGHT}: 150\%"{\textgreater}{\textless}I{\textgreater}The Cult of Statistical Significance{\textless}/I{\textgreater} shows, field by field, how "statistical significance," a technique that dominates many sciences, has been a huge mistake. The authors find that researchers in a broad spectrum of fields, from agronomy to zoology, employ "testing" that doesn't test and "estimating" that doesn't estimate. The facts will startle the outside reader: how could a group of brilliant scientists wander so far from scientific magnitudes? This study will encourage scientists who want to know how to get the statistical sciences back on track and fulfill their quantitative promise. The book shows for the first time how wide the disaster is, and how bad for science, and it traces the problem to its historical, sociological, and philosophical roots.{\textless}/P{\textgreater}{\textless}P style="{MARGIN}: 0in 0in 0pt; {LINE}-{HEIGHT}: 200\%"{\textgreater} {\textless}/P{\textgreater}{\textless}P style="{MARGIN}: 0in 0in 0pt; {LINE}-{HEIGHT}: 150\%; mso-layout-grid-align: none"{\textgreater}Stephen T. Ziliak is the author or editor of many articles and two books. He currently lives in Chicago, where he is Professor of Economics at Roosevelt University. Deirdre N. {McCloskey}, Distinguished Professor of Economics, History, English, and Communication at the University of Illinois at Chicago, is the author of twenty books and three hundred scholarly articles. She has held Guggenheim and National Humanities Fellowships. She is best known for {\textless}I{\textgreater}How to Be Human* Though an Economist {\textless}/I{\textgreater}(University of Michigan Press, 2000) and her most recent book, {\textless}I{\textgreater}The Bourgeois Virtues: Ethics for an Age of Commerce {\textless}/I{\textgreater}(2006).{\textless}/P{\textgreater}{\textless}/{DIV}{\textgreater}},
	publisher = {University of Michigan Press},
	author = {Ziliak, Steve and {McCloskey}, Deirdre Nansen},
	urldate = {2024-10-07},
	date = {2008},
	langid = {english},
	doi = {10.3998/mpub.186351},
}

@article{francisTooGoodBe2012,
	title = {Too good to be true: Publication bias in two prominent studies from experimental psychology},
	volume = {19},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/s13423-012-0227-9},
	doi = {10.3758/s13423-012-0227-9},
	shorttitle = {Too good to be true},
	abstract = {Empirical replication has long been considered the final arbiter of phenomena in science, but replication is undermined when there is evidence for publication bias. Evidence for publication bias in a set of experiments can be found when the observed number of rejections of the null hypothesis exceeds the expected number of rejections. Application of this test reveals evidence of publication bias in two prominent investigations from experimental psychology that have purported to reveal evidence of extrasensory perception and to indicate severe limitations of the scientific method. The presence of publication bias suggests that those investigations cannot be taken as proper scientific studies of such phenomena, because critical data are not available to the field. Publication bias could partly be avoided if experimental psychologists started using Bayesian data analysis techniques.},
	pages = {151--156},
	number = {2},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychon Bull Rev},
	author = {Francis, Gregory},
	urldate = {2024-10-07},
	date = {2012-04-01},
	langid = {english},
	keywords = {Repeated testing, Statistical inference},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\F87WVQDQ\\Francis - 2012 - Too good to be true Publication bias in two prominent studies from experimental psychology.pdf:application/pdf},
}

@article{bakkerRulesGameCalled2012,
	title = {The Rules of the Game Called Psychological Science},
	volume = {7},
	issn = {1745-6916},
	url = {https://doi.org/10.1177/1745691612459060},
	doi = {10.1177/1745691612459060},
	abstract = {If science were a game, a dominant rule would probably be to collect results that are statistically significant. Several reviews of the psychological literature have shown that around 96\% of papers involving the use of null hypothesis significance testing report significant outcomes for their main results but that the typical studies are insufficiently powerful for such a track record. We explain this paradox by showing that the use of several small underpowered samples often represents a more efficient research strategy (in terms of finding p {\textless} .05) than does the use of one larger (more powerful) sample. Publication bias and the most efficient strategy lead to inflated effects and high rates of false positives, especially when researchers also resorted to questionable research practices, such as adding participants after intermediate testing. We provide simulations that highlight the severity of such biases in meta-analyses. We consider 13 meta-analyses covering 281 primary studies in various fields of psychology and find indications of biases and/or an excess of significant results in seven. These results highlight the need for sufficiently powerful replications and changes in journal policies.},
	pages = {543--554},
	number = {6},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Bakker, Marjan and van Dijk, Annette and Wicherts, Jelte M.},
	urldate = {2024-10-07},
	date = {2012-11-01},
	langid = {english},
	note = {Publisher: {SAGE} Publications Inc},
	file = {SAGE PDF Full Text:C\:\\Users\\fgfra\\Zotero\\storage\\MYBXZGLR\\Bakker et al. - 2012 - The Rules of the Game Called Psychological Science.pdf:application/pdf},
}

@book{fisherDesignExperiments1935,
	location = {Oxford, England},
	title = {The design of experiments},
	series = {The design of experiments},
	abstract = {Different types of experimentation are considered with reference to their logical structure, to show that valid conclusions may be drawn from them without using the disputed theory of inductive inferences, i.e., of arguing from observation to explanatory theory. This is possible if a null hypothesis is explicitly formulated when the experiment is designed; this hypothesis can never be proved, but may be disproved with whatever probability one will accept as demonstrating a positive result. Chapters {II}, {III}, and {IV} illustrate simple applications of the principles involved in sensitiveness, significance, tests of wider hypotheses, validity, and estimation and elimination of error. More elaborate structures are treated in later chapters. Chapter titles are: (V) the Latin square; ({VI}) factorial design in experimentation; ({VII}) confounding; ({VIII}) special cases of partial confounding; ({IX}) increase of precision by concomitant measurements: statistical control; (X) generalization of null hypotheses: fiducial probability; ({XI}) measurement of amount of information in general. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pagetotal = {xi, 251},
	publisher = {Oliver \& Boyd},
	author = {Fisher, Ronald A.},
	date = {1935},
	note = {Pages: xi, 251},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\5RLPE88E\\1939-04964-000.html:text/html;The Design of Experiments.pdf:C\:\\Users\\fgfra\\Zotero\\storage\\ZDDTNAJI\\Fisher - 1935 - The design of experiments.pdf:application/pdf},
}

@article{neymanIXProblemMost1933,
	title = {{IX}. On the problem of the most efficient tests of statistical hypotheses},
	volume = {231},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.1933.0009},
	doi = {10.1098/rsta.1933.0009},
	abstract = {The problem of testing statistical hypotheses is an old one. Its origin is usually connected with the name of Thomas Bayes, who gave the well-known theorem on the probabilities a posteriori of the possible “causes" of a given event. Since then it has been discussed by many writers of whom we shall here mention two only, Bertrand and Borel, whose differing views serve well to illustrate the point from which we shall approach the subject. Bertrand put into statistical form a variety of hypotheses, as for example the hypothesis that a given group of stars with relatively small angular distances between them as seen from the earth, form a “system” or group in space. His method of attack, which is that in common use, consisted essentially in calculating the probability, P, that a certain character, x, of the observed facts would arise if the hypothesis tested were true. If P were very small, this would generally be considered as an indication that the hypothesis, H, was probably false, and vice versa. Bertrand expressed the pessimistic view that no test of this kind could give reliable results. Borel, however, in a later discussion, considered that the method described could be applied with success provided that the character, x, of the observed facts were properly chosen—were, in fact, a character which he terms “en quelque sorte remarquable.”},
	pages = {289--337},
	number = {694},
	journaltitle = {Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character},
	author = {Neyman, Jerzy and Pearson, Egon Sharpe},
	urldate = {2024-10-08},
	date = {1933-02-16},
	note = {Publisher: Royal Society},
	keywords = {data\_analysis},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\KYTX33BV\\Neyman et al. - 1997 - IX. On the problem of the most efficient tests of statistical hypotheses.pdf:application/pdf},
}

@book{bachmannVerificaSignificativitaDellipotesi2005,
	title = {La verifica della significatività dell'ipotesi nulla in psicologia},
	isbn = {978-88-8453-227-5},
	abstract = {Il volume affronta il problema della verifica dell'ipotesi nulla, nel corso degli ultimi anni animatamente dibattuto nel mondo dell'analisi dei dati della ricerca psicologica. Negli anni si è consolidato un paradigma di interpretazione inferenziale che è purtroppo frutto di un ibrido tra due approcci parzialmente incompatibili, che fanno capo rispettivamente da un lato a R. A. Fisher, dall'altro a J. Neyman e E. Pearson. Il volume esamina il costituirsi storico di questo paradigma, gli inconvenienti a cui seguita a dar luogo e indica le principali vie per superare tali inconvenienti. Prende infine in esame il problema dell'insegnamento della statistica ai futuri psicologi.},
	pagetotal = {208},
	publisher = {Firenze University Press},
	author = {Bachmann, Christina and Luccio, Riccardo and Salvadori, Emilia},
	date = {2005},
	langid = {italian},
	note = {Google-Books-{ID}: {udTdkQEACAAJ}},
	keywords = {Education / Educational Psychology},
}

@article{wagenmakersPracticalSolutionPervasive2007,
	title = {A practical solution to the pervasive problems of p-values},
	volume = {14},
	issn = {1531-5320},
	url = {https://doi.org/10.3758/BF03194105},
	doi = {10.3758/BF03194105},
	abstract = {In the field of psychology, the practice ofp value null-hypothesis testing is as widespread as ever. Despite this popularity, or perhaps because of it, most psychologists are not aware of the statistical peculiarities of thep value procedure. In particular,p values are based on data that were never observed, and these hypothetical data are themselves influenced by subjective intentions. Moreover,p values do not quantify statistical evidence. This article reviews thesep value problems and illustrates each problem with concrete examples. The three problems are familiar to statisticians but may be new to psychologists. A practical solution to thesep value problems is to adopt a model selection perspective and use the Bayesian information criterion ({BIC}) for statistical inference (Raftery, 1995). The {BIC} provides an approximation to a Bayesian hypothesis test, does not require the specification of priors, and can be easily calculated from {SPSS} output.},
	pages = {779--804},
	number = {5},
	journaltitle = {Psychonomic Bulletin \& Review},
	shortjournal = {Psychonomic Bulletin \& Review},
	author = {Wagenmakers, Eric Jan},
	urldate = {2024-10-10},
	date = {2007-10-01},
	langid = {english},
	keywords = {Bayesian Information Criterion, Null Hypothesis, Posterior Probability, Prior Distribution, Statistical Inference},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\I5LJWZMH\\Wagenmakers - 2007 - A practical solution to the pervasive problems ofp values.pdf:application/pdf},
}

@article{johnsonRevisedStandardsStatistical2013,
	title = {Revised standards for statistical evidence},
	volume = {110},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1313476110},
	doi = {10.1073/pnas.1313476110},
	abstract = {Recent advances in Bayesian hypothesis testing have led to the development of uniformly most powerful Bayesian tests, which represent an objective, default class of Bayesian hypothesis tests that have the same rejection regions as classical significance tests. Based on the correspondence between these two classes of tests, it is possible to equate the size of classical hypothesis tests with evidence thresholds in Bayesian tests, and to equate P values with Bayes factors. An examination of these connections suggest that recent concerns over the lack of reproducibility of scientific studies can be attributed largely to the conduct of significance tests at unjustifiably high levels of significance. To correct this problem, evidence thresholds required for the declaration of a significant finding should be increased to 25–50:1, and to 100–200:1 for the declaration of a highly significant finding. In terms of classical hypothesis tests, these evidence standards mandate the conduct of tests at the 0.005 or 0.001 level of significance.},
	pages = {19313--19317},
	number = {48},
	journaltitle = {Proceedings of the National Academy of Sciences},
	author = {Johnson, Valen E.},
	urldate = {2024-10-10},
	date = {2013-11-26},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\23XCDQV7\\Johnson - 2013 - Revised standards for statistical evidence.pdf:application/pdf},
}

@article{tverskyBeliefLawSmall1971,
	title = {Belief in the law of small numbers},
	volume = {76},
	issn = {1939-1455},
	doi = {10.1037/h0031322},
	abstract = {Reports that people have erroneous intuitions about the laws of chance. In particular, they regard a sample randomly drawn from a population as highly representative, I.e., similar to the population in all essential characteristics. The prevalence of the belief and its unfortunate consequences for psychological research are illustrated by the responses of 84 professional psychologists to a questionnaire concerning research decisions. ({PsycINFO} Database Record (c) 2016 {APA}, all rights reserved)},
	pages = {105--110},
	number = {2},
	journaltitle = {Psychological Bulletin},
	author = {Tversky, Amos and Kahneman, Daniel},
	date = {1971},
	note = {Place: {US}
Publisher: American Psychological Association},
	keywords = {Statistical Analysis, Methodology, Consequence},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\YURCUXEZ\\doiLanding.html:text/html},
}

@book{oakesStatisticalInferenceCommentary1986,
	location = {Chichester u.a},
	edition = {1st edition},
	title = {Statistical Inference: A Commentary for the Social and Behavioural Sciences},
	isbn = {978-0-471-10443-8},
	shorttitle = {Statistical Inference},
	abstract = {Critical analysis of the use of statistical inference in social and behavioural research, its proper role, logic, and its abuse. Argues that ignorance and misunderstanding of the role of statistical inference has had a detrimental effect upon research in the field. Points the way to an appropriate appreciation of the part played by quantification in these disciplines. Examines the significance test versus interval estimation and evaluates Neyman-Pearson, Fisherian, Bayesian, and Likelihood inference. Presents arguments in a plainly written, easily understood manner.},
	pagetotal = {196},
	publisher = {Wiley \& Sons},
	author = {Oakes, Michael},
	date = {1986-05-06},
}

@article{kahnemanBELIEFLAWSMALL,
	title = {{BELIEF} {IN} {THE} {LAW} {OF} {SMALL} {NUMBERS}},
	author = {Kahneman, Amos Tversky Anddaniel},
	langid = {english},
	file = {PDF:C\:\\Users\\fgfra\\Zotero\\storage\\6BZDLD55\\Kahneman - BELIEF IN THE LAW OF SMALL NUMBERS.pdf:application/pdf},
}

@book{dinuovoMetaAnalisi1995,
	title = {La Meta-Analisi},
	publisher = {Borla},
	author = {Di Nuovo, Santo},
	urldate = {2024-10-12},
	date = {1995},
	langid = {italian},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\M99P3WCS\\9788826311159.html:text/html},
}

@article{kaiserDirectionalStatisticalDecisions1960a,
	title = {Directional statistical decisions.},
	volume = {67},
	issn = {1939-1471, 0033-295X},
	url = {https://doi.apa.org/doi/10.1037/h0047595},
	doi = {10.1037/h0047595},
	pages = {160--167},
	number = {3},
	journaltitle = {Psychological Review},
	shortjournal = {Psychological Review},
	author = {Kaiser, Henry F.},
	urldate = {2024-10-12},
	date = {1960-05},
	langid = {english},
	file = {PDF:C\:\\Users\\fgfra\\Zotero\\storage\\I55QAJCG\\Kaiser - 1960 - Directional statistical decisions..pdf:application/pdf},
}

@article{rafteryBayesianModelSelection1995,
	title = {Bayesian Model Selection in Social Research},
	volume = {25},
	issn = {0081-1750},
	url = {https://www.jstor.org/stable/271063},
	doi = {10.2307/271063},
	abstract = {It is argued that P-values and the tests based upon them give unsatisfactory results, especially in large samples. It is shown that, in regression, when there are many candidate independent variables, standard variable selection procedures can give very misleading results. Also, by selecting a single model, they ignore model uncertainty and so underestimate the uncertainty about quantities of interest. The Bayesian approach to hypothesis testing, model selection, and accounting for model uncertainty is presented. Implementing this is straightforward through the use of the simple and accurate {BIC} approximation, and it can be done using the output from standard software. Specific results are presented for most of the types of model commonly used in sociology. It is shown that this approach overcomes the difficulties with P-values and standard model selection procedures based on them. It also allows easy comparison of nonnested models, and permits the quantification of the evidence for a null hypothesis of interest, such as a convergence theory or a hypothesis about societal norms.},
	pages = {111--163},
	journaltitle = {Sociological Methodology},
	author = {Raftery, Adrian E.},
	urldate = {2024-10-12},
	date = {1995},
	note = {Publisher: [American Sociological Association, Wiley, Sage Publications, Inc.]},
	file = {JSTOR Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\79QFAHGD\\Raftery - 1995 - Bayesian Model Selection in Social Research.pdf:application/pdf},
}

@book{pinheiroMixedEffectsModelsSPLUS2000,
	location = {New York},
	title = {Mixed-Effects Models in S and S-{PLUS}},
	rights = {http://www.springer.com/tdm},
	isbn = {978-0-387-98957-0},
	url = {http://link.springer.com/10.1007/b98882},
	series = {Statistics and Computing},
	publisher = {Springer-Verlag},
	author = {Pinheiro, José C. and Bates, Douglas M.},
	urldate = {2024-10-21},
	date = {2000},
	langid = {english},
	doi = {10.1007/b98882},
	keywords = {Regression analysis, Analysis, best fit, C programming language, Fitting, linear regression, modeling, Phar, {STATISTICA}, Statistical Computing, Turing},
	file = {Submitted Version:C\:\\Users\\fgfra\\Zotero\\storage\\3DLNZGXF\\2000 - Mixed-Effects Models in S and S-PLUS.pdf:application/pdf},
}

@incollection{pinheiroLinearMixedEffectsModels2000,
	location = {New York, {NY}},
	title = {Linear Mixed-Effects Models: Basic Concepts and Examples},
	isbn = {978-0-387-22747-4},
	url = {https://doi.org/10.1007/0-387-22747-4_1},
	shorttitle = {Linear Mixed-Effects Models},
	pages = {3--56},
	booktitle = {Mixed-Effects Models in S and S-{PLUS}},
	publisher = {Springer},
	editor = {Pinheiro, José C. and Bates, Douglas M.},
	urldate = {2024-10-21},
	date = {2000},
	langid = {english},
	doi = {10.1007/0-387-22747-4_1},
}

@book{lehmannTestingStatisticalHypotheses2022,
	location = {Cham},
	title = {Testing Statistical Hypotheses},
	rights = {https://www.springer.com/tdm},
	isbn = {978-3-030-70577-0 978-3-030-70578-7},
	url = {https://link.springer.com/10.1007/978-3-030-70578-7},
	series = {Springer Texts in Statistics},
	publisher = {Springer International Publishing},
	author = {Lehmann, E.L. and Romano, Joseph P.},
	urldate = {2024-10-24},
	date = {2022},
	langid = {english},
	doi = {10.1007/978-3-030-70578-7},
	keywords = {best fit, Excel, Resampling, Statistical Hypotheses, Statistical Theory},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\5BAWABZN\\Lehmann and Romano - 2022 - Testing Statistical Hypotheses.pdf:application/pdf},
}

@book{garthwaiteStatisticalInference2002,
	title = {Statistical Inference},
	isbn = {978-0-19-857226-8},
	abstract = {Adopting a broad view of statistical inference, the text concentrates on what various techniques do, with mathematical proof kept to a minimum. The approach is rigorous but accessible to final year undergraduates. Classical approaches to point estimation, hypothesis testing and interval estimation are all covered thoroughly with recent developments outlined. Separate chapters are devoted to Bayesian inference, to decision theory and to non-parametric and robust inference. The increasingly important topics of computationally intensive methods and generalized linear models are also included. In this edition, the material on recent developments has been updated, and additional exercises are included in most chapters.},
	pagetotal = {346},
	publisher = {Oxford University Press},
	author = {Garthwaite, Paul H. and Jolliffe, I. T. and Jones, Byron},
	date = {2002},
	langid = {english},
	keywords = {Mathematics / Probability \& Statistics / General},
}

@online{gelmanDataAnalysisUsing2006,
	title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
	url = {https://www.cambridge.org/highereducation/books/data-analysis-using-regression-and-multilevel-hierarchical-models/32A29531C7FD730C3A68951A17C9D983},
	abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models, first published in 2007, is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.},
	titleaddon = {Higher Education from Cambridge University Press},
	author = {Gelman, Andrew and Hill, Jennifer},
	urldate = {2024-10-24},
	date = {2006-12-18},
	langid = {english},
	doi = {10.1017/CBO9780511790942},
	note = {{ISBN}: 9780511790942
Publisher: Cambridge University Press},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\YFITA76Q\\32A29531C7FD730C3A68951A17C9D983.html:text/html},
}

@article{batesLme4MixedeffectsModeling2010,
	title = {lme4: Mixed-eﬀects modeling with R},
	author = {Bates, Douglas M},
	date = {2010},
	langid = {english},
	file = {PDF:C\:\\Users\\fgfra\\Zotero\\storage\\3ZWGSGPQ\\Bates - lme4 Mixed-eﬀects modeling with R.pdf:application/pdf},
}

@article{crainiceanuLikelihoodRatioTests2004,
	title = {Likelihood ratio tests in linear mixed models with one variance component},
	volume = {66},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2004.00438.x},
	doi = {10.1111/j.1467-9868.2004.00438.x},
	abstract = {Summary. We consider the problem of testing null hypotheses that include restrictions on the variance component in a linear mixed model with one variance component and we derive the finite sample and asymptotic distribution of the likelihood ratio test and the restricted likelihood ratio test. The spectral representations of the likelihood ratio test and the restricted likelihood ratio test statistics are used as the basis of efficient simulation algorithms of their null distributions. The large sample χ2 mixture approximations using the usual asymptotic theory for a null hypothesis on the boundary of the parameter space have been shown to be poor in simulation studies. Our asymptotic calculations explain these empirical results. The theory of Self and Liang applies only to linear mixed models for which the data vector can be partitioned into a large number of independent and identically distributed subvectors. One-way analysis of variance and penalized splines models illustrate the results.},
	pages = {165--185},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Crainiceanu, Ciprian M. and Ruppert, David},
	urldate = {2024-10-24},
	date = {2004},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2004.00438.x},
	keywords = {Degrees of freedom, Non-regular problems, Penalized splines},
	file = {Full Text PDF:C\:\\Users\\fgfra\\Zotero\\storage\\BZRHZLQY\\Crainiceanu and Ruppert - 2004 - Likelihood ratio tests in linear mixed models with one variance component.pdf:application/pdf;Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\R8AI5TCG\\j.1467-9868.2004.00438.html:text/html},
}

@article{scheiplRLRsimTestingRandom2008,
	title = {{RLRsim}: Testing for Random Effects or Nonparametric Regression Functions in Additive Mixed Models},
	journaltitle = {Linear Mixed Models},
	author = {Scheipl, Fabian and Greven, Sonia and K ̈uchenhoff, Helmut},
	date = {2008},
	langid = {english},
	file = {PDF:C\:\\Users\\fgfra\\Zotero\\storage\\U24QVCQQ\\Scheipl - RLRsim Testing for Random Effects or   Nonparametric Regression Functions   in Additive Mixed Model.pdf:application/pdf},
}

@book{azzaliniInferenzaStatisticaPresentazione2001,
	title = {Inferenza statistica: Una presentazione basata sul concetto di verosimiglianza},
	url = {https://link.springer.com/book/9788847001305},
	author = {Azzalini, Adelchi},
	urldate = {2024-10-24},
	date = {2001},
	langid = {english},
	file = {Snapshot:C\:\\Users\\fgfra\\Zotero\\storage\\K6I8PBTK\\9788847001305.html:text/html},
}
